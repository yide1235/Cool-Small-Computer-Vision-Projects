{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "premium",
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b36cd5aab8944947915b99bdfcee9905": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9821cd2b1d78410bac0ae1ce44b5c4a7",
              "IPY_MODEL_fc2d24c89ef74614822b44788deeb5af",
              "IPY_MODEL_072e10750a154bad93a12ae1033e4bff"
            ],
            "layout": "IPY_MODEL_a52c328960a14fb38efcea323309fa9d"
          }
        },
        "9821cd2b1d78410bac0ae1ce44b5c4a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2bb853dabcb434da3db902525da4ac8",
            "placeholder": "​",
            "style": "IPY_MODEL_83b563085c6f4b37a5b96173665ee61a",
            "value": "100%"
          }
        },
        "fc2d24c89ef74614822b44788deeb5af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49fe3fc41ac349369b1718391045b834",
            "max": 46830571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_770febf48afb48acbe4cc464ca720d73",
            "value": 46830571
          }
        },
        "072e10750a154bad93a12ae1033e4bff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40b7c5cd86e6432cb8f73bb4645f1e1a",
            "placeholder": "​",
            "style": "IPY_MODEL_499a586312074a14a69213d04b861295",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 113MB/s]"
          }
        },
        "a52c328960a14fb38efcea323309fa9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2bb853dabcb434da3db902525da4ac8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83b563085c6f4b37a5b96173665ee61a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "49fe3fc41ac349369b1718391045b834": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "770febf48afb48acbe4cc464ca720d73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "40b7c5cd86e6432cb8f73bb4645f1e1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "499a586312074a14a69213d04b861295": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmCbxoDrADcF"
      },
      "source": [
        "# Google Colab setup with Google Drive folder\n",
        "\n",
        "This notebook provides the code you need to set up Google Colab to run and import files from within a Google Drive folder.\n",
        "\n",
        "This will allow you to upload assignment code to your Google Drive and then run the code on Google Colab machines (with free GPUs if needed). \n",
        "\n",
        "You will need to create a folder in your Google Drive to hold your assignments and you will need to open Colaboratory within this folder before running the set up code (check the link above to see how)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWhrmhqVCyGH"
      },
      "source": [
        "# Mount Google Drive\n",
        "\n",
        "This will allow the Colab machine to access Google Drive folders by mounting the drive on the machine. You will be asked to copy and paste an authentication code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wv2oKmF9AJtI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04bfd367-daf2-441c-d0cb-207b41f74e3e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKGxaMcmP_Et",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd90919c-99e9-4b11-b1f2-37c316680071"
      },
      "source": [
        "ls"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mdata\u001b[0m/  different_loss.png  \u001b[01;34mweights\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Qs04PPwDOFy"
      },
      "source": [
        "# Change directory to allow imports\n",
        "\n",
        "\n",
        "As noted above, you should create a Google Drive folder to hold all your assignment files. You will need to add this code to the top of any python notebook you run to be able to import python files from your drive assignment folder (you should change the file path below to be your own assignment folder). Following the hand-out, you should have a directory \"SFU_CMPT_CV_lab2\" on g-drive, which should have a directory \"data\", which contains three tar.gz files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UA2-UyfpEc9O"
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/gdrive/My Drive/SFU_CMPT_CV_lab2\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyRCWAIyRHWc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b58a2d73-2305-49e7-c508-90921010d4e4"
      },
      "source": [
        "ls # Check if this is your folder"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mdata\u001b[0m/  different_loss.png  \u001b[01;34mweights\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJOCaUMilRz_"
      },
      "source": [
        "# Copy data to local dir"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90MxG_eRla0W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0af84408-9dd1-4db0-8361-63dd0a92308d"
      },
      "source": [
        "!mkdir /data\n",
        "!cp data/cifar100.tar.gz /data/\n",
        "!tar -xf /data/cifar100.tar.gz -C /data/\n",
        "!cp data/test.tar.gz /data\n",
        "!tar -xf /data/test.tar.gz -C /data\n",
        "!cp data/train.tar.gz /data\n",
        "!tar -xf /data/train.tar.gz -C /data/"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/data’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvFEFItpl98p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cec8ddec-eb9c-4f4e-e55e-8b8ff828cd98"
      },
      "source": [
        "ls /data"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mcifar100\u001b[0m/  cifar100.tar.gz  \u001b[01;34mtest\u001b[0m/  test.tar.gz  \u001b[01;34mtrain\u001b[0m/  train.tar.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDU5aVgR9QBx"
      },
      "source": [
        "# Set up GPU and PyTorch\n",
        "\n",
        "First, ensure that your notebook on Colaboratory is set up to use GPU. After opening the notebook on Colaboratory, go to Edit>Notebook settings, select Python 3 under \"Runtime type,\" select GPU under \"Hardware accelerator,\" and save.\n",
        "\n",
        "Next, install PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjbQtzKT9Uc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e99603c0-b2a5-4b15-e05c-2bda4bca3fcb"
      },
      "source": [
        "!pip3 install torch torchvision"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_BekZYY9Vzx"
      },
      "source": [
        "Make sure that pytorch is installed and works with GPU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TXSJWQa9efx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f6dcbfb-742f-4998-c1b2-830a757fce42"
      },
      "source": [
        "import torch\n",
        "a = torch.Tensor([1]).cuda()\n",
        "print(a)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEeRNsCjRXZK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5889497-bbdd-4f0a-df69-478168393c31"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qChgLJERsvZP"
      },
      "source": [
        "# Part 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlyCnvf6WzjR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb8c660e-6199-43d1-9758-da91f3fb223f"
      },
      "source": [
        "\"\"\"Headers\"\"\"\n",
        "\n",
        "from __future__ import print_function\n",
        "from PIL import Image\n",
        "import os\n",
        "import os.path\n",
        "import numpy as np\n",
        "import sys\n",
        "if sys.version_info[0] == 2:\n",
        "    import cPickle as pickle\n",
        "else:\n",
        "    import pickle\n",
        "\n",
        "import torch.utils.data as data\n",
        "from torchvision.datasets.utils import download_url, check_integrity\n",
        "\n",
        "import csv\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os.path\n",
        "import sys\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "np.random.seed(111)\n",
        "torch.cuda.manual_seed_all(111)\n",
        "torch.manual_seed(111)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f42e1e65c90>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "137GhZMrcTuj"
      },
      "source": [
        "\n",
        "\n",
        "## **Just execute the cell below. This is the dataloader. DO NOT CHANGE ANYTHING IN HERE!**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URUH4fzzWqKr"
      },
      "source": [
        "\"\"\"\"\"\"\n",
        "\n",
        "class CIFAR10_SFU_CV(data.Dataset):\n",
        "    \"\"\"`CIFAR10 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
        "\n",
        "    Args:\n",
        "        root (string): Root directory of dataset where directory\n",
        "            ``cifar-10-batches-py`` exists or will be saved to if download is set to True.\n",
        "        train (bool, optional): If True, creates dataset from training set, otherwise\n",
        "            creates from test set.\n",
        "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
        "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
        "        target_transform (callable, optional): A function/transform that takes in the\n",
        "            target and transforms it.\n",
        "        download (bool, optional): If true, downloads the dataset from the internet and\n",
        "            puts it in root directory. If dataset is already downloaded, it is not\n",
        "            downloaded again.\n",
        "\n",
        "    \"\"\"\n",
        "    base_folder = 'cifar100'\n",
        "    url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
        "    filename = \"cifar100.tar.gz\"\n",
        "    tgz_md5 = 'c58f30108f718f92721af3b95e74349a'\n",
        "    train_list = [\n",
        "        ['data_batch_1', 'c99cafc152244af753f735de768cd75f'],\n",
        "        ['data_batch_2', 'd4bba439e000b95fd0a9bffe97cbabec'],\n",
        "        ['data_batch_3', '54ebc095f3ab1f0389bbae665268c751'],\n",
        "        ['data_batch_4', '634d18415352ddfa80567beed471001a'],\n",
        "        ['data_batch_5', '482c414d41f54cd18b22e5b47cb7c3cb'],\n",
        "    ]\n",
        "\n",
        "    test_list = [\n",
        "        ['test_batch', '40351d587109b95175f43aff81a1287e'],\n",
        "    ]\n",
        "\n",
        "    def __init__(self, root, fold=\"train\",\n",
        "                 transform=None, target_transform=None,\n",
        "                 download=False):\n",
        "        \n",
        "        fold = fold.lower()\n",
        "\n",
        "        self.train = False\n",
        "        self.test = False\n",
        "        self.val = False\n",
        "\n",
        "        if fold == \"train\":\n",
        "            self.train = True\n",
        "        elif fold == \"test\":\n",
        "            self.test = True\n",
        "        elif fold == \"val\":\n",
        "            self.val = True\n",
        "        else:\n",
        "            raise RuntimeError(\"Not train-val-test\")\n",
        "\n",
        "\n",
        "        self.root = os.path.expanduser(root)\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        fpath = os.path.join(root, self.filename)\n",
        "        if not self._check_integrity():\n",
        "            raise RuntimeError('Dataset not found or corrupted.' +\n",
        "                               ' Download it and extract the file again.')\n",
        "\n",
        "        # now load the picked numpy arrays\n",
        "        if self.train or self.val:\n",
        "            self.train_data = []\n",
        "            self.train_labels = []\n",
        "            for fentry in self.train_list:\n",
        "                f = fentry[0]\n",
        "                file = os.path.join(self.root, self.base_folder, f)\n",
        "                fo = open(file, 'rb')\n",
        "                if sys.version_info[0] == 2:\n",
        "                    entry = pickle.load(fo)\n",
        "                else:\n",
        "                    entry = pickle.load(fo, encoding='latin1')\n",
        "                self.train_data.append(entry['data'])\n",
        "                if 'labels' in entry:\n",
        "                    self.train_labels += entry['labels']\n",
        "                else:\n",
        "                    self.train_labels += entry['fine_labels']\n",
        "                fo.close()\n",
        "\n",
        "            self.train_data = np.concatenate(self.train_data)\n",
        "            self.train_data = self.train_data.reshape((50000, 3, 32, 32))\n",
        "            self.train_data = self.train_data.transpose((0, 2, 3, 1))  # convert to HWC\n",
        "            \n",
        "            p = np.arange(0,50000,10)\n",
        "            mask_train = np.ones((50000,), dtype=bool)\n",
        "            mask_train[p] = False\n",
        "            mask_val = np.zeros((50000,), dtype=bool)\n",
        "            mask_val[p] = True\n",
        "\n",
        "            copy_all_data = np.array(self.train_data)\n",
        "            self.val_data = np.array(copy_all_data[mask_val])\n",
        "            self.train_data = np.array(copy_all_data[mask_train])\n",
        "            \n",
        "            copy_all_labels = np.array(self.train_labels)\n",
        "            self.val_labels = np.array(copy_all_labels[mask_val])\n",
        "            self.train_labels = np.array(copy_all_labels[mask_train])\n",
        "\n",
        "        elif self.test:\n",
        "            f = self.test_list[0][0]\n",
        "            file = os.path.join(self.root, self.base_folder, f)\n",
        "            fo = open(file, 'rb')\n",
        "            if sys.version_info[0] == 2:\n",
        "                entry = pickle.load(fo)\n",
        "            else:\n",
        "                entry = pickle.load(fo, encoding='latin1')\n",
        "            self.test_data = entry['data']\n",
        "\n",
        "            if 'labels' in entry:\n",
        "                self.test_labels = entry['labels']\n",
        "            else:\n",
        "                self.test_labels = entry['fine_labels']\n",
        "            fo.close()\n",
        "            self.test_data = self.test_data.reshape((10000, 3, 32, 32))\n",
        "            self.test_data = self.test_data.transpose((0, 2, 3, 1))  # convert to HWC\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "\n",
        "        Returns:\n",
        "            tuple: (image, target) where target is index of the target class.\n",
        "        \"\"\"\n",
        "        if self.train:\n",
        "            img, target = self.train_data[index], self.train_labels[index]\n",
        "        elif self.test:\n",
        "            img, target = self.test_data[index], self.test_labels[index]\n",
        "        elif self.val:\n",
        "            img, target = self.val_data[index], self.val_labels[index]\n",
        "\n",
        "        # doing this so that it is consistent with all other datasets\n",
        "        # to return a PIL Image\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.train:\n",
        "            return len(self.train_data)\n",
        "        elif self.test:\n",
        "            return len(self.test_data)\n",
        "        elif self.val:\n",
        "            return len(self.val_data)\n",
        "\n",
        "    def _check_integrity(self):\n",
        "        root = self.root\n",
        "        for fentry in (self.train_list + self.test_list):\n",
        "            filename, md5 = fentry[0], fentry[1]\n",
        "            fpath = os.path.join(root, self.base_folder, filename)\n",
        "            if not check_integrity(fpath, md5):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def __repr__(self):\n",
        "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
        "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
        "        tmp = 'train' if self.train is True else 'test'\n",
        "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
        "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
        "        tmp = '    Transforms (if any): '\n",
        "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
        "        tmp = '    Target Transforms (if any): '\n",
        "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
        "        return fmt_str\n",
        "\n",
        "\n",
        "class CIFAR100_SFU_CV(CIFAR10_SFU_CV):\n",
        "    \"\"\"`CIFAR100 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
        "\n",
        "    This is a subclass of the `CIFAR10` Dataset.\n",
        "    \"\"\"\n",
        "    base_folder = 'cifar100'\n",
        "    filename = \"cifar100.tar.gz\"\n",
        "    tgz_md5 = 'e68a4c763591787a0b39fe2209371f32'\n",
        "    train_list = [\n",
        "        ['train_cs543', '49eee854445c1e2ebe796cd93c20bb0f'],\n",
        "    ]\n",
        "\n",
        "    test_list = [\n",
        "        ['test_cs543', 'd3fe9f6a9251bd443f428f896d27384f'],\n",
        "    ]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpFMv7HtcII4"
      },
      "source": [
        "This file has been adapted from the easy-to-use tutorial released by PyTorch:\n",
        "http://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "\n",
        "Training an image classifier\n",
        "----------------------------\n",
        "\n",
        "We will do the following steps in order:\n",
        "\n",
        "1. Load the CIFAR100_SFU_CV training, validation and test datasets using\n",
        "   torchvision. Use torchvision.transforms to apply transforms on the\n",
        "   dataset.\n",
        "2. Define a Convolution Neural Network - BaseNet\n",
        "3. Define a loss function and optimizer\n",
        "4. Train the network on training data and check performance on val set.\n",
        "   Plot train loss and validation accuracies.\n",
        "5. Try the network on test data and create .csv file for submission to kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld6juH34dWWq"
      },
      "source": [
        "# <<TODO#5>> Based on the val set performance, decide how many\n",
        "# epochs are apt for your model.\n",
        "# ---------\n",
        "EPOCHS = 400\n",
        "# ---------\n",
        "\n",
        "IS_GPU = True\n",
        "TEST_BS = 128\n",
        "TOTAL_CLASSES = 100\n",
        "TRAIN_BS = 128\n",
        "PATH_TO_CIFAR100_SFU_CV = \"/data/\"\n",
        "# checkpoint_save_dir=\"./weights/\""
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ENlTTMi-qFD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea374a39-5c6b-4861-d181-3bba5c3abf76"
      },
      "source": [
        "ls /data/cifar100/"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_cs543  train_cs543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d57CSAj1dfix"
      },
      "source": [
        "def calculate_val_accuracy(valloader, is_gpu):\n",
        "    \"\"\" Util function to calculate val set accuracy,\n",
        "    both overall and per class accuracy\n",
        "    Args:\n",
        "        valloader (torch.utils.data.DataLoader): val set \n",
        "        is_gpu (bool): whether to run on GPU\n",
        "    Returns:\n",
        "        tuple: (overall accuracy, class level accuracy)\n",
        "    \"\"\"    \n",
        "    correct = 0.\n",
        "    total = 0.\n",
        "    predictions = []\n",
        "\n",
        "    class_correct = list(0. for i in range(TOTAL_CLASSES))\n",
        "    class_total = list(0. for i in range(TOTAL_CLASSES))\n",
        "\n",
        "    for data in valloader:\n",
        "        images, labels = data\n",
        "        if is_gpu:\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "        outputs = net(Variable(images))\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        predictions.extend(list(predicted.cpu().numpy()))\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum()\n",
        "\n",
        "        c = (predicted == labels).squeeze()\n",
        "        c = c.cpu()\n",
        "        for i in range(len(labels)):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i]\n",
        "            class_total[label] += 1\n",
        "\n",
        "    class_accuracy = 100 * np.divide(class_correct, class_total)\n",
        "    return 100*correct/total, class_accuracy"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aq2qOUaJeAWJ"
      },
      "source": [
        "1.** Loading CIFAR100_SFU_CV**\n",
        "\n",
        "We modify the dataset to create CIFAR100_SFU_CV dataset which consist of 45000 training images (450 of each class), 5000 validation images (50 of each class) and 10000 test images (100 of each class). The train and val datasets have labels while all the labels in the test set are set to 0.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2UcDZmtdfq3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18de23c0-0a70-413a-a267-b8dc0a9dbbb1"
      },
      "source": [
        "# The output of torchvision datasets are PILImage images of range [0, 1].\n",
        "# Using transforms.ToTensor(), transform them to Tensors of normalized range\n",
        "# [-1, 1].\n",
        "\n",
        "\n",
        "# <<TODO#1>> Use transforms.Normalize() with the right parameters to \n",
        "# make the data well conditioned (zero mean, std dev=1) for improved training.\n",
        "# <<TODO#2>> Try using transforms.RandomCrop() and/or transforms.RandomHorizontalFlip()\n",
        "# to augment training data.\n",
        "# After your edits, make sure that test_transform should have the same data\n",
        "# normalization parameters as train_transform\n",
        "# You shouldn't have any data augmentation in test_transform (val or test data is never augmented).\n",
        "# ---------------------\n",
        "IMAGE_HEIGHT_WIDTH=32\n",
        "train_transform = transforms.Compose(\n",
        "    [transforms.RandomCrop(IMAGE_HEIGHT_WIDTH, padding=(IMAGE_HEIGHT_WIDTH // 8)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.49139968, 0.48215827, 0.44653124), (0.24703233, 0.24348505, 0.26158768)),])\n",
        "test_transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "    transforms.Normalize((0.49139968, 0.48215827, 0.44653124), (0.24703233, 0.24348505, 0.26158768)),])\n",
        "# ---------------------\n",
        "\n",
        "trainset = CIFAR100_SFU_CV(root=PATH_TO_CIFAR100_SFU_CV, fold=\"train\",\n",
        "                                        download=True, transform=train_transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "print(\"Train set size: \"+str(len(trainset)))\n",
        "\n",
        "valset = CIFAR100_SFU_CV(root=PATH_TO_CIFAR100_SFU_CV, fold=\"val\",\n",
        "                                       download=True, transform=test_transform)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=128,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "print(\"Val set size: \"+str(len(valset)))\n",
        "\n",
        "testset = CIFAR100_SFU_CV(root=PATH_TO_CIFAR100_SFU_CV, fold=\"test\",\n",
        "                                       download=True, transform=test_transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "print(\"Test set size: \"+str(len(testset)))\n",
        "\n",
        "# The 100 classes for CIFAR100\n",
        "classes = ['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm']\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set size: 45000\n",
            "Val set size: 5000\n",
            "Test set size: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b_fBznndp4W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f0e9687-d35e-4275-fde6-b9602d29103c"
      },
      "source": [
        "########################################################################\n",
        "# 2. Define a Convolution Neural Network\n",
        "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "# We provide a basic network that you should understand, run and\n",
        "# eventually improve\n",
        "# <<TODO>> Add more conv layers\n",
        "# <<TODO>> Add more fully connected (fc) layers\n",
        "# <<TODO>> Add regularization layers like Batchnorm.\n",
        "#          nn.BatchNorm2d after conv layers:\n",
        "#          http://pytorch.org/docs/master/nn.html#batchnorm2d\n",
        "#          nn.BatchNorm1d after fc layers:\n",
        "#          http://pytorch.org/docs/master/nn.html#batchnorm1d\n",
        "# This is a good resource for developing a CNN for classification:\n",
        "# http://cs231n.github.io/convolutional-networks/#layers\n",
        "\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# import torch\n",
        "\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "\n",
        "# import torch.nn.functional as F\n",
        "# from torch.autograd import Variable\n",
        "\n",
        "# import torchvision.datasets as dset\n",
        "# import torchvision.transforms as transforms\n",
        "# from torch.utils.data import DataLoader\n",
        "\n",
        "# import torchvision.models as models\n",
        "\n",
        "# import sys\n",
        "# import math\n",
        "\n",
        "\n",
        "\n",
        "# class BaseNet(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(BaseNet, self).__init__()\n",
        "        \n",
        "#         # <<TODO#3>> Add more conv layers with increasing \n",
        "#         # output channels\n",
        "#         # <<TODO#4>> Add normalization layers after conv\n",
        "#         # layers (nn.BatchNorm2d)\n",
        "\n",
        "#         # Also experiment with kernel size in conv2d layers (say 3\n",
        "#         # inspired from VGGNet)\n",
        "#         # To keep it simple, keep the same kernel size\n",
        "#         # (right now set to 5) in all conv layers.\n",
        "#         # Do not have a maxpool layer after every conv layer in your\n",
        "#         # deeper network as it leads to too much loss of information.\n",
        "        \n",
        "#         self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "#         self.pool = nn.MaxPool2d(2, 2)\n",
        "#         self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "\n",
        "#         # <<TODO#3>> Add more linear (fc) layers\n",
        "#         # <<TODO#4>> Add normalization layers after linear and\n",
        "#         # experiment inserting them before or after ReLU (nn.BatchNorm1d)\n",
        "#         # More on nn.sequential:\n",
        "#         # http://pytorch.org/docs/master/nn.html#torch.nn.Sequential\n",
        "        \n",
        "#         self.fc_net = nn.Sequential(\n",
        "#             nn.Linear(16 * 5 * 5, TOTAL_CLASSES//2),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Linear(TOTAL_CLASSES//2, TOTAL_CLASSES),\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "\n",
        "#         # <<TODO#3&#4>> Based on the above edits, you'll have\n",
        "#         # to edit the forward pass description here.\n",
        "\n",
        "#         x = self.pool(F.relu(self.conv1(x)))\n",
        "#         # Output size = 28//2 x 28//2 = 14 x 14\n",
        "\n",
        "#         x = self.pool(F.relu(self.conv2(x)))\n",
        "#         # Output size = 10//2 x 10//2 = 5 x 5\n",
        "\n",
        "#         # See the CS231 link to understand why this is 16*5*5!\n",
        "#         # This will help you design your own deeper network\n",
        "#         x = x.view(-1, 16 * 5 * 5)\n",
        "#         x = self.fc_net(x)\n",
        "\n",
        "#         # No softmax is needed as the loss function in step 3\n",
        "#         # takes care of that\n",
        "        \n",
        "#         return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# class EdwardNet(nn.Module):\n",
        "#     def __init__(self, num_classes=100):\n",
        "#         super(EdwardNet, self).__init__()\n",
        "#         #32\n",
        "#         self.layer1 = nn.Sequential(\n",
        "#             nn.Conv2d(3, 64, kernel_size=5, stride=1, padding=1),\n",
        "#             nn.BatchNorm2d(64),\n",
        "#             nn.ReLU())\n",
        "#         #30\n",
        "#         self.layer2 = nn.Sequential(\n",
        "#             nn.Conv2d(64, 64, kernel_size=5, stride=1, padding=1),\n",
        "#             nn.BatchNorm2d(64),\n",
        "#             nn.ReLU())\n",
        "#         #28\n",
        "#         self.layer3 = nn.Sequential(\n",
        "#             nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=1),\n",
        "#             nn.BatchNorm2d(128),\n",
        "#             nn.ReLU(),\n",
        "            \n",
        "        \n",
        "#             )\n",
        "#         #26\n",
        "#         self.layer4 = nn.Sequential(\n",
        "#             nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "#             nn.BatchNorm2d(128),\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(kernel_size=2,stride=1,padding=0),\n",
        "#         )\n",
        "#         #25\n",
        "#         self.layer5 = nn.Sequential(\n",
        "#             nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "#             nn.BatchNorm2d(256),\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(kernel_size=2,stride=1,padding=0),\n",
        "#             )\n",
        "#         #24\n",
        "#         self.layer6 = nn.Sequential(\n",
        "#             nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "#             nn.BatchNorm2d(256),\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(kernel_size=2,stride=2,padding=0),\n",
        "        \n",
        "#             )\n",
        "#         #12\n",
        "#         self.layer7 = nn.Sequential(\n",
        "#             nn.Conv2d(256, 256, kernel_size=5, stride=1, padding=1),\n",
        "#             nn.BatchNorm2d(256),\n",
        "#             nn.ReLU(),\n",
        "\n",
        "        \n",
        "#             )\n",
        "#         #10\n",
        "#         self.layer8 = nn.Sequential(\n",
        "#             nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "#             nn.BatchNorm2d(512),\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(kernel_size=2,stride=1,padding=0),\n",
        "        \n",
        "#             )\n",
        "#         #9\n",
        "#         self.layer9 = nn.Sequential(\n",
        "#             nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "#             nn.BatchNorm2d(512),\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(kernel_size=2,stride=1,padding=0),\n",
        "        \n",
        "#             )\n",
        "#         #8\n",
        "#         self.layer10 = nn.Sequential(\n",
        "#             nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "#             nn.BatchNorm2d(512),\n",
        "#             nn.ReLU(),\n",
        "\n",
        "        \n",
        "#             )\n",
        "        \n",
        "#         self.layer11 = nn.Sequential(\n",
        "#             nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "#             nn.BatchNorm2d(512),\n",
        "#             nn.ReLU(),\n",
        "            \n",
        "        \n",
        "#             )\n",
        "#         #8\n",
        "#         self.layer12 = nn.Sequential(\n",
        "#             nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "#             nn.BatchNorm2d(512),\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(kernel_size=2,stride=1,padding=0),\n",
        "        \n",
        "#             )\n",
        "\n",
        "#         #7\n",
        "#         self.fc13 = nn.Sequential(\n",
        "#             # nn.Dropout(0.5),\n",
        "#             nn.Linear(25088, 12544),\n",
        "#             nn.ReLU())\n",
        "#         self.fc14 = nn.Sequential(\n",
        "#             # nn.Dropout(0.5),\n",
        "#             nn.Linear(12544, 6272),\n",
        "#             nn.ReLU())\n",
        "#         self.fc15 = nn.Sequential(\n",
        "#             # nn.Dropout(0.5),\n",
        "#             nn.Linear(6272, 3136),\n",
        "#             nn.ReLU())\n",
        "#         self.fc16= nn.Sequential(\n",
        "#             nn.Linear(3136, num_classes))\n",
        "        \n",
        "#     def forward(self, x):\n",
        "#         out = self.layer1(x)\n",
        "#         out = self.layer2(out)\n",
        "#         out = self.layer3(out)\n",
        "#         out = self.layer4(out)\n",
        "#         out = self.layer5(out)\n",
        "#         out = self.layer6(out)\n",
        "#         out = self.layer7(out)\n",
        "#         out = self.layer8(out)\n",
        "#         out = self.layer9(out)\n",
        "#         out = self.layer10(out)\n",
        "#         out = self.layer11(out)\n",
        "#         out = self.layer12(out)\n",
        "\n",
        "#         out = out.reshape(out.size(0), -1)\n",
        "\n",
        "#         out = self.fc13(out)\n",
        "#         out = self.fc14(out)\n",
        "#         out = self.fc15(out)\n",
        "#         out = self.fc16(out)\n",
        "#         return out\n",
        "\n",
        "\n",
        "\n",
        "# # Create an instance of the nn.module class defined above:\n",
        "# net = EdwardNet()\n",
        "# #result for EdwardNet()\n",
        "# '''\n",
        "# [1] loss: 4.018\n",
        "# Accuracy of the network on the val images: 12 %\n",
        "# [2] loss: 3.473\n",
        "# Accuracy of the network on the val images: 21 %\n",
        "# [3] loss: 3.032\n",
        "# Accuracy of the network on the val images: 29 %\n",
        "# [4] loss: 2.688\n",
        "# Accuracy of the network on the val images: 36 %\n",
        "# [5] loss: 2.437\n",
        "# Accuracy of the network on the val images: 39 %\n",
        "# [6] loss: 2.243\n",
        "# Accuracy of the network on the val images: 43 %\n",
        "# [7] loss: 2.081\n",
        "# Accuracy of the network on the val images: 46 %\n",
        "# [8] loss: 1.952\n",
        "# Accuracy of the network on the val images: 47 %\n",
        "# [9] loss: 1.831\n",
        "# Accuracy of the network on the val images: 49 %\n",
        "# [10] loss: 1.723\n",
        "# Accuracy of the network on the val images: 53 %\n",
        "# [11] loss: 1.631\n",
        "# Accuracy of the network on the val images: 51 %\n",
        "# [12] loss: 1.535\n",
        "# Accuracy of the network on the val images: 54 %\n",
        "# [13] loss: 1.456\n",
        "# Accuracy of the network on the val images: 55 %\n",
        "# [14] loss: 1.378\n",
        "# Accuracy of the network on the val images: 57 %\n",
        "# [15] loss: 1.303\n",
        "# Accuracy of the network on the val images: 58 %\n",
        "# [16] loss: 1.246\n",
        "# Accuracy of the network on the val images: 58 %\n",
        "# [17] loss: 1.178\n",
        "# Accuracy of the network on the val images: 58 %\n",
        "# [18] loss: 1.105\n",
        "# Accuracy of the network on the val images: 58 %\n",
        "# [19] loss: 1.059\n",
        "# Accuracy of the network on the val images: 60 %\n",
        "# [20] loss: 0.996\n",
        "# Accuracy of the network on the val images: 60 %\n",
        "# [21] loss: 0.939\n",
        "# Accuracy of the network on the val images: 60 %\n",
        "# [22] loss: 0.888\n",
        "# Accuracy of the network on the val images: 61 %\n",
        "# [23] loss: 0.839\n",
        "# Accuracy of the network on the val images: 62 %\n",
        "# [24] loss: 0.786\n",
        "# Accuracy of the network on the val images: 61 %\n",
        "# [25] loss: 0.745\n",
        "# Accuracy of the network on the val images: 62 %\n",
        "# [26] loss: 0.708\n",
        "# Accuracy of the network on the val images: 62 %\n",
        "# [27] loss: 0.668\n",
        "# Accuracy of the network on the val images: 62 %\n",
        "# [28] loss: 0.620\n",
        "# Accuracy of the network on the val images: 63 %\n",
        "# [29] loss: 0.573\n",
        "# Accuracy of the network on the val images: 63 %\n",
        "# [30] loss: 0.542\n",
        "# Accuracy of the network on the val images: 62 %\n",
        "# [31] loss: 0.509\n",
        "# Accuracy of the network on the val images: 62 %\n",
        "# [32] loss: 0.474\n",
        "# Accuracy of the network on the val images: 63 %\n",
        "# [33] loss: 0.445\n",
        "# Accuracy of the network on the val images: 63 %\n",
        "# [34] loss: 0.417\n",
        "# Accuracy of the network on the val images: 62 %\n",
        "# [35] loss: 0.383\n",
        "# Accuracy of the network on the val images: 63 %\n",
        "# [36] loss: 0.366\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [37] loss: 0.340\n",
        "# Accuracy of the network on the val images: 63 %\n",
        "# [38] loss: 0.328\n",
        "# Accuracy of the network on the val images: 63 %\n",
        "# [39] loss: 0.296\n",
        "# Accuracy of the network on the val images: 63 %\n",
        "# [40] loss: 0.286\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [41] loss: 0.260\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [42] loss: 0.245\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [43] loss: 0.231\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [44] loss: 0.217\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [45] loss: 0.203\n",
        "# Accuracy of the network on the val images: 63 %\n",
        "# [46] loss: 0.197\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [47] loss: 0.184\n",
        "# Accuracy of the network on the val images: 63 %\n",
        "# [48] loss: 0.182\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [49] loss: 0.167\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [50] loss: 0.158\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [51] loss: 0.154\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [52] loss: 0.139\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [53] loss: 0.136\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [54] loss: 0.131\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [55] loss: 0.125\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [56] loss: 0.119\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [57] loss: 0.117\n",
        "# Accuracy of the network on the val images: 63 %\n",
        "# [58] loss: 0.113\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [59] loss: 0.107\n",
        "# Accuracy of the network on the val images: 63 %\n",
        "# [60] loss: 0.102\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [61] loss: 0.092\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [62] loss: 0.090\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [63] loss: 0.088\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [64] loss: 0.083\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [65] loss: 0.085\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [66] loss: 0.075\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [67] loss: 0.073\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [68] loss: 0.076\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [69] loss: 0.075\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [70] loss: 0.074\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [71] loss: 0.072\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [72] loss: 0.070\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [73] loss: 0.069\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [74] loss: 0.060\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [75] loss: 0.064\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [76] loss: 0.055\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [77] loss: 0.057\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [78] loss: 0.055\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [79] loss: 0.054\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [80] loss: 0.053\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [81] loss: 0.051\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [82] loss: 0.053\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [83] loss: 0.051\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [84] loss: 0.046\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [85] loss: 0.044\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [86] loss: 0.037\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [87] loss: 0.045\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [88] loss: 0.048\n",
        "# Accuracy of the network on the val images: 66 %\n",
        "# [89] loss: 0.043\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [90] loss: 0.041\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [91] loss: 0.041\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [92] loss: 0.042\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [93] loss: 0.034\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [94] loss: 0.044\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [95] loss: 0.037\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [96] loss: 0.029\n",
        "# Accuracy of the network on the val images: 66 %\n",
        "# [97] loss: 0.035\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [98] loss: 0.032\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [99] loss: 0.035\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [100] loss: 0.036\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# Finished Training\n",
        "# '''\n",
        "# #seems that its would not get more using this simple architecture\n",
        "# #hence, I re-implemneted dense net\n",
        "\n",
        "\n",
        "\n",
        "# #\n",
        "\n",
        "# '''\n",
        "# class Bottleneck(nn.Module):\n",
        "#     def __init__(self, nChannels, growthRate):\n",
        "#         super(Bottleneck, self).__init__()\n",
        "#         interChannels = 4*growthRate\n",
        "#         self.bn1 = nn.BatchNorm2d(nChannels)\n",
        "#         self.conv1 = nn.Conv2d(nChannels, interChannels, kernel_size=1,\n",
        "#                                bias=False)\n",
        "#         self.bn2 = nn.BatchNorm2d(interChannels)\n",
        "#         self.conv2 = nn.Conv2d(interChannels, growthRate, kernel_size=3,\n",
        "#                                padding=1, bias=False)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         out = self.conv1(F.relu(self.bn1(x)))\n",
        "#         out = self.conv2(F.relu(self.bn2(out)))\n",
        "#         out = torch.cat((x, out), 1)\n",
        "#         return out\n",
        "\n",
        "# class SingleLayer(nn.Module):\n",
        "#     def __init__(self, nChannels, growthRate):\n",
        "#         super(SingleLayer, self).__init__()\n",
        "#         self.bn1 = nn.BatchNorm2d(nChannels)\n",
        "#         self.conv1 = nn.Conv2d(nChannels, growthRate, kernel_size=3,\n",
        "#                                padding=1, bias=False)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         out = self.conv1(F.relu(self.bn1(x)))\n",
        "#         out = torch.cat((x, out), 1)\n",
        "#         return out\n",
        "\n",
        "# class Transition(nn.Module):\n",
        "#     def __init__(self, nChannels, nOutChannels):\n",
        "#         super(Transition, self).__init__()\n",
        "#         self.bn1 = nn.BatchNorm2d(nChannels)\n",
        "#         self.conv1 = nn.Conv2d(nChannels, nOutChannels, kernel_size=1,\n",
        "#                                bias=False)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         out = self.conv1(F.relu(self.bn1(x)))\n",
        "#         out = F.avg_pool2d(out, 2)\n",
        "#         return out\n",
        "\n",
        "\n",
        "# class DenseNet2(nn.Module):\n",
        "#     def __init__(self, growthRate, depth, reduction, nClasses, bottleneck):\n",
        "#         super(DenseNet2, self).__init__()\n",
        "\n",
        "#         nDenseBlocks = (depth-4) // 3\n",
        "#         if bottleneck:\n",
        "#             nDenseBlocks //= 2\n",
        "\n",
        "#         nChannels = 2*growthRate\n",
        "#         self.conv1 = nn.Conv2d(3, nChannels, kernel_size=3, padding=1,\n",
        "#                                bias=False)\n",
        "#         self.dense1 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
        "#         nChannels += nDenseBlocks*growthRate\n",
        "#         nOutChannels = int(math.floor(nChannels*reduction))\n",
        "#         self.trans1 = Transition(nChannels, nOutChannels)\n",
        "\n",
        "#         nChannels = nOutChannels\n",
        "#         self.dense2 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
        "#         nChannels += nDenseBlocks*growthRate\n",
        "#         nOutChannels = int(math.floor(nChannels*reduction))\n",
        "#         self.trans2 = Transition(nChannels, nOutChannels)\n",
        "\n",
        "#         nChannels = nOutChannels\n",
        "#         self.dense3 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
        "#         nChannels += nDenseBlocks*growthRate\n",
        "\n",
        "#         self.bn1 = nn.BatchNorm2d(nChannels)\n",
        "#         self.fc = nn.Linear(nChannels, nClasses)\n",
        "\n",
        "#         for m in self.modules():\n",
        "#             if isinstance(m, nn.Conv2d):\n",
        "#                 n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "#                 m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "#             elif isinstance(m, nn.BatchNorm2d):\n",
        "#                 m.weight.data.fill_(1)\n",
        "#                 m.bias.data.zero_()\n",
        "#             elif isinstance(m, nn.Linear):\n",
        "#                 m.bias.data.zero_()\n",
        "\n",
        "#     def _make_dense(self, nChannels, growthRate, nDenseBlocks, bottleneck):\n",
        "#         layers = []\n",
        "#         for i in range(int(nDenseBlocks)):\n",
        "#             if bottleneck:\n",
        "#                 layers.append(Bottleneck(nChannels, growthRate))\n",
        "#             else:\n",
        "#                 layers.append(SingleLayer(nChannels, growthRate))\n",
        "#             nChannels += growthRate\n",
        "#         return nn.Sequential(*layers)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         out = self.conv1(x)\n",
        "#         out = self.trans1(self.dense1(out))\n",
        "#         out = self.trans2(self.dense2(out))\n",
        "#         out = self.dense3(out)\n",
        "#         out = torch.squeeze(F.avg_pool2d(F.relu(self.bn1(out)), 8))\n",
        "#         out = F.log_softmax(self.fc(out))\n",
        "#         return out\n",
        "\n",
        "# #first training:\n",
        "# '''\n",
        "# '''\n",
        "\n",
        "\n",
        "\n",
        "# [1] loss: 4.044\n",
        "# Accuracy of the network on the val images: 12 %\n",
        "# [2] loss: 3.534\n",
        "# Accuracy of the network on the val images: 19 %\n",
        "# [3] loss: 3.117\n",
        "# Accuracy of the network on the val images: 29 %\n",
        "# [4] loss: 2.754\n",
        "# Accuracy of the network on the val images: 34 %\n",
        "# [5] loss: 2.484\n",
        "# Accuracy of the network on the val images: 39 %\n",
        "# [6] loss: 2.291\n",
        "# Accuracy of the network on the val images: 41 %\n",
        "# [7] loss: 2.125\n",
        "# Accuracy of the network on the val images: 45 %\n",
        "# [8] loss: 1.985\n",
        "# Accuracy of the network on the val images: 47 %\n",
        "# [9] loss: 1.857\n",
        "# Accuracy of the network on the val images: 49 %\n",
        "# [10] loss: 1.752\n",
        "# Accuracy of the network on the val images: 51 %\n",
        "# [11] loss: 1.653\n",
        "# Accuracy of the network on the val images: 52 %\n",
        "# [12] loss: 1.561\n",
        "# Accuracy of the network on the val images: 55 %\n",
        "# [13] loss: 1.466\n",
        "# Accuracy of the network on the val images: 55 %\n",
        "# [14] loss: 1.403\n",
        "# Accuracy of the network on the val images: 57 %\n",
        "# [15] loss: 1.321\n",
        "# Accuracy of the network on the val images: 58 %\n",
        "# [16] loss: 1.255\n",
        "# Accuracy of the network on the val images: 58 %\n",
        "# [17] loss: 1.184\n",
        "# Accuracy of the network on the val images: 57 %\n",
        "# [18] loss: 1.126\n",
        "# Accuracy of the network on the val images: 59 %\n",
        "# [19] loss: 1.058\n",
        "# Accuracy of the network on the val images: 59 %\n",
        "# [20] loss: 1.002\n",
        "# Accuracy of the network on the val images: 61 %\n",
        "# [21] loss: 0.952\n",
        "# Accuracy of the network on the val images: 61 %\n",
        "# [22] loss: 0.900\n",
        "# Accuracy of the network on the val images: 61 %\n",
        "# [23] loss: 0.846\n",
        "# Accuracy of the network on the val images: 61 %\n",
        "# [24] loss: 0.792\n",
        "# Accuracy of the network on the val images: 61 %\n",
        "# [25] loss: 0.747\n",
        "# Accuracy of the network on the val images: 61 %\n",
        "# [26] loss: 0.703\n",
        "# Accuracy of the network on the val images: 61 %\n",
        "# [27] loss: 0.662\n",
        "# Accuracy of the network on the val images: 61 %\n",
        "# [28] loss: 0.621\n",
        "# Accuracy of the network on the val images: 61 %\n",
        "# [29] loss: 0.591\n",
        "# Accuracy of the network on the val images: 62 %\n",
        "# [30] loss: 0.544\n",
        "# Accuracy of the network on the val images: 62 %\n",
        "# [31] loss: 0.511\n",
        "# Accuracy of the network on the val images: 63 %\n",
        "# [32] loss: 0.485\n",
        "# Accuracy of the network on the val images: 63 %\n",
        "# [33] loss: 0.441\n",
        "# Accuracy of the network on the val images: 63 %\n",
        "# [34] loss: 0.413\n",
        "# Accuracy of the network on the val images: 63 %\n",
        "# [35] loss: 0.387\n",
        "# Accuracy of the network on the val images: 62 %\n",
        "# [36] loss: 0.359\n",
        "# Accuracy of the network on the val images: 63 %\n",
        "# [37] loss: 0.339\n",
        "# Accuracy of the network on the val images: 63 %\n",
        "# [38] loss: 0.322\n",
        "# Accuracy of the network on the val images: 63 %\n",
        "# [39] loss: 0.298\n",
        "# Accuracy of the network on the val images: 63 %\n",
        "# [40] loss: 0.282\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [41] loss: 0.256\n",
        "# Accuracy of the network on the val images: 63 %\n",
        "# [42] loss: 0.248\n",
        "# Accuracy of the network on the val images: 63 %\n",
        "# [43] loss: 0.228\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [44] loss: 0.216\n",
        "# Accuracy of the network on the val images: 63 %\n",
        "# [45] loss: 0.210\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [46] loss: 0.192\n",
        "# Accuracy of the network on the val images: 63 %\n",
        "# [47] loss: 0.185\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [48] loss: 0.180\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [49] loss: 0.165\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [50] loss: 0.161\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [51] loss: 0.155\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [52] loss: 0.145\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [53] loss: 0.139\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [54] loss: 0.124\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [55] loss: 0.117\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [56] loss: 0.120\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [57] loss: 0.112\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [58] loss: 0.109\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [59] loss: 0.105\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [60] loss: 0.098\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [61] loss: 0.094\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [62] loss: 0.089\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [63] loss: 0.097\n",
        "# Accuracy of the network on the val images: 63 %\n",
        "# [64] loss: 0.086\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [65] loss: 0.084\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [66] loss: 0.082\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [67] loss: 0.080\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [68] loss: 0.071\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [69] loss: 0.070\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [70] loss: 0.069\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [71] loss: 0.067\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [72] loss: 0.066\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [73] loss: 0.061\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [74] loss: 0.061\n",
        "# Accuracy of the network on the val images: 66 %\n",
        "# [75] loss: 0.055\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [76] loss: 0.058\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [77] loss: 0.054\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [78] loss: 0.055\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [79] loss: 0.054\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [80] loss: 0.051\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [81] loss: 0.053\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [82] loss: 0.044\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [83] loss: 0.049\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [84] loss: 0.044\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [85] loss: 0.044\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [86] loss: 0.045\n",
        "# Accuracy of the network on the val images: 64 %\n",
        "# [87] loss: 0.049\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [88] loss: 0.042\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [89] loss: 0.041\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [90] loss: 0.039\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [91] loss: 0.029\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [92] loss: 0.034\n",
        "# Accuracy of the network on the val images: 66 %\n",
        "# [93] loss: 0.034\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [94] loss: 0.040\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [95] loss: 0.036\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [96] loss: 0.036\n",
        "# Accuracy of the network on the val images: 66 %\n",
        "# [97] loss: 0.035\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [98] loss: 0.039\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [99] loss: 0.038\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [100] loss: 0.035\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [101] loss: 0.031\n",
        "# Accuracy of the network on the val images: 66 %\n",
        "# [102] loss: 0.030\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [103] loss: 0.030\n",
        "# Accuracy of the network on the val images: 66 %\n",
        "# [104] loss: 0.029\n",
        "# Accuracy of the network on the val images: 66 %\n",
        "# [105] loss: 0.034\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [106] loss: 0.038\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [107] loss: 0.027\n",
        "# Accuracy of the network on the val images: 66 %\n",
        "# [108] loss: 0.029\n",
        "# Accuracy of the network on the val images: 66 %\n",
        "# [109] loss: 0.026\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [110] loss: 0.028\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [111] loss: 0.032\n",
        "# Accuracy of the network on the val images: 66 %\n",
        "# [112] loss: 0.023\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [113] loss: 0.026\n",
        "# Accuracy of the network on the val images: 66 %\n",
        "# [114] loss: 0.026\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [115] loss: 0.023\n",
        "# Accuracy of the network on the val images: 66 %\n",
        "# [116] loss: 0.024\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [117] loss: 0.025\n",
        "# Accuracy of the network on the val images: 66 %\n",
        "# [118] loss: 0.023\n",
        "# Accuracy of the network on the val images: 66 %\n",
        "# [119] loss: 0.020\n",
        "# Accuracy of the network on the val images: 65 %\n",
        "# [120] loss: 0.021\n",
        "# Accuracy of the network on the val images: 66 %\n",
        "# Finished Training\n",
        "\n",
        "# '''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "import sys\n",
        "import math\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, nChannels, growthRate):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        interChannels = 4*growthRate\n",
        "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
        "        self.conv1 = nn.Conv2d(nChannels, interChannels, kernel_size=1,\n",
        "                               bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(interChannels)\n",
        "        self.conv2 = nn.Conv2d(interChannels, growthRate, kernel_size=3,\n",
        "                               padding=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(F.relu(self.bn1(x)))\n",
        "        out = self.conv2(F.relu(self.bn2(out)))\n",
        "        out = torch.cat((x, out), 1)\n",
        "        return out\n",
        "\n",
        "class SingleLayer(nn.Module):\n",
        "    def __init__(self, nChannels, growthRate):\n",
        "        super(SingleLayer, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
        "        self.conv1 = nn.Conv2d(nChannels, growthRate, kernel_size=3,\n",
        "                               padding=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(F.relu(self.bn1(x)))\n",
        "        out = torch.cat((x, out), 1)\n",
        "        return out\n",
        "\n",
        "class Transition(nn.Module):\n",
        "    def __init__(self, nChannels, nOutChannels):\n",
        "        super(Transition, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
        "        self.conv1 = nn.Conv2d(nChannels, nOutChannels, kernel_size=1,\n",
        "                               bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(F.relu(self.bn1(x)))\n",
        "        out = F.avg_pool2d(out, 2)\n",
        "        return out\n",
        "\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "    def __init__(self, growthRate, depth, reduction, nClasses, bottleneck):\n",
        "        super(DenseNet, self).__init__()\n",
        "\n",
        "        nDenseBlocks = (depth-4) // 3\n",
        "        if bottleneck:\n",
        "            nDenseBlocks //= 2\n",
        "\n",
        "        nChannels = 2*growthRate\n",
        "        self.conv1 = nn.Conv2d(3, nChannels, kernel_size=3, padding=1,\n",
        "                               bias=False)\n",
        "        self.dense1 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
        "        nChannels += nDenseBlocks*growthRate\n",
        "        nOutChannels = int(math.floor(nChannels*reduction))\n",
        "        self.trans1 = Transition(nChannels, nOutChannels)\n",
        "\n",
        "        nChannels = nOutChannels\n",
        "        self.dense2 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
        "        nChannels += nDenseBlocks*growthRate\n",
        "        nOutChannels = int(math.floor(nChannels*reduction))\n",
        "        self.trans2 = Transition(nChannels, nOutChannels)\n",
        "\n",
        "        nChannels = nOutChannels\n",
        "        self.dense3 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
        "        nChannels += nDenseBlocks*growthRate\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
        "        self.fc = nn.Linear(nChannels, nClasses)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_dense(self, nChannels, growthRate, nDenseBlocks, bottleneck):\n",
        "        layers = []\n",
        "        for i in range(int(nDenseBlocks)):\n",
        "            if bottleneck:\n",
        "                layers.append(Bottleneck(nChannels, growthRate))\n",
        "            else:\n",
        "                layers.append(SingleLayer(nChannels, growthRate))\n",
        "            nChannels += growthRate\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.trans1(self.dense1(out))\n",
        "        out = self.trans2(self.dense2(out))\n",
        "        out = self.dense3(out)\n",
        "        out = torch.squeeze(F.avg_pool2d(F.relu(self.bn1(out)), 8))\n",
        "        out = F.log_softmax(self.fc(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class simplenet(nn.Module):\n",
        "    def __init__(self, classes=100, simpnet_name='simplenet'):\n",
        "        super(simplenet, self).__init__()\n",
        "        #print(simpnet_name)\n",
        "        self.features = self._make_layers() #self._make_layers(cfg[simpnet_name])\n",
        "        self.classifier = nn.Linear(256, classes)\n",
        "        self.drp = nn.Dropout(0.1)\n",
        "\n",
        "    def load_my_state_dict(self, state_dict):\n",
        "\n",
        "        own_state = self.state_dict()\n",
        "\n",
        "        # print(own_state.keys())\n",
        "        # for name, val in own_state:\n",
        "        # print(name)\n",
        "        for name, param in state_dict.items():\n",
        "            name = name.replace('module.', '')\n",
        "            if name not in own_state:\n",
        "                # print(name)\n",
        "                continue\n",
        "            if isinstance(param, Parameter):\n",
        "                # backwards compatibility for serialized parameters\n",
        "                param = param.data\n",
        "            print(\"STATE_DICT: {}\".format(name))\n",
        "            try:\n",
        "                own_state[name].copy_(param)\n",
        "            except:\n",
        "                print('While copying the parameter named {}, whose dimensions in the model are'\n",
        "                      ' {} and whose dimensions in the checkpoint are {}, ... Using Initial Params'.format(\n",
        "                    name, own_state[name].size(), param.size()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "\n",
        "        #Global Max Pooling\n",
        "        out = F.max_pool2d(out, kernel_size=out.size()[2:]) \n",
        "        # out = F.dropout2d(out, 0.1, training=True)\n",
        "        out = self.drp(out)\n",
        "\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layers(self):\n",
        "\n",
        "        model = nn.Sequential(\n",
        "                             nn.Conv2d(3, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)),\n",
        "                             nn.BatchNorm2d(64, eps=1e-05, momentum=0.05, affine=True),\n",
        "                             nn.ReLU(inplace=True),\n",
        "\n",
        "                             nn.Conv2d(64, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)),\n",
        "                             nn.BatchNorm2d(128, eps=1e-05, momentum=0.05, affine=True),\n",
        "                             nn.ReLU(inplace=True),\n",
        "\n",
        "                             nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)),\n",
        "                             nn.BatchNorm2d(128, eps=1e-05, momentum=0.05, affine=True),\n",
        "                             nn.ReLU(inplace=True),\n",
        "\n",
        "                             nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)),\n",
        "                             nn.BatchNorm2d(128, eps=1e-05, momentum=0.05, affine=True),\n",
        "                             nn.ReLU(inplace=True),\n",
        "\n",
        "\n",
        "                             nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False),\n",
        "                             nn.Dropout2d(p=0.1),\n",
        "\n",
        "\n",
        "                             nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)),\n",
        "                             nn.BatchNorm2d(128, eps=1e-05, momentum=0.05, affine=True),\n",
        "                             nn.ReLU(inplace=True),\n",
        "\n",
        "                             nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)),\n",
        "                             nn.BatchNorm2d(128, eps=1e-05, momentum=0.05, affine=True),\n",
        "                             nn.ReLU(inplace=True),\n",
        "\n",
        "                             nn.Conv2d(128, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)),\n",
        "                             nn.BatchNorm2d(256, eps=1e-05, momentum=0.05, affine=True),\n",
        "                             nn.ReLU(inplace=True),\n",
        "\n",
        "\n",
        "\n",
        "                             nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False),\n",
        "                             nn.Dropout2d(p=0.1),\n",
        "\n",
        "\n",
        "                             nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)),\n",
        "                             nn.BatchNorm2d(256, eps=1e-05, momentum=0.05, affine=True),\n",
        "                             nn.ReLU(inplace=True),\n",
        "\n",
        "\n",
        "                             nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)),\n",
        "                             nn.BatchNorm2d(256, eps=1e-05, momentum=0.05, affine=True),\n",
        "                             nn.ReLU(inplace=True),\n",
        "\n",
        "\n",
        "\n",
        "                             nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False),\n",
        "                             nn.Dropout2d(p=0.1),\n",
        "\n",
        "\n",
        "\n",
        "                             nn.Conv2d(256, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)),\n",
        "                             nn.BatchNorm2d(512, eps=1e-05, momentum=0.05, affine=True),\n",
        "                             nn.ReLU(inplace=True),\n",
        "\n",
        "\n",
        "\n",
        "                             nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False),\n",
        "                             nn.Dropout2d(p=0.1),\n",
        "\n",
        "\n",
        "                             nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), padding=(0, 0)),\n",
        "                             nn.BatchNorm2d(2048, eps=1e-05, momentum=0.05, affine=True),\n",
        "                             nn.ReLU(inplace=True),\n",
        "\n",
        "\n",
        "\n",
        "                             nn.Conv2d(2048, 256, kernel_size=[1, 1], stride=(1, 1), padding=(0, 0)),\n",
        "                             nn.BatchNorm2d(256, eps=1e-05, momentum=0.05, affine=True),\n",
        "                             nn.ReLU(inplace=True),\n",
        "\n",
        "\n",
        "                             nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False),\n",
        "                             nn.Dropout2d(p=0.1),\n",
        "\n",
        "\n",
        "                             nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)),\n",
        "                             nn.BatchNorm2d(256, eps=1e-05, momentum=0.05, affine=True),\n",
        "                             nn.ReLU(inplace=True),\n",
        "\n",
        "                            )\n",
        "\n",
        "        for m in model.modules():\n",
        "          if isinstance(m, nn.Conv2d):\n",
        "            nn.init.xavier_uniform_(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def resnet(**kwargs):\n",
        "#     \"\"\"\n",
        "#     Constructs a ResNet model.\n",
        "#     \"\"\"\n",
        "#     return ResNet(**kwargs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# device = args.device # \"cuda\" / \"cpu\"\n",
        "# if \"cuda\" in device and not torch.cuda.is_available():\n",
        "#     device = \"cpu\"\n",
        "# data = data.to(device)\n",
        "# model.to(device)\n",
        "def main():\n",
        "    # device = args.device # \"cuda\" / \"cpu\"\n",
        "    # if \"cuda\" in device and not torch.cuda.is_available():\n",
        "    #   device = \"cpu\"\n",
        "\n",
        "    a=torch.rand(32,3,32,32)\n",
        "    a=a.cuda()\n",
        "    # net=EdwardNet()\n",
        "    # net =DenseNet(growthRate=12, depth=100, reduction=0.5,\n",
        "    #                         bottleneck=True, nClasses=100)\n",
        "    # net=simplenet()\n",
        "    net=DenseNet(growthRate=12, depth=120, reduction=0.5,\n",
        "                            bottleneck=True, nClasses=100)\n",
        "    if IS_GPU:\n",
        "        net=net.cuda()\n",
        "\n",
        "    outputs = net(a)\n",
        "    print(a.shape)\n",
        "    print(outputs.shape)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3, 32, 32])\n",
            "torch.Size([32, 100])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:908: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################################\n",
        "# 3. Define a Loss function and optimizer\n",
        "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "# Here we use Cross-Entropy loss and SGD with momentum.\n",
        "# The CrossEntropyLoss criterion already includes softmax within its\n",
        "# implementation. That's why we don't use a softmax in our model\n",
        "# definition.\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "net=simplenet()\n",
        "if IS_GPU:\n",
        "    net=net.cuda()\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# net =DenseNet(growthRate=12, depth=100, reduction=0.5,\n",
        "#                             bottleneck=True, nClasses=100)\n",
        "\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.06,momentum=0.9, weight_decay=5e-4)\n",
        "# optimizer = torch.optim.Adadelta(net.parameters(), lr=0.1, rho=0.9, eps=1e-3, # momentum=state['momentum'],\n",
        "#                                      weight_decay=0.001)\n",
        "plt.ioff()\n",
        "fig = plt.figure()\n",
        "train_loss_over_epochs = []\n",
        "val_accuracy_over_epochs = []"
      ],
      "metadata": {
        "id": "s1tKM2VTrj_W"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def adjust_opt(optAlg, optimizer, epoch):\n",
        "    if optAlg == 'sgd':\n",
        "        if epoch < 150: lr = 1e-1\n",
        "        elif epoch == 150: lr = 1e-2\n",
        "        elif epoch == 225: lr = 1e-3\n",
        "        else: return\n",
        "\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr"
      ],
      "metadata": {
        "id": "sQcZFxkN8Q0K"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this block is just for recording anther try of using different model\n",
        "\n",
        "#well, the simple net converge at 65%, so this is useless\n",
        "\n",
        "#***************ignore this block\n",
        "\n",
        "# '''\n",
        "\n",
        "# '''\n",
        "\n",
        "# checkpoint_save_dir='/weights/'\n",
        "\n",
        "# CHECKPOINT_SAVE_EPOCH_INTERVAL=30\n",
        "# if (not os.path.exists(checkpoint_save_dir)):\n",
        "#         os.makedirs(checkpoint_save_dir)\n",
        "#         print(f\"Created checkpoint save dir `{checkpoint_save_dir}`.\")\n",
        "# elif (os.path.isdir(checkpoint_save_dir)):\n",
        "#         print(f\"Using existing checkpoint save dir `{checkpoint_save_dir}`.\")\n",
        "#         existing_filenames = os.listdir(checkpoint_save_dir)\n",
        "#         if existing_filenames:\n",
        "#             print(f\"Existing checkpoint files in this directory will be overwritten.\")\n",
        "# else:\n",
        "#         raise FileExistsError(f\"Checkpoint save dir `{checkpoint_save_dir}` is not a folder.\")   \n",
        "# ########################################################################\n",
        "# # 4. Train the network\n",
        "# # ^^^^^^^^^^^^^^^^^^^^\n",
        "# #\n",
        "# # We simply have to loop over our data iterator, and feed the inputs to the\n",
        "# # network and optimize. We evaluate the validation accuracy at each\n",
        "# # epoch and plot these values over the number of epochs\n",
        "# # Nothing to change here\n",
        "# # -----------------------------\n",
        "# train_loss_visual=[]\n",
        "# valid_loss_visual=[]\n",
        "\n",
        "\n",
        "# VAL_EPOCH_INTERVAL=1\n",
        "\n",
        "# # for epoch in range(EPOCHS):  # loop over the dataset multiple times\n",
        "\n",
        "# #     running_loss = 0.0\n",
        "# #     for i, data in enumerate(trainloader, 0):\n",
        "# #         # get the inputs\n",
        "# #         inputs, labels = data\n",
        "\n",
        "# #         if IS_GPU:\n",
        "# #             inputs = inputs.cuda()\n",
        "# #             labels = labels.cuda()\n",
        "\n",
        "# #         # wrap them in Variable\n",
        "# #         inputs, labels = Variable(inputs), Variable(labels)\n",
        "\n",
        "# #         # zero the parameter gradients\n",
        "# #         optimizer.zero_grad()\n",
        "\n",
        "# #         # forward + backward + optimize\n",
        "# #         outputs = net(inputs)\n",
        "# #         loss = criterion(outputs, labels)\n",
        "# #         loss.backward()\n",
        "# #         optimizer.step()\n",
        "\n",
        "# #         # print statistics\n",
        "# #         running_loss += loss.item()\n",
        "    \n",
        "# #     # Normalizing the loss by the total number of train batches\n",
        "# #     running_loss/=len(trainloader)\n",
        "# #     print('[%d] loss: %.3f' %\n",
        "# #           (epoch + 1, running_loss))\n",
        "\n",
        "# #     # Scale of 0.0 to 100.0\n",
        "# #     # Calculate validation set accuracy of the existing model\n",
        "# #     val_accuracy, val_classwise_accuracy = \n",
        "# #         calculate_val_accuracy(valloader, IS_GPU)\n",
        "# #     print('Accuracy of the network on the val images: %d %%' % (val_accuracy))\n",
        "\n",
        "# #     # # Optionally print classwise accuracies\n",
        "# #     # for c_i in range(TOTAL_CLASSES):\n",
        "# #     #     print('Accuracy of %5s : %2d %%' % (\n",
        "# #     #         classes[c_i], 100 * val_classwise_accuracy[c_i]))\n",
        "\n",
        "# #     train_loss_over_epochs.append(running_loss)\n",
        "# #     val_accuracy_over_epochs.append(val_accuracy)\n",
        "# # # -----------------------------\n",
        "# #     # MARK: Save checkpoint\n",
        "# #     if ((epoch % CHECKPOINT_SAVE_EPOCH_INTERVAL) == 0):\n",
        "# #         checkpoint_filename = os.path.join(checkpoint_save_dir, f\"{epoch}.pth\")\n",
        "# #         torch.save(net.state_dict(), checkpoint_filename)\n",
        "# #         print(f\"Checkpoint saved as `{checkpoint_filename}`\")\n",
        "\n",
        "\n",
        "# trainloss=[]\n",
        "# validloss=[]\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# for epoch in range(EPOCHS):  # loop over the dataset multiple times\n",
        "#         net.train()\n",
        "#         # running_loss = 0.0\n",
        "#         running_loss = 0.0\n",
        "#         train_loss=0.\n",
        "#         train_count=0\n",
        "#         train_correct_count = 0\n",
        "#         for image, label in trainloader:\n",
        "\n",
        "#             if IS_GPU:\n",
        "#                 image = image.cuda()\n",
        "#                 label = label.cuda()\n",
        "#             image, label = Variable(image), Variable(label)\n",
        "#             # zero the parameter gradients\n",
        "#             optimizer.zero_grad()\n",
        "#             # forward + backward + optimize\n",
        "#             net=net.cuda()\n",
        "\n",
        "#             outputs = net(image)\n",
        "       \n",
        "#             loss = criterion(outputs, label)\n",
        "\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#             # print statistics\n",
        "#         #     running_loss += loss.item()\n",
        "#         # running_loss/=len(trainloader)\n",
        "#         # print('[%d] loss: %.3f' %\n",
        "#         # (epoch + 1, running_loss))\n",
        "#         # train_loss_visual.append(running_loss)\n",
        "\n",
        "#             train_loss += loss.item()\n",
        "#             train_count += image.shape[0]\n",
        "\n",
        "#             _, max_prediction_indices = torch.max(outputs, -1)\n",
        "#             train_correct_count += torch.sum(max_prediction_indices == label).item()\n",
        "\n",
        "#         print(f\"Epoch {epoch}: accuracy: {train_correct_count / train_count}, total loss: {train_loss}, average loss: {train_loss / train_count}\")\n",
        "#         train_loss_visual.append(train_loss)\n",
        "#         trainloss.append(train_loss)\n",
        "        \n",
        "#         # Normalizing the loss by the total number of train batches\n",
        "#         # running_loss/=len(trainloader)\n",
        "#         # print('[%d] loss: %.3f' %\n",
        "#         #     (epoch + 1, running_loss))\n",
        "\n",
        "#         # Scale of 0.0 to 100.0\n",
        "#         # Calculate validation set accuracy of the existing model\n",
        "#         # val_accuracy, val_classwise_accuracy = \\\n",
        "#         #     calculate_val_accuracy(valloader, IS_GPU)\n",
        "#         # print('Accuracy of the network on the val images: %d %%' % (val_accuracy))\n",
        "\n",
        "#         # # Optionally print classwise accuracies\n",
        "#         # for c_i in range(TOTAL_CLASSES):\n",
        "#         #     print('Accuracy of %5s : %2d %%' % (\n",
        "#         #         classes[c_i], 100 * val_classwise_accuracy[c_i]))\n",
        "#         if (epoch % VAL_EPOCH_INTERVAL == 0):\n",
        "           \n",
        "#             net.eval()\n",
        "#             validation_loss = 0.\n",
        "#             validation_count = 0\n",
        "#             validation_correct_count = 0\n",
        "#             with torch.no_grad():    # [Very important] Reduce memory usage.\n",
        "#                 for image, label in valloader :\n",
        "#                     image = image.cuda()\n",
        "#                     label = label.cuda()\n",
        "\n",
        "#                     prediction = net(image)\n",
        "#                     loss = criterion(prediction, label)\n",
        "#                     validation_loss += loss.item()\n",
        "#                     validation_count += image.shape[0]\n",
        "\n",
        "#                     _, max_prediction_indices = torch.max(prediction, -1)\n",
        "#                     validation_correct_count += torch.sum(max_prediction_indices == label).item()\n",
        "\n",
        "       \n",
        "#             print(f\"Validation: accuracy: {validation_correct_count / validation_count}, total loss: {validation_loss}, average loss: {validation_loss / validation_count}\")\n",
        "#             valid_loss_visual.append(validation_loss)\n",
        "#             validloss.append(validation_loss)\n",
        "#               # Calculate validation set accuracy of the existing model\n",
        "#             # temp=calculate_val_accuracy(valloader, IS_GPU)\n",
        "#             # val_accuracy, val_classwise_accuracy = temp\n",
        "#             # print('Accuracy of the network on the val images: %d %%' % (val_accuracy))\n",
        "\n",
        "#     # # # Optionally print classwise accuracies\n",
        "#     #     train_loss_over_epochs.append(running_loss)\n",
        "#     #     val_accuracy_over_epochs.append(val_accuracy)\n",
        "\n",
        "\n",
        "#     # -----------------------------\n",
        "#         # MARK: Save checkpoint\n",
        "#         if ((epoch % CHECKPOINT_SAVE_EPOCH_INTERVAL) == 0):\n",
        "        \n",
        "#             checkpoint_filename = os.path.join(checkpoint_save_dir, f\"{epoch}.pth\")\n",
        "#             torch.save(net.state_dict(), checkpoint_filename)\n",
        "#             print(f\"Checkpoint saved as `{checkpoint_filename}`\")\n",
        "\n",
        "        \n",
        "\n",
        "# trainlossnp=np.array(trainloss)\n",
        "# validlossnp=np.array(validloss)\n",
        "# plt.plot(trainlossnp)\n",
        "# plt.plot(validlossnp)\n",
        "# plt.show()\n",
        "# plt.savefig('different_loss.png')\n",
        "\n",
        "\n",
        "# # Plot train loss over epochs and val set accuracy over epochs\n",
        "# # Nothing to change here\n",
        "# # -------------\n",
        "# # plt.subplot(2, 1, 1)\n",
        "# # plt.ylabel('Train loss')\n",
        "# # plt.plot(np.arange(EPOCHS), train_loss_over_epochs, 'k-')\n",
        "# # plt.title('train loss and val accuracy')\n",
        "# # plt.xticks(np.arange(EPOCHS, dtype=int))\n",
        "# # plt.grid(True)\n",
        "\n",
        "# # plt.subplot(2, 1, 2)\n",
        "# # val_accuracy_over_epochs = torch.tensor(val_accuracy_over_epochs, device = 'cpu')\n",
        "# # plt.plot(np.arange(EPOCHS), val_accuracy_over_epochs, 'b-')\n",
        "# # plt.ylabel('Val accuracy')\n",
        "# # plt.xlabel('Epochs')\n",
        "# # plt.xticks(np.arange(EPOCHS, dtype=int))\n",
        "# # plt.grid(True)\n",
        "# # plt.savefig(\"plot.png\")\n",
        "# # plt.close(fig)\n",
        "# # print('Finished Training')\n",
        "# # -------------\n"
      ],
      "metadata": {
        "id": "U3pDbrMMFzU0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4fafb20b-cad3-4971-dbef-74f0458357ea"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created checkpoint save dir `/weights/`.\n",
            "Epoch 0: accuracy: 0.08186666666666667, total loss: 1420.4041509628296, average loss: 0.03156453668806288\n",
            "Validation: accuracy: 0.1186, total loss: 148.92884588241577, average loss: 0.029785769176483155\n",
            "Checkpoint saved as `/weights/0.pth`\n",
            "Epoch 1: accuracy: 0.15344444444444444, total loss: 1245.875774383545, average loss: 0.02768612831963433\n",
            "Validation: accuracy: 0.1866, total loss: 135.34211897850037, average loss: 0.027068423795700073\n",
            "Epoch 2: accuracy: 0.20537777777777777, total loss: 1142.3123931884766, average loss: 0.025384719848632813\n",
            "Validation: accuracy: 0.2516, total loss: 118.55835604667664, average loss: 0.023711671209335327\n",
            "Epoch 3: accuracy: 0.2510222222222222, total loss: 1053.6407656669617, average loss: 0.023414239237043593\n",
            "Validation: accuracy: 0.2772, total loss: 113.32902264595032, average loss: 0.022665804529190064\n",
            "Epoch 4: accuracy: 0.29131111111111113, total loss: 974.4333233833313, average loss: 0.02165407385296292\n",
            "Validation: accuracy: 0.302, total loss: 107.5762505531311, average loss: 0.02151525011062622\n",
            "Epoch 5: accuracy: 0.3268666666666667, total loss: 910.0914273262024, average loss: 0.020224253940582276\n",
            "Validation: accuracy: 0.3326, total loss: 104.76379179954529, average loss: 0.02095275835990906\n",
            "Epoch 6: accuracy: 0.36035555555555554, total loss: 852.5792970657349, average loss: 0.018946206601460774\n",
            "Validation: accuracy: 0.396, total loss: 89.62279891967773, average loss: 0.017924559783935548\n",
            "Epoch 7: accuracy: 0.3920888888888889, total loss: 802.3298047780991, average loss: 0.01782955121729109\n",
            "Validation: accuracy: 0.4268, total loss: 86.29454004764557, average loss: 0.017258908009529113\n",
            "Epoch 8: accuracy: 0.4209333333333333, total loss: 757.3264741897583, average loss: 0.01682947720421685\n",
            "Validation: accuracy: 0.4386, total loss: 82.89564168453217, average loss: 0.016579128336906432\n",
            "Epoch 9: accuracy: 0.44137777777777776, total loss: 722.5765966176987, average loss: 0.016057257702615527\n",
            "Validation: accuracy: 0.46, total loss: 79.59789097309113, average loss: 0.015919578194618225\n",
            "Epoch 10: accuracy: 0.46053333333333335, total loss: 690.4315642118454, average loss: 0.01534292364915212\n",
            "Validation: accuracy: 0.4636, total loss: 77.8877182006836, average loss: 0.015577543640136719\n",
            "Epoch 11: accuracy: 0.48644444444444446, total loss: 658.5932065248489, average loss: 0.014635404589441087\n",
            "Validation: accuracy: 0.5024, total loss: 71.14310467243195, average loss: 0.014228620934486388\n",
            "Epoch 12: accuracy: 0.5027777777777778, total loss: 634.818244934082, average loss: 0.014107072109646268\n",
            "Validation: accuracy: 0.5174, total loss: 68.29696321487427, average loss: 0.013659392642974853\n",
            "Epoch 13: accuracy: 0.5166666666666667, total loss: 610.7164310216904, average loss: 0.013571476244926453\n",
            "Validation: accuracy: 0.519, total loss: 69.40885102748871, average loss: 0.013881770205497742\n",
            "Epoch 14: accuracy: 0.5332222222222223, total loss: 588.9011870622635, average loss: 0.013086693045828077\n",
            "Validation: accuracy: 0.5132, total loss: 68.92238056659698, average loss: 0.013784476113319398\n",
            "Epoch 15: accuracy: 0.5462666666666667, total loss: 568.8247456550598, average loss: 0.012640549903445774\n",
            "Validation: accuracy: 0.514, total loss: 71.77841353416443, average loss: 0.014355682706832885\n",
            "Epoch 16: accuracy: 0.5578444444444445, total loss: 551.0813112258911, average loss: 0.012246251360575357\n",
            "Validation: accuracy: 0.5298, total loss: 68.23352527618408, average loss: 0.013646705055236817\n",
            "Epoch 17: accuracy: 0.5716444444444444, total loss: 533.8455984592438, average loss: 0.011863235521316528\n",
            "Validation: accuracy: 0.5318, total loss: 67.4633948802948, average loss: 0.01349267897605896\n",
            "Epoch 18: accuracy: 0.5815555555555556, total loss: 519.5636049509048, average loss: 0.011545857887797885\n",
            "Validation: accuracy: 0.5518, total loss: 64.27234852313995, average loss: 0.01285446970462799\n",
            "Epoch 19: accuracy: 0.5910222222222222, total loss: 505.6146088838577, average loss: 0.01123588019741906\n",
            "Validation: accuracy: 0.5546, total loss: 64.9178991317749, average loss: 0.012983579826354981\n",
            "Epoch 20: accuracy: 0.6036222222222222, total loss: 492.2400361299515, average loss: 0.010938667469554478\n",
            "Validation: accuracy: 0.5672, total loss: 62.6853814125061, average loss: 0.012537076282501221\n",
            "Epoch 21: accuracy: 0.6106444444444444, total loss: 477.43319272994995, average loss: 0.01060962650511\n",
            "Validation: accuracy: 0.5926, total loss: 57.63367772102356, average loss: 0.011526735544204712\n",
            "Epoch 22: accuracy: 0.6192222222222222, total loss: 465.14041447639465, average loss: 0.010336453655030992\n",
            "Validation: accuracy: 0.601, total loss: 56.316842555999756, average loss: 0.011263368511199951\n",
            "Epoch 23: accuracy: 0.6334222222222222, total loss: 451.67768293619156, average loss: 0.010037281843026479\n",
            "Validation: accuracy: 0.5804, total loss: 58.29797524213791, average loss: 0.011659595048427581\n",
            "Epoch 24: accuracy: 0.6392222222222222, total loss: 439.7336627840996, average loss: 0.00977185917297999\n",
            "Validation: accuracy: 0.5472, total loss: 67.7911194562912, average loss: 0.01355822389125824\n",
            "Epoch 25: accuracy: 0.6465333333333333, total loss: 430.93235087394714, average loss: 0.009576274463865492\n",
            "Validation: accuracy: 0.6044, total loss: 57.493998765945435, average loss: 0.011498799753189086\n",
            "Epoch 26: accuracy: 0.6561555555555556, total loss: 418.22549271583557, average loss: 0.00929389983812968\n",
            "Validation: accuracy: 0.567, total loss: 63.75835907459259, average loss: 0.012751671814918517\n",
            "Epoch 27: accuracy: 0.6603555555555556, total loss: 413.3907005786896, average loss: 0.009186460012859768\n",
            "Validation: accuracy: 0.5894, total loss: 60.07819592952728, average loss: 0.012015639185905456\n",
            "Epoch 28: accuracy: 0.6677111111111111, total loss: 402.91430324316025, average loss: 0.008953651183181338\n",
            "Validation: accuracy: 0.58, total loss: 62.375280141830444, average loss: 0.012475056028366089\n",
            "Epoch 29: accuracy: 0.6727333333333333, total loss: 394.9121986627579, average loss: 0.008775826636950175\n",
            "Validation: accuracy: 0.6098, total loss: 56.81125509738922, average loss: 0.011362251019477845\n",
            "Epoch 30: accuracy: 0.6804888888888889, total loss: 387.27160584926605, average loss: 0.008606035685539245\n",
            "Validation: accuracy: 0.58, total loss: 61.74847340583801, average loss: 0.012349694681167603\n",
            "Checkpoint saved as `/weights/30.pth`\n",
            "Epoch 31: accuracy: 0.6839111111111111, total loss: 380.6864560842514, average loss: 0.008459699024094476\n",
            "Validation: accuracy: 0.5824, total loss: 60.1343435049057, average loss: 0.01202686870098114\n",
            "Epoch 32: accuracy: 0.6886, total loss: 376.6819417476654, average loss: 0.008370709816614787\n",
            "Validation: accuracy: 0.5946, total loss: 58.999298453330994, average loss: 0.011799859690666199\n",
            "Epoch 33: accuracy: 0.6945777777777777, total loss: 367.8457868695259, average loss: 0.008174350819322797\n",
            "Validation: accuracy: 0.6002, total loss: 59.10720682144165, average loss: 0.01182144136428833\n",
            "Epoch 34: accuracy: 0.6960444444444445, total loss: 364.05432826280594, average loss: 0.00809009618361791\n",
            "Validation: accuracy: 0.6212, total loss: 54.063688933849335, average loss: 0.010812737786769867\n",
            "Epoch 35: accuracy: 0.7050444444444445, total loss: 356.5188564658165, average loss: 0.007922641254795922\n",
            "Validation: accuracy: 0.6364, total loss: 53.144906878471375, average loss: 0.010628981375694275\n",
            "Epoch 36: accuracy: 0.7071777777777778, total loss: 352.89202386140823, average loss: 0.00784204497469796\n",
            "Validation: accuracy: 0.6208, total loss: 53.488646388053894, average loss: 0.01069772927761078\n",
            "Epoch 37: accuracy: 0.7137111111111111, total loss: 346.43643832206726, average loss: 0.007698587518268162\n",
            "Validation: accuracy: 0.5784, total loss: 63.48159050941467, average loss: 0.012696318101882934\n",
            "Epoch 38: accuracy: 0.7140666666666666, total loss: 344.3929064273834, average loss: 0.007653175698386298\n",
            "Validation: accuracy: 0.6258, total loss: 54.58010399341583, average loss: 0.010916020798683167\n",
            "Epoch 39: accuracy: 0.7198222222222223, total loss: 337.1835178732872, average loss: 0.007492967063850827\n",
            "Validation: accuracy: 0.6076, total loss: 57.03705835342407, average loss: 0.011407411670684814\n",
            "Epoch 40: accuracy: 0.7232, total loss: 333.81663727760315, average loss: 0.007418147495057848\n",
            "Validation: accuracy: 0.5862, total loss: 60.14256101846695, average loss: 0.01202851220369339\n",
            "Epoch 41: accuracy: 0.7228888888888889, total loss: 332.1433163881302, average loss: 0.007380962586402893\n",
            "Validation: accuracy: 0.5866, total loss: 62.05026459693909, average loss: 0.012410052919387817\n",
            "Epoch 42: accuracy: 0.7261111111111112, total loss: 326.1265484690666, average loss: 0.007247256632645925\n",
            "Validation: accuracy: 0.6324, total loss: 54.26028257608414, average loss: 0.010852056515216828\n",
            "Epoch 43: accuracy: 0.7309777777777777, total loss: 321.4709673523903, average loss: 0.007143799274497562\n",
            "Validation: accuracy: 0.6046, total loss: 60.39903008937836, average loss: 0.012079806017875672\n",
            "Epoch 44: accuracy: 0.7328222222222223, total loss: 318.42067247629166, average loss: 0.007076014943917593\n",
            "Validation: accuracy: 0.6298, total loss: 53.68294417858124, average loss: 0.010736588835716248\n",
            "Epoch 45: accuracy: 0.7373111111111111, total loss: 313.235467672348, average loss: 0.006960788170496623\n",
            "Validation: accuracy: 0.6044, total loss: 61.089001417160034, average loss: 0.012217800283432006\n",
            "Epoch 46: accuracy: 0.7379777777777777, total loss: 313.64973253011703, average loss: 0.006969994056224823\n",
            "Validation: accuracy: 0.632, total loss: 54.84106516838074, average loss: 0.010968213033676148\n",
            "Epoch 47: accuracy: 0.7404666666666667, total loss: 310.10654443502426, average loss: 0.006891256543000539\n",
            "Validation: accuracy: 0.5904, total loss: 62.34471094608307, average loss: 0.012468942189216613\n",
            "Epoch 48: accuracy: 0.7436444444444444, total loss: 306.59899389743805, average loss: 0.006813310975498623\n",
            "Validation: accuracy: 0.6118, total loss: 57.16657900810242, average loss: 0.011433315801620483\n",
            "Epoch 49: accuracy: 0.7444666666666667, total loss: 305.7628204226494, average loss: 0.006794729342725542\n",
            "Validation: accuracy: 0.6356, total loss: 53.386971175670624, average loss: 0.010677394235134125\n",
            "Epoch 50: accuracy: 0.7487333333333334, total loss: 300.48087698221207, average loss: 0.006677352821826935\n",
            "Validation: accuracy: 0.6062, total loss: 60.67217695713043, average loss: 0.012134435391426087\n",
            "Epoch 51: accuracy: 0.7548222222222222, total loss: 294.91864240169525, average loss: 0.0065537476089265615\n",
            "Validation: accuracy: 0.6406, total loss: 52.13384687900543, average loss: 0.010426769375801087\n",
            "Epoch 52: accuracy: 0.7538666666666667, total loss: 294.5891623497009, average loss: 0.006546425829993354\n",
            "Validation: accuracy: 0.6166, total loss: 58.216333508491516, average loss: 0.011643266701698303\n",
            "Epoch 53: accuracy: 0.7533333333333333, total loss: 294.03187504410744, average loss: 0.006534041667646832\n",
            "Validation: accuracy: 0.626, total loss: 55.36064910888672, average loss: 0.011072129821777344\n",
            "Epoch 54: accuracy: 0.7546444444444445, total loss: 292.1756702065468, average loss: 0.006492792671256595\n",
            "Validation: accuracy: 0.6128, total loss: 59.0067383646965, average loss: 0.0118013476729393\n",
            "Epoch 55: accuracy: 0.7603555555555556, total loss: 285.1598091125488, average loss: 0.006336884646945529\n",
            "Validation: accuracy: 0.613, total loss: 57.475844383239746, average loss: 0.011495168876647949\n",
            "Epoch 56: accuracy: 0.7632, total loss: 285.0774631500244, average loss: 0.0063350547366672096\n",
            "Validation: accuracy: 0.6288, total loss: 56.12087309360504, average loss: 0.011224174618721008\n",
            "Epoch 57: accuracy: 0.7637555555555555, total loss: 281.2287977039814, average loss: 0.006249528837866254\n",
            "Validation: accuracy: 0.6304, total loss: 54.05526000261307, average loss: 0.010811052000522613\n",
            "Epoch 58: accuracy: 0.7673777777777778, total loss: 279.3180310726166, average loss: 0.006207067357169257\n",
            "Validation: accuracy: 0.6316, total loss: 55.34989786148071, average loss: 0.011069979572296143\n",
            "Epoch 59: accuracy: 0.7683555555555556, total loss: 276.6997299194336, average loss: 0.0061488828870985245\n",
            "Validation: accuracy: 0.659, total loss: 50.94682091474533, average loss: 0.010189364182949067\n",
            "Epoch 60: accuracy: 0.7681555555555556, total loss: 274.9459321498871, average loss: 0.006109909603330824\n",
            "Validation: accuracy: 0.623, total loss: 57.8115239739418, average loss: 0.01156230479478836\n",
            "Checkpoint saved as `/weights/60.pth`\n",
            "Epoch 61: accuracy: 0.7699555555555555, total loss: 274.3372440338135, average loss: 0.006096383200751411\n",
            "Validation: accuracy: 0.5894, total loss: 67.25822949409485, average loss: 0.01345164589881897\n",
            "Epoch 62: accuracy: 0.7664222222222222, total loss: 276.35739001631737, average loss: 0.006141275333695941\n",
            "Validation: accuracy: 0.6282, total loss: 53.692115902900696, average loss: 0.01073842318058014\n",
            "Epoch 63: accuracy: 0.7703777777777778, total loss: 273.12998229265213, average loss: 0.0060695551620589365\n",
            "Validation: accuracy: 0.6338, total loss: 55.342976093292236, average loss: 0.011068595218658448\n",
            "Epoch 64: accuracy: 0.7730444444444444, total loss: 271.59567952156067, average loss: 0.006035459544923571\n",
            "Validation: accuracy: 0.6208, total loss: 56.88165456056595, average loss: 0.01137633091211319\n",
            "Epoch 65: accuracy: 0.7755333333333333, total loss: 267.74861174821854, average loss: 0.0059499691499604115\n",
            "Validation: accuracy: 0.6724, total loss: 47.72753405570984, average loss: 0.009545506811141968\n",
            "Epoch 66: accuracy: 0.7773555555555556, total loss: 267.86003279685974, average loss: 0.00595244517326355\n",
            "Validation: accuracy: 0.6098, total loss: 58.616876006126404, average loss: 0.01172337520122528\n",
            "Epoch 67: accuracy: 0.7794888888888889, total loss: 261.0333503782749, average loss: 0.005800741119517221\n",
            "Validation: accuracy: 0.6322, total loss: 56.47262680530548, average loss: 0.011294525361061096\n",
            "Epoch 68: accuracy: 0.7794, total loss: 264.5796329379082, average loss: 0.005879547398620182\n",
            "Validation: accuracy: 0.6402, total loss: 54.52870488166809, average loss: 0.010905740976333619\n",
            "Epoch 69: accuracy: 0.7783111111111111, total loss: 264.2675543129444, average loss: 0.005872612318065432\n",
            "Validation: accuracy: 0.6008, total loss: 60.80138498544693, average loss: 0.012160276997089386\n",
            "Epoch 70: accuracy: 0.7814, total loss: 261.8796780705452, average loss: 0.005819548401567671\n",
            "Validation: accuracy: 0.6358, total loss: 55.177196979522705, average loss: 0.011035439395904541\n",
            "Epoch 71: accuracy: 0.7822222222222223, total loss: 259.9795979857445, average loss: 0.0057773243996832105\n",
            "Validation: accuracy: 0.592, total loss: 63.529879689216614, average loss: 0.012705975937843323\n",
            "Epoch 72: accuracy: 0.7842, total loss: 256.5221658349037, average loss: 0.005700492574108972\n",
            "Validation: accuracy: 0.6312, total loss: 54.773566365242004, average loss: 0.010954713273048401\n",
            "Epoch 73: accuracy: 0.7845777777777778, total loss: 257.7932557761669, average loss: 0.0057287390172481535\n",
            "Validation: accuracy: 0.6124, total loss: 59.709452748298645, average loss: 0.01194189054965973\n",
            "Epoch 74: accuracy: 0.7887777777777778, total loss: 252.48906156420708, average loss: 0.005610868034760157\n",
            "Validation: accuracy: 0.6224, total loss: 57.355171620845795, average loss: 0.011471034324169158\n",
            "Epoch 75: accuracy: 0.7867555555555555, total loss: 253.0739741921425, average loss: 0.005623866093158722\n",
            "Validation: accuracy: 0.6382, total loss: 55.292592883110046, average loss: 0.011058518576622009\n",
            "Epoch 76: accuracy: 0.7892222222222223, total loss: 251.04075214266777, average loss: 0.005578683380948172\n",
            "Validation: accuracy: 0.6136, total loss: 60.69378811120987, average loss: 0.012138757622241973\n",
            "Epoch 77: accuracy: 0.7889333333333334, total loss: 252.29945197701454, average loss: 0.005606654488378101\n",
            "Validation: accuracy: 0.638, total loss: 54.26642894744873, average loss: 0.010853285789489747\n",
            "Epoch 78: accuracy: 0.7918666666666667, total loss: 248.58155140280724, average loss: 0.005524034475617939\n",
            "Validation: accuracy: 0.6424, total loss: 53.56645089387894, average loss: 0.010713290178775788\n",
            "Epoch 79: accuracy: 0.7928888888888889, total loss: 248.73242995142937, average loss: 0.0055273873322539855\n",
            "Validation: accuracy: 0.6414, total loss: 53.16556692123413, average loss: 0.010633113384246827\n",
            "Epoch 80: accuracy: 0.7956444444444445, total loss: 244.0740307867527, average loss: 0.005423867350816727\n",
            "Validation: accuracy: 0.6656, total loss: 50.12500184774399, average loss: 0.010025000369548797\n",
            "Epoch 81: accuracy: 0.7904, total loss: 248.45403254032135, average loss: 0.005521200723118252\n",
            "Validation: accuracy: 0.6436, total loss: 53.36426377296448, average loss: 0.010672852754592895\n",
            "Epoch 82: accuracy: 0.7932, total loss: 247.1471512913704, average loss: 0.0054921589175860085\n",
            "Validation: accuracy: 0.6526, total loss: 52.659828543663025, average loss: 0.010531965708732605\n",
            "Epoch 83: accuracy: 0.7920222222222222, total loss: 247.83130076527596, average loss: 0.005507362239228354\n",
            "Validation: accuracy: 0.6244, total loss: 58.23613917827606, average loss: 0.011647227835655213\n",
            "Epoch 84: accuracy: 0.7909333333333334, total loss: 247.1502537727356, average loss: 0.0054922278616163465\n",
            "Validation: accuracy: 0.6186, total loss: 58.62915802001953, average loss: 0.011725831604003907\n",
            "Epoch 85: accuracy: 0.7985333333333333, total loss: 240.57869112491608, average loss: 0.005346193136109246\n",
            "Validation: accuracy: 0.622, total loss: 59.74299895763397, average loss: 0.011948599791526794\n",
            "Epoch 86: accuracy: 0.7962, total loss: 241.5834810435772, average loss: 0.005368521800968382\n",
            "Validation: accuracy: 0.643, total loss: 54.913050174713135, average loss: 0.010982610034942627\n",
            "Epoch 87: accuracy: 0.7989555555555555, total loss: 241.66433641314507, average loss: 0.005370318586958779\n",
            "Validation: accuracy: 0.6388, total loss: 55.41509675979614, average loss: 0.011083019351959229\n",
            "Epoch 88: accuracy: 0.7989111111111111, total loss: 241.62146684527397, average loss: 0.005369365929894977\n",
            "Validation: accuracy: 0.6256, total loss: 57.291985869407654, average loss: 0.011458397173881531\n",
            "Epoch 89: accuracy: 0.8020666666666667, total loss: 236.0030560195446, average loss: 0.00524451235598988\n",
            "Validation: accuracy: 0.6464, total loss: 54.329625606536865, average loss: 0.010865925121307372\n",
            "Epoch 90: accuracy: 0.7997777777777778, total loss: 236.031243622303, average loss: 0.005245138747162289\n",
            "Validation: accuracy: 0.632, total loss: 56.75785821676254, average loss: 0.011351571643352508\n",
            "Checkpoint saved as `/weights/90.pth`\n",
            "Epoch 91: accuracy: 0.8011111111111111, total loss: 238.62623271346092, average loss: 0.005302805171410243\n",
            "Validation: accuracy: 0.6218, total loss: 60.41587221622467, average loss: 0.012083174443244934\n",
            "Epoch 92: accuracy: 0.8027333333333333, total loss: 236.632109105587, average loss: 0.005258491313457489\n",
            "Validation: accuracy: 0.6452, total loss: 55.783486008644104, average loss: 0.011156697201728821\n",
            "Epoch 93: accuracy: 0.7999777777777778, total loss: 237.1982107758522, average loss: 0.0052710713505744935\n",
            "Validation: accuracy: 0.6626, total loss: 51.38563621044159, average loss: 0.010277127242088319\n",
            "Epoch 94: accuracy: 0.8020888888888889, total loss: 236.0626138150692, average loss: 0.005245835862557093\n",
            "Validation: accuracy: 0.66, total loss: 51.86135518550873, average loss: 0.010372271037101746\n",
            "Epoch 95: accuracy: 0.8042, total loss: 234.3429345190525, average loss: 0.005207620767090056\n",
            "Validation: accuracy: 0.6076, total loss: 61.67019426822662, average loss: 0.012334038853645325\n",
            "Epoch 96: accuracy: 0.8021555555555555, total loss: 235.01021486520767, average loss: 0.005222449219226837\n",
            "Validation: accuracy: 0.664, total loss: 49.41330337524414, average loss: 0.009882660675048827\n",
            "Epoch 97: accuracy: 0.8021555555555555, total loss: 236.86079761385918, average loss: 0.005263573280307981\n",
            "Validation: accuracy: 0.644, total loss: 54.677897930145264, average loss: 0.010935579586029053\n",
            "Epoch 98: accuracy: 0.8036, total loss: 233.15182867646217, average loss: 0.005181151748365826\n",
            "Validation: accuracy: 0.6298, total loss: 55.82820928096771, average loss: 0.011165641856193542\n",
            "Epoch 99: accuracy: 0.8070888888888889, total loss: 228.94346061348915, average loss: 0.005087632458077537\n",
            "Validation: accuracy: 0.6636, total loss: 51.20390748977661, average loss: 0.010240781497955322\n",
            "Epoch 100: accuracy: 0.8079111111111111, total loss: 229.20099544525146, average loss: 0.0050933554543389215\n",
            "Validation: accuracy: 0.6526, total loss: 53.969421565532684, average loss: 0.010793884313106537\n",
            "Epoch 101: accuracy: 0.8084666666666667, total loss: 228.02726337313652, average loss: 0.005067272519403034\n",
            "Validation: accuracy: 0.649, total loss: 55.40262413024902, average loss: 0.011080524826049805\n",
            "Epoch 102: accuracy: 0.8037555555555556, total loss: 233.56452852487564, average loss: 0.005190322856108347\n",
            "Validation: accuracy: 0.6292, total loss: 59.2342369556427, average loss: 0.01184684739112854\n",
            "Epoch 103: accuracy: 0.8091111111111111, total loss: 227.28933635354042, average loss: 0.005050874141189787\n",
            "Validation: accuracy: 0.6382, total loss: 57.33696961402893, average loss: 0.011467393922805786\n",
            "Epoch 104: accuracy: 0.8056888888888889, total loss: 231.90945717692375, average loss: 0.0051535434928205275\n",
            "Validation: accuracy: 0.656, total loss: 54.03013098239899, average loss: 0.010806026196479798\n",
            "Epoch 105: accuracy: 0.8119777777777778, total loss: 225.8760548233986, average loss: 0.005019467884964413\n",
            "Validation: accuracy: 0.6676, total loss: 51.275057792663574, average loss: 0.010255011558532715\n",
            "Epoch 106: accuracy: 0.8108, total loss: 225.343423306942, average loss: 0.0050076316290431555\n",
            "Validation: accuracy: 0.6622, total loss: 51.4118669629097, average loss: 0.01028237339258194\n",
            "Epoch 107: accuracy: 0.8117111111111112, total loss: 225.3626430928707, average loss: 0.005008058735397127\n",
            "Validation: accuracy: 0.6708, total loss: 49.830749332904816, average loss: 0.009966149866580964\n",
            "Epoch 108: accuracy: 0.8102666666666667, total loss: 224.98617205023766, average loss: 0.0049996927122275036\n",
            "Validation: accuracy: 0.6416, total loss: 56.1086528301239, average loss: 0.01122173056602478\n",
            "Epoch 109: accuracy: 0.8099555555555555, total loss: 225.5787947177887, average loss: 0.005012862104839749\n",
            "Validation: accuracy: 0.641, total loss: 58.25600779056549, average loss: 0.011651201558113097\n",
            "Epoch 110: accuracy: 0.8111555555555555, total loss: 224.4102957546711, average loss: 0.004986895461214913\n",
            "Validation: accuracy: 0.669, total loss: 51.028750002384186, average loss: 0.010205750000476837\n",
            "Epoch 111: accuracy: 0.8097777777777778, total loss: 225.9657678604126, average loss: 0.005021461508009169\n",
            "Validation: accuracy: 0.6392, total loss: 55.239273607730865, average loss: 0.011047854721546173\n",
            "Epoch 112: accuracy: 0.8128444444444445, total loss: 224.13635617494583, average loss: 0.004980807914998797\n",
            "Validation: accuracy: 0.628, total loss: 56.11490774154663, average loss: 0.011222981548309327\n",
            "Epoch 113: accuracy: 0.8134888888888889, total loss: 222.36059936881065, average loss: 0.004941346652640237\n",
            "Validation: accuracy: 0.6594, total loss: 51.656759679317474, average loss: 0.010331351935863495\n",
            "Epoch 114: accuracy: 0.8132888888888888, total loss: 222.8523297905922, average loss: 0.004952273995346493\n",
            "Validation: accuracy: 0.64, total loss: 54.03179466724396, average loss: 0.01080635893344879\n",
            "Epoch 115: accuracy: 0.8111333333333334, total loss: 224.0769565999508, average loss: 0.004979487924443351\n",
            "Validation: accuracy: 0.644, total loss: 55.52297902107239, average loss: 0.011104595804214478\n",
            "Epoch 116: accuracy: 0.8111333333333334, total loss: 224.83042281866074, average loss: 0.004996231618192461\n",
            "Validation: accuracy: 0.6656, total loss: 51.401885986328125, average loss: 0.010280377197265624\n",
            "Epoch 117: accuracy: 0.8170222222222222, total loss: 218.30096971988678, average loss: 0.004851132660441929\n",
            "Validation: accuracy: 0.6504, total loss: 54.40297722816467, average loss: 0.010880595445632935\n",
            "Epoch 118: accuracy: 0.815, total loss: 219.25767704844475, average loss: 0.004872392823298773\n",
            "Validation: accuracy: 0.6558, total loss: 54.13242840766907, average loss: 0.010826485681533814\n",
            "Epoch 119: accuracy: 0.8111777777777778, total loss: 222.56690645217896, average loss: 0.004945931254492866\n",
            "Validation: accuracy: 0.6518, total loss: 56.030341148376465, average loss: 0.011206068229675292\n",
            "Epoch 120: accuracy: 0.8182666666666667, total loss: 215.60543984174728, average loss: 0.004791231996483273\n",
            "Validation: accuracy: 0.6516, total loss: 54.26634228229523, average loss: 0.010853268456459045\n",
            "Checkpoint saved as `/weights/120.pth`\n",
            "Epoch 121: accuracy: 0.8126888888888889, total loss: 221.35822382569313, average loss: 0.004919071640570959\n",
            "Validation: accuracy: 0.6694, total loss: 52.44301092624664, average loss: 0.01048860218524933\n",
            "Epoch 122: accuracy: 0.8176, total loss: 217.53239125013351, average loss: 0.004834053138891856\n",
            "Validation: accuracy: 0.6686, total loss: 51.133575201034546, average loss: 0.01022671504020691\n",
            "Epoch 123: accuracy: 0.8178, total loss: 218.09425726532936, average loss: 0.004846539050340652\n",
            "Validation: accuracy: 0.6574, total loss: 52.932421803474426, average loss: 0.010586484360694886\n",
            "Epoch 124: accuracy: 0.8187555555555556, total loss: 216.59856379032135, average loss: 0.004813301417562697\n",
            "Validation: accuracy: 0.6406, total loss: 58.01627063751221, average loss: 0.011603254127502441\n",
            "Epoch 125: accuracy: 0.8158, total loss: 217.55660799145699, average loss: 0.004834591288699044\n",
            "Validation: accuracy: 0.6584, total loss: 52.8202691078186, average loss: 0.01056405382156372\n",
            "Epoch 126: accuracy: 0.8172888888888888, total loss: 219.58922305703163, average loss: 0.00487976051237848\n",
            "Validation: accuracy: 0.6624, total loss: 52.464879155159, average loss: 0.010492975831031799\n",
            "Epoch 127: accuracy: 0.8182222222222222, total loss: 215.29033580422401, average loss: 0.0047842296845383115\n",
            "Validation: accuracy: 0.6482, total loss: 55.72353541851044, average loss: 0.011144707083702087\n",
            "Epoch 128: accuracy: 0.8171777777777778, total loss: 219.19469371438026, average loss: 0.004870993193652895\n",
            "Validation: accuracy: 0.6478, total loss: 54.73771685361862, average loss: 0.010947543370723725\n",
            "Epoch 129: accuracy: 0.8179777777777778, total loss: 217.11944150924683, average loss: 0.004824876477983263\n",
            "Validation: accuracy: 0.6504, total loss: 52.89790213108063, average loss: 0.010579580426216125\n",
            "Epoch 130: accuracy: 0.8161555555555555, total loss: 220.58081859350204, average loss: 0.00490179596874449\n",
            "Validation: accuracy: 0.6534, total loss: 53.95110780000687, average loss: 0.010790221560001373\n",
            "Epoch 131: accuracy: 0.8253555555555555, total loss: 210.849824488163, average loss: 0.004685551655292511\n",
            "Validation: accuracy: 0.6168, total loss: 59.45353448390961, average loss: 0.011890706896781922\n",
            "Epoch 132: accuracy: 0.82, total loss: 215.75407993793488, average loss: 0.004794535109731886\n",
            "Validation: accuracy: 0.6518, total loss: 55.002085506916046, average loss: 0.01100041710138321\n",
            "Epoch 133: accuracy: 0.8203333333333334, total loss: 214.01561969518661, average loss: 0.004755902659893036\n",
            "Validation: accuracy: 0.6518, total loss: 56.124279499053955, average loss: 0.01122485589981079\n",
            "Epoch 134: accuracy: 0.8200666666666667, total loss: 214.76210156083107, average loss: 0.004772491145796246\n",
            "Validation: accuracy: 0.6578, total loss: 52.42363619804382, average loss: 0.010484727239608764\n",
            "Epoch 135: accuracy: 0.8189111111111111, total loss: 213.8943283855915, average loss: 0.004753207297457589\n",
            "Validation: accuracy: 0.6212, total loss: 60.73995351791382, average loss: 0.012147990703582764\n",
            "Epoch 136: accuracy: 0.8214444444444444, total loss: 211.6831609904766, average loss: 0.004704070244232813\n",
            "Validation: accuracy: 0.638, total loss: 57.778260827064514, average loss: 0.011555652165412903\n",
            "Epoch 137: accuracy: 0.8208888888888889, total loss: 213.69017353653908, average loss: 0.004748670523034202\n",
            "Validation: accuracy: 0.6568, total loss: 53.97389781475067, average loss: 0.010794779562950134\n",
            "Epoch 138: accuracy: 0.8240444444444445, total loss: 212.38183546066284, average loss: 0.0047195963435702855\n",
            "Validation: accuracy: 0.645, total loss: 55.58722335100174, average loss: 0.011117444670200347\n",
            "Epoch 139: accuracy: 0.8231333333333334, total loss: 209.7812767624855, average loss: 0.0046618061502774555\n",
            "Validation: accuracy: 0.664, total loss: 50.3800385594368, average loss: 0.01007600771188736\n",
            "Epoch 140: accuracy: 0.8253111111111111, total loss: 210.7811177968979, average loss: 0.004684024839931064\n",
            "Validation: accuracy: 0.6528, total loss: 54.48612320423126, average loss: 0.010897224640846253\n",
            "Epoch 141: accuracy: 0.8226888888888889, total loss: 210.63629546761513, average loss: 0.004680806565947003\n",
            "Validation: accuracy: 0.6572, total loss: 53.634148716926575, average loss: 0.010726829743385315\n",
            "Epoch 142: accuracy: 0.8266222222222223, total loss: 208.4609988629818, average loss: 0.004632466641399596\n",
            "Validation: accuracy: 0.6558, total loss: 53.984907150268555, average loss: 0.01079698143005371\n",
            "Epoch 143: accuracy: 0.8271111111111111, total loss: 207.5105393230915, average loss: 0.0046113453182909225\n",
            "Validation: accuracy: 0.6556, total loss: 52.77975416183472, average loss: 0.010555950832366944\n",
            "Epoch 144: accuracy: 0.8216, total loss: 212.33526703715324, average loss: 0.004718561489714517\n",
            "Validation: accuracy: 0.6412, total loss: 56.88477826118469, average loss: 0.01137695565223694\n",
            "Epoch 145: accuracy: 0.8234222222222223, total loss: 209.76694092154503, average loss: 0.004661487576034334\n",
            "Validation: accuracy: 0.6524, total loss: 54.12603461742401, average loss: 0.010825206923484803\n",
            "Epoch 146: accuracy: 0.8276222222222223, total loss: 206.21433198451996, average loss: 0.0045825407107671105\n",
            "Validation: accuracy: 0.6264, total loss: 61.808207273483276, average loss: 0.012361641454696655\n",
            "Epoch 147: accuracy: 0.8261111111111111, total loss: 207.5134373307228, average loss: 0.004611409718460507\n",
            "Validation: accuracy: 0.6114, total loss: 63.888481855392456, average loss: 0.01277769637107849\n",
            "Epoch 148: accuracy: 0.8264888888888889, total loss: 209.64219084382057, average loss: 0.004658715352084902\n",
            "Validation: accuracy: 0.643, total loss: 56.60846269130707, average loss: 0.011321692538261414\n",
            "Epoch 149: accuracy: 0.8238666666666666, total loss: 210.8750824034214, average loss: 0.004686112942298253\n",
            "Validation: accuracy: 0.6498, total loss: 52.70554208755493, average loss: 0.010541108417510986\n",
            "Epoch 150: accuracy: 0.8260222222222222, total loss: 207.43475505709648, average loss: 0.004609661223491033\n",
            "Validation: accuracy: 0.6208, total loss: 61.42391633987427, average loss: 0.012284783267974853\n",
            "Checkpoint saved as `/weights/150.pth`\n",
            "Epoch 151: accuracy: 0.8254, total loss: 207.79674023389816, average loss: 0.00461770533853107\n",
            "Validation: accuracy: 0.6682, total loss: 51.99015474319458, average loss: 0.010398030948638916\n",
            "Epoch 152: accuracy: 0.8255111111111111, total loss: 208.40166595578194, average loss: 0.00463114813235071\n",
            "Validation: accuracy: 0.6446, total loss: 56.71293556690216, average loss: 0.011342587113380433\n",
            "Epoch 153: accuracy: 0.824, total loss: 210.90771701931953, average loss: 0.004686838155984878\n",
            "Validation: accuracy: 0.6554, total loss: 54.18954986333847, average loss: 0.010837909972667694\n",
            "Epoch 154: accuracy: 0.8279333333333333, total loss: 205.58932587504387, average loss: 0.004568651686112086\n",
            "Validation: accuracy: 0.6784, total loss: 50.07065212726593, average loss: 0.010014130425453186\n",
            "Epoch 155: accuracy: 0.8305555555555556, total loss: 203.91938588023186, average loss: 0.004531541908449597\n",
            "Validation: accuracy: 0.6542, total loss: 52.70833134651184, average loss: 0.010541666269302368\n",
            "Epoch 156: accuracy: 0.8281777777777778, total loss: 204.19980081915855, average loss: 0.004537773351536857\n",
            "Validation: accuracy: 0.663, total loss: 51.91161686182022, average loss: 0.010382323372364045\n",
            "Epoch 157: accuracy: 0.8296444444444444, total loss: 203.38988587260246, average loss: 0.004519775241613388\n",
            "Validation: accuracy: 0.6402, total loss: 55.19249176979065, average loss: 0.01103849835395813\n",
            "Epoch 158: accuracy: 0.8280444444444445, total loss: 206.56089580059052, average loss: 0.004590242128902011\n",
            "Validation: accuracy: 0.6548, total loss: 53.39996236562729, average loss: 0.010679992473125458\n",
            "Epoch 159: accuracy: 0.8296, total loss: 202.49690639972687, average loss: 0.004499931253327264\n",
            "Validation: accuracy: 0.6858, total loss: 49.0435494184494, average loss: 0.00980870988368988\n",
            "Epoch 160: accuracy: 0.8286888888888889, total loss: 203.9535725414753, average loss: 0.0045323016120327846\n",
            "Validation: accuracy: 0.6634, total loss: 51.75852406024933, average loss: 0.010351704812049866\n",
            "Epoch 161: accuracy: 0.8256888888888889, total loss: 207.91300997138023, average loss: 0.0046202891104751165\n",
            "Validation: accuracy: 0.6374, total loss: 56.039974212646484, average loss: 0.011207994842529297\n",
            "Epoch 162: accuracy: 0.8291111111111111, total loss: 204.11944511532784, average loss: 0.004535987669229508\n",
            "Validation: accuracy: 0.6654, total loss: 51.750220000743866, average loss: 0.010350044000148773\n",
            "Epoch 163: accuracy: 0.8258222222222222, total loss: 207.16905063390732, average loss: 0.004603756680753496\n",
            "Validation: accuracy: 0.666, total loss: 51.68321245908737, average loss: 0.010336642491817474\n",
            "Epoch 164: accuracy: 0.8274, total loss: 205.93391597270966, average loss: 0.004576309243837992\n",
            "Validation: accuracy: 0.6286, total loss: 59.236899971961975, average loss: 0.011847379994392395\n",
            "Epoch 165: accuracy: 0.8310666666666666, total loss: 202.45639550685883, average loss: 0.00449903101126353\n",
            "Validation: accuracy: 0.6526, total loss: 56.322080850601196, average loss: 0.011264416170120238\n",
            "Epoch 166: accuracy: 0.8282666666666667, total loss: 204.52182930707932, average loss: 0.004544929540157318\n",
            "Validation: accuracy: 0.6416, total loss: 56.668511390686035, average loss: 0.011333702278137207\n",
            "Epoch 167: accuracy: 0.8299111111111112, total loss: 202.31123208999634, average loss: 0.004495805157555474\n",
            "Validation: accuracy: 0.633, total loss: 60.482547760009766, average loss: 0.012096509552001953\n",
            "Epoch 168: accuracy: 0.8294444444444444, total loss: 205.24615374207497, average loss: 0.004561025638712777\n",
            "Validation: accuracy: 0.6538, total loss: 55.9024316072464, average loss: 0.01118048632144928\n",
            "Epoch 169: accuracy: 0.8304444444444444, total loss: 202.9642794728279, average loss: 0.004510317321618398\n",
            "Validation: accuracy: 0.6646, total loss: 51.13209021091461, average loss: 0.010226418042182923\n",
            "Epoch 170: accuracy: 0.8278, total loss: 204.81140902638435, average loss: 0.004551364645030764\n",
            "Validation: accuracy: 0.661, total loss: 51.92989760637283, average loss: 0.010385979521274567\n",
            "Epoch 171: accuracy: 0.8278, total loss: 204.92808836698532, average loss: 0.004553957519266341\n",
            "Validation: accuracy: 0.6646, total loss: 51.67112958431244, average loss: 0.010334225916862488\n",
            "Epoch 172: accuracy: 0.8305555555555556, total loss: 201.23997527360916, average loss: 0.004471999450524648\n",
            "Validation: accuracy: 0.6438, total loss: 56.37897264957428, average loss: 0.011275794529914855\n",
            "Epoch 173: accuracy: 0.8311777777777778, total loss: 200.97308945655823, average loss: 0.0044660686545901825\n",
            "Validation: accuracy: 0.6636, total loss: 51.90558445453644, average loss: 0.010381116890907288\n",
            "Epoch 174: accuracy: 0.8324444444444444, total loss: 200.20781448483467, average loss: 0.004449062544107437\n",
            "Validation: accuracy: 0.6292, total loss: 59.88572335243225, average loss: 0.01197714467048645\n",
            "Epoch 175: accuracy: 0.832, total loss: 199.14776954054832, average loss: 0.004425505989789962\n",
            "Validation: accuracy: 0.63, total loss: 57.36473500728607, average loss: 0.011472947001457214\n",
            "Epoch 176: accuracy: 0.8336666666666667, total loss: 198.96249794960022, average loss: 0.004421388843324449\n",
            "Validation: accuracy: 0.6576, total loss: 52.85704731941223, average loss: 0.010571409463882447\n",
            "Epoch 177: accuracy: 0.8288, total loss: 202.47951635718346, average loss: 0.00449954480793741\n",
            "Validation: accuracy: 0.6678, total loss: 51.342809081077576, average loss: 0.010268561816215514\n",
            "Epoch 178: accuracy: 0.8331777777777778, total loss: 200.00673231482506, average loss: 0.004444594051440557\n",
            "Validation: accuracy: 0.6576, total loss: 52.31333816051483, average loss: 0.010462667632102966\n",
            "Epoch 179: accuracy: 0.8331333333333333, total loss: 199.73144334554672, average loss: 0.004438476518789927\n",
            "Validation: accuracy: 0.6618, total loss: 52.397662580013275, average loss: 0.010479532516002655\n",
            "Epoch 180: accuracy: 0.8326666666666667, total loss: 198.7200186252594, average loss: 0.004416000413894654\n",
            "Validation: accuracy: 0.6542, total loss: 53.73156547546387, average loss: 0.010746313095092773\n",
            "Checkpoint saved as `/weights/180.pth`\n",
            "Epoch 181: accuracy: 0.8318666666666666, total loss: 200.39215523004532, average loss: 0.004453159005112118\n",
            "Validation: accuracy: 0.6424, total loss: 56.63344705104828, average loss: 0.011326689410209655\n",
            "Epoch 182: accuracy: 0.8252444444444444, total loss: 207.1315039396286, average loss: 0.004602922309769525\n",
            "Validation: accuracy: 0.6704, total loss: 50.08368057012558, average loss: 0.010016736114025116\n",
            "Epoch 183: accuracy: 0.8333333333333334, total loss: 199.23130798339844, average loss: 0.004427362399631076\n",
            "Validation: accuracy: 0.6676, total loss: 50.1643488407135, average loss: 0.0100328697681427\n",
            "Epoch 184: accuracy: 0.8339111111111112, total loss: 199.89272019267082, average loss: 0.004442060448726018\n",
            "Validation: accuracy: 0.6744, total loss: 49.906351536512375, average loss: 0.009981270307302476\n",
            "Epoch 185: accuracy: 0.8347555555555556, total loss: 196.97152516245842, average loss: 0.0043771450036101875\n",
            "Validation: accuracy: 0.645, total loss: 54.986570954322815, average loss: 0.010997314190864563\n",
            "Epoch 186: accuracy: 0.8310666666666666, total loss: 200.71119651198387, average loss: 0.004460248811377419\n",
            "Validation: accuracy: 0.6552, total loss: 51.653363049030304, average loss: 0.01033067260980606\n",
            "Epoch 187: accuracy: 0.8306666666666667, total loss: 202.4429665505886, average loss: 0.00449873259001308\n",
            "Validation: accuracy: 0.6372, total loss: 58.4383989572525, average loss: 0.0116876797914505\n",
            "Epoch 188: accuracy: 0.8332, total loss: 198.98467481136322, average loss: 0.004421881662474738\n",
            "Validation: accuracy: 0.6366, total loss: 56.213276505470276, average loss: 0.011242655301094055\n",
            "Epoch 189: accuracy: 0.8286888888888889, total loss: 202.1563446521759, average loss: 0.004492363214492798\n",
            "Validation: accuracy: 0.622, total loss: 60.53637421131134, average loss: 0.012107274842262268\n",
            "Epoch 190: accuracy: 0.8352444444444445, total loss: 196.3824025094509, average loss: 0.004364053389098909\n",
            "Validation: accuracy: 0.6468, total loss: 54.67250597476959, average loss: 0.010934501194953919\n",
            "Epoch 191: accuracy: 0.8342888888888889, total loss: 199.00320795178413, average loss: 0.0044222935100396476\n",
            "Validation: accuracy: 0.6544, total loss: 52.389783799648285, average loss: 0.010477956759929657\n",
            "Epoch 192: accuracy: 0.8356444444444444, total loss: 197.40080985426903, average loss: 0.0043866846634282005\n",
            "Validation: accuracy: 0.6454, total loss: 56.45836925506592, average loss: 0.011291673851013184\n",
            "Epoch 193: accuracy: 0.8350222222222222, total loss: 196.62655273079872, average loss: 0.004369478949573305\n",
            "Validation: accuracy: 0.663, total loss: 54.02630716562271, average loss: 0.010805261433124541\n",
            "Epoch 194: accuracy: 0.8337333333333333, total loss: 197.76218384504318, average loss: 0.004394715196556515\n",
            "Validation: accuracy: 0.6488, total loss: 54.237516939640045, average loss: 0.010847503387928009\n",
            "Epoch 195: accuracy: 0.8366888888888889, total loss: 194.8219112753868, average loss: 0.0043293758061197065\n",
            "Validation: accuracy: 0.6612, total loss: 52.3407039642334, average loss: 0.01046814079284668\n",
            "Epoch 196: accuracy: 0.8354, total loss: 196.9801853299141, average loss: 0.004377337451775869\n",
            "Validation: accuracy: 0.6688, total loss: 50.54283618927002, average loss: 0.010108567237854004\n",
            "Epoch 197: accuracy: 0.8322444444444445, total loss: 199.79448601603508, average loss: 0.004439877467023002\n",
            "Validation: accuracy: 0.6406, total loss: 56.32813876867294, average loss: 0.011265627753734589\n",
            "Epoch 198: accuracy: 0.8355333333333334, total loss: 196.98743358254433, average loss: 0.004377498524056541\n",
            "Validation: accuracy: 0.6654, total loss: 51.99701523780823, average loss: 0.010399403047561645\n",
            "Epoch 199: accuracy: 0.8365555555555556, total loss: 195.29453176259995, average loss: 0.004339878483613332\n",
            "Validation: accuracy: 0.6614, total loss: 53.20189690589905, average loss: 0.01064037938117981\n",
            "Epoch 200: accuracy: 0.8358666666666666, total loss: 196.1852234005928, average loss: 0.004359671631124285\n",
            "Validation: accuracy: 0.6736, total loss: 49.341622710227966, average loss: 0.009868324542045594\n",
            "Epoch 201: accuracy: 0.8356444444444444, total loss: 194.54350143671036, average loss: 0.004323188920815786\n",
            "Validation: accuracy: 0.6614, total loss: 52.15353065729141, average loss: 0.010430706131458283\n",
            "Epoch 202: accuracy: 0.8341555555555555, total loss: 197.93882030248642, average loss: 0.004398640451166365\n",
            "Validation: accuracy: 0.6542, total loss: 53.04730957746506, average loss: 0.010609461915493012\n",
            "Epoch 203: accuracy: 0.8338444444444445, total loss: 198.32571658492088, average loss: 0.004407238146331575\n",
            "Validation: accuracy: 0.6608, total loss: 54.87574237585068, average loss: 0.010975148475170135\n",
            "Epoch 204: accuracy: 0.8374666666666667, total loss: 194.19784507155418, average loss: 0.0043155076682567595\n",
            "Validation: accuracy: 0.643, total loss: 56.19746446609497, average loss: 0.011239492893218994\n",
            "Epoch 205: accuracy: 0.8396444444444444, total loss: 193.25593274831772, average loss: 0.00429457628329595\n",
            "Validation: accuracy: 0.6714, total loss: 51.14591807126999, average loss: 0.010229183614253998\n",
            "Epoch 206: accuracy: 0.8373111111111111, total loss: 194.60995748639107, average loss: 0.004324665721919802\n",
            "Validation: accuracy: 0.6374, total loss: 57.36223793029785, average loss: 0.01147244758605957\n",
            "Epoch 207: accuracy: 0.8371333333333333, total loss: 195.24237075448036, average loss: 0.0043387193500995635\n",
            "Validation: accuracy: 0.665, total loss: 52.99755039811134, average loss: 0.010599510079622268\n",
            "Epoch 208: accuracy: 0.8367777777777777, total loss: 192.49195823073387, average loss: 0.004277599071794086\n",
            "Validation: accuracy: 0.6286, total loss: 60.29885947704315, average loss: 0.01205977189540863\n",
            "Epoch 209: accuracy: 0.8346666666666667, total loss: 197.7510848045349, average loss: 0.004394468551211887\n",
            "Validation: accuracy: 0.6526, total loss: 54.13971662521362, average loss: 0.010827943325042725\n",
            "Epoch 210: accuracy: 0.8386666666666667, total loss: 193.56303602457047, average loss: 0.00430140080054601\n",
            "Validation: accuracy: 0.6252, total loss: 61.58694648742676, average loss: 0.012317389297485352\n",
            "Checkpoint saved as `/weights/210.pth`\n",
            "Epoch 211: accuracy: 0.8365555555555556, total loss: 195.1996382176876, average loss: 0.004337769738170836\n",
            "Validation: accuracy: 0.6614, total loss: 54.040968894958496, average loss: 0.0108081937789917\n",
            "Epoch 212: accuracy: 0.8330888888888889, total loss: 196.02605044841766, average loss: 0.004356134454409281\n",
            "Validation: accuracy: 0.684, total loss: 49.497214674949646, average loss: 0.009899442934989929\n",
            "Epoch 213: accuracy: 0.8393333333333334, total loss: 192.5645904392004, average loss: 0.00427921312087112\n",
            "Validation: accuracy: 0.6344, total loss: 58.310373187065125, average loss: 0.011662074637413025\n",
            "Epoch 214: accuracy: 0.8389333333333333, total loss: 191.5770252943039, average loss: 0.004257267228762309\n",
            "Validation: accuracy: 0.6564, total loss: 53.87365114688873, average loss: 0.010774730229377747\n",
            "Epoch 215: accuracy: 0.8394444444444444, total loss: 192.6285510957241, average loss: 0.004280634468793869\n",
            "Validation: accuracy: 0.6416, total loss: 55.556827425956726, average loss: 0.011111365485191345\n",
            "Epoch 216: accuracy: 0.8392888888888889, total loss: 191.90318357944489, average loss: 0.004264515190654331\n",
            "Validation: accuracy: 0.6558, total loss: 54.98798203468323, average loss: 0.010997596406936646\n",
            "Epoch 217: accuracy: 0.8368, total loss: 193.51207229495049, average loss: 0.004300268273221122\n",
            "Validation: accuracy: 0.6762, total loss: 49.51089459657669, average loss: 0.009902178919315338\n",
            "Epoch 218: accuracy: 0.8367111111111111, total loss: 194.17717343568802, average loss: 0.004315048298570845\n",
            "Validation: accuracy: 0.674, total loss: 50.94447588920593, average loss: 0.010188895177841186\n",
            "Epoch 219: accuracy: 0.8401111111111111, total loss: 192.77377006411552, average loss: 0.004283861556980345\n",
            "Validation: accuracy: 0.6598, total loss: 52.40641611814499, average loss: 0.010481283223628998\n",
            "Epoch 220: accuracy: 0.8382666666666667, total loss: 192.4136446416378, average loss: 0.004275858769814174\n",
            "Validation: accuracy: 0.6422, total loss: 56.3763130903244, average loss: 0.01127526261806488\n",
            "Epoch 221: accuracy: 0.8368666666666666, total loss: 194.71206265687943, average loss: 0.004326934725708432\n",
            "Validation: accuracy: 0.6136, total loss: 63.99702179431915, average loss: 0.012799404358863831\n",
            "Epoch 222: accuracy: 0.8386444444444444, total loss: 192.7452440559864, average loss: 0.0042832276456885865\n",
            "Validation: accuracy: 0.6562, total loss: 54.3739595413208, average loss: 0.01087479190826416\n",
            "Epoch 223: accuracy: 0.8376888888888889, total loss: 193.06204172968864, average loss: 0.004290267593993081\n",
            "Validation: accuracy: 0.6662, total loss: 52.28354048728943, average loss: 0.010456708097457886\n",
            "Epoch 224: accuracy: 0.8398, total loss: 188.65836557745934, average loss: 0.004192408123943541\n",
            "Validation: accuracy: 0.6672, total loss: 52.43991106748581, average loss: 0.010487982213497163\n",
            "Epoch 225: accuracy: 0.8383111111111111, total loss: 193.08776554465294, average loss: 0.004290839234325621\n",
            "Validation: accuracy: 0.668, total loss: 51.193769693374634, average loss: 0.010238753938674927\n",
            "Epoch 226: accuracy: 0.8396222222222223, total loss: 192.16235530376434, average loss: 0.004270274562305875\n",
            "Validation: accuracy: 0.6282, total loss: 59.6961532831192, average loss: 0.01193923065662384\n",
            "Epoch 227: accuracy: 0.8387333333333333, total loss: 191.71318367123604, average loss: 0.004260292970471912\n",
            "Validation: accuracy: 0.6458, total loss: 55.694578409194946, average loss: 0.011138915681838989\n",
            "Epoch 228: accuracy: 0.8380666666666666, total loss: 193.06762060523033, average loss: 0.004290391569005118\n",
            "Validation: accuracy: 0.6356, total loss: 57.64903545379639, average loss: 0.011529807090759278\n",
            "Epoch 229: accuracy: 0.8398666666666667, total loss: 192.29594403505325, average loss: 0.0042732432007789615\n",
            "Validation: accuracy: 0.6544, total loss: 54.42273199558258, average loss: 0.010884546399116516\n",
            "Epoch 230: accuracy: 0.8388666666666666, total loss: 193.88999950885773, average loss: 0.004308666655752394\n",
            "Validation: accuracy: 0.6474, total loss: 55.51527464389801, average loss: 0.011103054928779601\n",
            "Epoch 231: accuracy: 0.8368444444444444, total loss: 195.52867925167084, average loss: 0.0043450817611482406\n",
            "Validation: accuracy: 0.6592, total loss: 52.38837647438049, average loss: 0.010477675294876098\n",
            "Epoch 232: accuracy: 0.8416666666666667, total loss: 189.07925367355347, average loss: 0.004201761192745633\n",
            "Validation: accuracy: 0.6442, total loss: 57.29667639732361, average loss: 0.011459335279464722\n",
            "Epoch 233: accuracy: 0.8431555555555555, total loss: 189.3672845363617, average loss: 0.004208161878585815\n",
            "Validation: accuracy: 0.6316, total loss: 59.34510266780853, average loss: 0.011869020533561706\n",
            "Epoch 234: accuracy: 0.8386666666666667, total loss: 191.68962740898132, average loss: 0.0042597694979773625\n",
            "Validation: accuracy: 0.6676, total loss: 54.40430265665054, average loss: 0.01088086053133011\n",
            "Epoch 235: accuracy: 0.8399333333333333, total loss: 191.81175863742828, average loss: 0.004262483525276184\n",
            "Validation: accuracy: 0.6614, total loss: 53.04879093170166, average loss: 0.010609758186340332\n",
            "Epoch 236: accuracy: 0.8405333333333334, total loss: 190.20827943086624, average loss: 0.00422685065401925\n",
            "Validation: accuracy: 0.6712, total loss: 49.63562220335007, average loss: 0.009927124440670013\n",
            "Epoch 237: accuracy: 0.8427555555555556, total loss: 188.17095956206322, average loss: 0.00418157687915696\n",
            "Validation: accuracy: 0.663, total loss: 52.86326053738594, average loss: 0.010572652107477189\n",
            "Epoch 238: accuracy: 0.8438888888888889, total loss: 187.56700253486633, average loss: 0.0041681556118859184\n",
            "Validation: accuracy: 0.6756, total loss: 51.01333898305893, average loss: 0.010202667796611786\n",
            "Epoch 239: accuracy: 0.8395777777777778, total loss: 193.0727492570877, average loss: 0.0042905055390463935\n",
            "Validation: accuracy: 0.6584, total loss: 54.5392861366272, average loss: 0.01090785722732544\n",
            "Epoch 240: accuracy: 0.8417111111111111, total loss: 189.9296417236328, average loss: 0.004220658704969618\n",
            "Validation: accuracy: 0.6284, total loss: 59.697266817092896, average loss: 0.011939453363418579\n",
            "Checkpoint saved as `/weights/240.pth`\n",
            "Epoch 241: accuracy: 0.8411333333333333, total loss: 190.95449393987656, average loss: 0.004243433198663923\n",
            "Validation: accuracy: 0.6692, total loss: 52.77670353651047, average loss: 0.010555340707302093\n",
            "Epoch 242: accuracy: 0.8381555555555555, total loss: 194.01450353860855, average loss: 0.004311433411969079\n",
            "Validation: accuracy: 0.6602, total loss: 53.238258719444275, average loss: 0.010647651743888854\n",
            "Epoch 243: accuracy: 0.8387333333333333, total loss: 192.05440565943718, average loss: 0.004267875681320826\n",
            "Validation: accuracy: 0.636, total loss: 57.90210473537445, average loss: 0.01158042094707489\n",
            "Epoch 244: accuracy: 0.8394888888888888, total loss: 190.73275661468506, average loss: 0.004238505702548557\n",
            "Validation: accuracy: 0.6552, total loss: 55.20902365446091, average loss: 0.01104180473089218\n",
            "Epoch 245: accuracy: 0.8403555555555555, total loss: 190.71173530817032, average loss: 0.0042380385624037845\n",
            "Validation: accuracy: 0.6356, total loss: 59.19572103023529, average loss: 0.011839144206047057\n",
            "Epoch 246: accuracy: 0.8457333333333333, total loss: 185.2747307419777, average loss: 0.0041172162387106154\n",
            "Validation: accuracy: 0.6836, total loss: 48.46068066358566, average loss: 0.009692136132717133\n",
            "Epoch 247: accuracy: 0.8401333333333333, total loss: 191.8772883117199, average loss: 0.004263939740260442\n",
            "Validation: accuracy: 0.6784, total loss: 50.800308525562286, average loss: 0.010160061705112458\n",
            "Epoch 248: accuracy: 0.8409111111111112, total loss: 188.9316474199295, average loss: 0.004198481053776211\n",
            "Validation: accuracy: 0.6424, total loss: 58.5603763461113, average loss: 0.01171207526922226\n",
            "Epoch 249: accuracy: 0.8421777777777778, total loss: 189.75497421622276, average loss: 0.0042167772048049505\n",
            "Validation: accuracy: 0.6638, total loss: 53.82553881406784, average loss: 0.010765107762813569\n",
            "Epoch 250: accuracy: 0.8437777777777777, total loss: 186.9532264471054, average loss: 0.004154516143269009\n",
            "Validation: accuracy: 0.6608, total loss: 55.01708173751831, average loss: 0.011003416347503662\n",
            "Epoch 251: accuracy: 0.8422444444444445, total loss: 188.95337417721748, average loss: 0.004198963870604833\n",
            "Validation: accuracy: 0.661, total loss: 54.01778960227966, average loss: 0.010803557920455932\n",
            "Epoch 252: accuracy: 0.8401777777777778, total loss: 189.6271506845951, average loss: 0.004213936681879892\n",
            "Validation: accuracy: 0.65, total loss: 55.39682579040527, average loss: 0.011079365158081054\n",
            "Epoch 253: accuracy: 0.8406888888888889, total loss: 190.3186895251274, average loss: 0.004229304211669498\n",
            "Validation: accuracy: 0.6432, total loss: 56.486875772476196, average loss: 0.01129737515449524\n",
            "Epoch 254: accuracy: 0.8431777777777778, total loss: 186.4738981127739, average loss: 0.004143864402506086\n",
            "Validation: accuracy: 0.6546, total loss: 54.76187801361084, average loss: 0.010952375602722168\n",
            "Epoch 255: accuracy: 0.8440444444444445, total loss: 186.5742306113243, average loss: 0.004146094013584985\n",
            "Validation: accuracy: 0.645, total loss: 55.97976911067963, average loss: 0.011195953822135925\n",
            "Epoch 256: accuracy: 0.8411777777777778, total loss: 190.49099466204643, average loss: 0.004233133214712143\n",
            "Validation: accuracy: 0.6228, total loss: 61.375266671180725, average loss: 0.012275053334236145\n",
            "Epoch 257: accuracy: 0.8436666666666667, total loss: 186.72308605909348, average loss: 0.0041494019124243\n",
            "Validation: accuracy: 0.6408, total loss: 58.10084289312363, average loss: 0.011620168578624725\n",
            "Epoch 258: accuracy: 0.8372666666666667, total loss: 191.70217034220695, average loss: 0.004260048229826821\n",
            "Validation: accuracy: 0.6512, total loss: 56.078679382801056, average loss: 0.01121573587656021\n",
            "Epoch 259: accuracy: 0.8406666666666667, total loss: 188.90208742022514, average loss: 0.004197824164893892\n",
            "Validation: accuracy: 0.6666, total loss: 51.015510857105255, average loss: 0.010203102171421051\n",
            "Epoch 260: accuracy: 0.8427111111111111, total loss: 186.32508900761604, average loss: 0.0041405575335025785\n",
            "Validation: accuracy: 0.6744, total loss: 49.47266575694084, average loss: 0.009894533151388168\n",
            "Epoch 261: accuracy: 0.8443555555555555, total loss: 186.6232694685459, average loss: 0.004147183765967687\n",
            "Validation: accuracy: 0.6602, total loss: 53.62696045637131, average loss: 0.010725392091274261\n",
            "Epoch 262: accuracy: 0.8401777777777778, total loss: 190.95224550366402, average loss: 0.004243383233414756\n",
            "Validation: accuracy: 0.6444, total loss: 56.36387366056442, average loss: 0.011272774732112885\n",
            "Epoch 263: accuracy: 0.8433111111111111, total loss: 186.95228579640388, average loss: 0.004154495239920087\n",
            "Validation: accuracy: 0.655, total loss: 55.444540321826935, average loss: 0.011088908064365388\n",
            "Epoch 264: accuracy: 0.8405333333333334, total loss: 188.53225561976433, average loss: 0.0041896056804392075\n",
            "Validation: accuracy: 0.6494, total loss: 54.944018721580505, average loss: 0.010988803744316102\n",
            "Epoch 265: accuracy: 0.8446444444444444, total loss: 185.63595274090767, average loss: 0.004125243394242393\n",
            "Validation: accuracy: 0.6682, total loss: 51.18437063694, average loss: 0.010236874127388001\n",
            "Epoch 266: accuracy: 0.8449111111111111, total loss: 184.19354873895645, average loss: 0.00409318997197681\n",
            "Validation: accuracy: 0.646, total loss: 56.16586077213287, average loss: 0.011233172154426575\n",
            "Epoch 267: accuracy: 0.8429777777777778, total loss: 187.76922941207886, average loss: 0.0041726495424906414\n",
            "Validation: accuracy: 0.6652, total loss: 52.63090544939041, average loss: 0.010526181089878081\n",
            "Epoch 268: accuracy: 0.8431555555555555, total loss: 186.0138871818781, average loss: 0.004133641937375069\n",
            "Validation: accuracy: 0.66, total loss: 54.80983144044876, average loss: 0.010961966288089752\n",
            "Epoch 269: accuracy: 0.8447777777777777, total loss: 184.76277667284012, average loss: 0.0041058394816186694\n",
            "Validation: accuracy: 0.6598, total loss: 54.10842764377594, average loss: 0.010821685528755188\n",
            "Epoch 270: accuracy: 0.8442222222222222, total loss: 187.05224472284317, average loss: 0.004156716549396515\n",
            "Validation: accuracy: 0.6676, total loss: 53.11643588542938, average loss: 0.010623287177085876\n",
            "Checkpoint saved as `/weights/270.pth`\n",
            "Epoch 271: accuracy: 0.8434666666666667, total loss: 186.4712491631508, average loss: 0.0041438055369589066\n",
            "Validation: accuracy: 0.647, total loss: 57.1198148727417, average loss: 0.01142396297454834\n",
            "Epoch 272: accuracy: 0.8491333333333333, total loss: 182.3700869679451, average loss: 0.004052668599287669\n",
            "Validation: accuracy: 0.643, total loss: 57.89657258987427, average loss: 0.011579314517974854\n",
            "Epoch 273: accuracy: 0.8449777777777778, total loss: 186.03086432814598, average loss: 0.004134019207292133\n",
            "Validation: accuracy: 0.6496, total loss: 55.327027320861816, average loss: 0.011065405464172364\n",
            "Epoch 274: accuracy: 0.8436444444444444, total loss: 185.65290877223015, average loss: 0.004125620194938447\n",
            "Validation: accuracy: 0.674, total loss: 52.25974917411804, average loss: 0.010451949834823608\n",
            "Epoch 275: accuracy: 0.8429555555555556, total loss: 186.3727424144745, average loss: 0.004141616498099433\n",
            "Validation: accuracy: 0.6554, total loss: 54.932213306427, average loss: 0.0109864426612854\n",
            "Epoch 276: accuracy: 0.8404888888888888, total loss: 189.09639218449593, average loss: 0.004202142048544354\n",
            "Validation: accuracy: 0.647, total loss: 54.89251756668091, average loss: 0.010978503513336181\n",
            "Epoch 277: accuracy: 0.8453777777777778, total loss: 185.01463812589645, average loss: 0.004111436402797699\n",
            "Validation: accuracy: 0.6276, total loss: 59.41326403617859, average loss: 0.011882652807235718\n",
            "Epoch 278: accuracy: 0.8418888888888889, total loss: 187.7112211585045, average loss: 0.004171360470188988\n",
            "Validation: accuracy: 0.6518, total loss: 55.37262433767319, average loss: 0.011074524867534637\n",
            "Epoch 279: accuracy: 0.8432888888888889, total loss: 188.02284011244774, average loss: 0.004178285335832172\n",
            "Validation: accuracy: 0.657, total loss: 53.73088401556015, average loss: 0.01074617680311203\n",
            "Epoch 280: accuracy: 0.8462888888888889, total loss: 184.8831273317337, average loss: 0.004108513940705194\n",
            "Validation: accuracy: 0.6688, total loss: 52.41298687458038, average loss: 0.010482597374916077\n",
            "Epoch 281: accuracy: 0.8414222222222222, total loss: 188.00597396492958, average loss: 0.0041779105325539904\n",
            "Validation: accuracy: 0.6442, total loss: 55.55277490615845, average loss: 0.01111055498123169\n",
            "Epoch 282: accuracy: 0.8432888888888889, total loss: 187.70273837447166, average loss: 0.004171171963877148\n",
            "Validation: accuracy: 0.6686, total loss: 50.511790573596954, average loss: 0.01010235811471939\n",
            "Epoch 283: accuracy: 0.8461777777777778, total loss: 184.38347974419594, average loss: 0.004097410660982132\n",
            "Validation: accuracy: 0.6416, total loss: 57.01542031764984, average loss: 0.011403084063529969\n",
            "Epoch 284: accuracy: 0.8450444444444445, total loss: 185.7016341984272, average loss: 0.004126702982187271\n",
            "Validation: accuracy: 0.6302, total loss: 59.54243719577789, average loss: 0.011908487439155579\n",
            "Epoch 285: accuracy: 0.8439333333333333, total loss: 188.08372229337692, average loss: 0.004179638273186154\n",
            "Validation: accuracy: 0.6728, total loss: 50.992006957530975, average loss: 0.010198401391506195\n",
            "Epoch 286: accuracy: 0.8457555555555556, total loss: 185.16505825519562, average loss: 0.004114779072337681\n",
            "Validation: accuracy: 0.6586, total loss: 53.03439462184906, average loss: 0.010606878924369812\n",
            "Epoch 287: accuracy: 0.8472444444444445, total loss: 183.9546870291233, average loss: 0.004087881933980518\n",
            "Validation: accuracy: 0.6576, total loss: 53.59756588935852, average loss: 0.010719513177871703\n",
            "Epoch 288: accuracy: 0.8417555555555556, total loss: 187.50453835725784, average loss: 0.004166767519050174\n",
            "Validation: accuracy: 0.623, total loss: 60.93907141685486, average loss: 0.012187814283370972\n",
            "Epoch 289: accuracy: 0.8480444444444445, total loss: 184.21977439522743, average loss: 0.004093772764338388\n",
            "Validation: accuracy: 0.634, total loss: 59.024582266807556, average loss: 0.011804916453361512\n",
            "Epoch 290: accuracy: 0.8493777777777778, total loss: 179.93134009838104, average loss: 0.003998474224408468\n",
            "Validation: accuracy: 0.6622, total loss: 52.787203550338745, average loss: 0.010557440710067749\n",
            "Epoch 291: accuracy: 0.8459111111111111, total loss: 184.04596757888794, average loss: 0.004089910390641954\n",
            "Validation: accuracy: 0.647, total loss: 55.28002607822418, average loss: 0.011056005215644836\n",
            "Epoch 292: accuracy: 0.8468, total loss: 184.65096989274025, average loss: 0.004103354886505339\n",
            "Validation: accuracy: 0.6586, total loss: 51.47701394557953, average loss: 0.010295402789115905\n",
            "Epoch 293: accuracy: 0.8476666666666667, total loss: 183.18959411978722, average loss: 0.004070879869328605\n",
            "Validation: accuracy: 0.6636, total loss: 52.352614402770996, average loss: 0.010470522880554198\n",
            "Epoch 294: accuracy: 0.8450888888888889, total loss: 184.43171852827072, average loss: 0.004098482633961571\n",
            "Validation: accuracy: 0.6684, total loss: 50.39954972267151, average loss: 0.010079909944534301\n",
            "Epoch 295: accuracy: 0.8467111111111111, total loss: 183.06017097830772, average loss: 0.00406800379951795\n",
            "Validation: accuracy: 0.6486, total loss: 56.62492644786835, average loss: 0.01132498528957367\n",
            "Epoch 296: accuracy: 0.8468888888888889, total loss: 183.8429816365242, average loss: 0.00408539959192276\n",
            "Validation: accuracy: 0.6516, total loss: 54.904662013053894, average loss: 0.010980932402610779\n",
            "Epoch 297: accuracy: 0.8486444444444444, total loss: 181.48193192481995, average loss: 0.004032931820551555\n",
            "Validation: accuracy: 0.679, total loss: 51.32129913568497, average loss: 0.010264259827136994\n",
            "Epoch 298: accuracy: 0.8459777777777778, total loss: 184.16058206558228, average loss: 0.004092457379235161\n",
            "Validation: accuracy: 0.6674, total loss: 51.266616225242615, average loss: 0.010253323245048523\n",
            "Epoch 299: accuracy: 0.8440222222222222, total loss: 186.44358038902283, average loss: 0.004143190675311618\n",
            "Validation: accuracy: 0.6564, total loss: 55.0833997130394, average loss: 0.01101667994260788\n",
            "Epoch 300: accuracy: 0.8463777777777778, total loss: 184.67194712162018, average loss: 0.004103821047147115\n",
            "Validation: accuracy: 0.6794, total loss: 50.46373242139816, average loss: 0.010092746484279633\n",
            "Checkpoint saved as `/weights/300.pth`\n",
            "Epoch 301: accuracy: 0.8466666666666667, total loss: 184.64116472005844, average loss: 0.004103136993779077\n",
            "Validation: accuracy: 0.663, total loss: 51.56654238700867, average loss: 0.010313308477401733\n",
            "Epoch 302: accuracy: 0.8445333333333334, total loss: 185.08364501595497, average loss: 0.004112969889243444\n",
            "Validation: accuracy: 0.6842, total loss: 49.04731607437134, average loss: 0.009809463214874267\n",
            "Epoch 303: accuracy: 0.8444444444444444, total loss: 187.0546960234642, average loss: 0.004156771022743649\n",
            "Validation: accuracy: 0.6602, total loss: 53.622161507606506, average loss: 0.010724432301521302\n",
            "Epoch 304: accuracy: 0.8486, total loss: 182.6874848306179, average loss: 0.004059721885124842\n",
            "Validation: accuracy: 0.6324, total loss: 59.657474994659424, average loss: 0.011931494998931884\n",
            "Epoch 305: accuracy: 0.8468222222222223, total loss: 181.72870880365372, average loss: 0.004038415751192305\n",
            "Validation: accuracy: 0.6616, total loss: 52.677583158016205, average loss: 0.01053551663160324\n",
            "Epoch 306: accuracy: 0.8498, total loss: 179.94549226760864, average loss: 0.00399878871705797\n",
            "Validation: accuracy: 0.6466, total loss: 56.87972849607468, average loss: 0.011375945699214936\n",
            "Epoch 307: accuracy: 0.8461333333333333, total loss: 183.5356131196022, average loss: 0.004078569180435605\n",
            "Validation: accuracy: 0.6694, total loss: 53.54556083679199, average loss: 0.010709112167358398\n",
            "Epoch 308: accuracy: 0.8483333333333334, total loss: 181.01276686787605, average loss: 0.004022505930397245\n",
            "Validation: accuracy: 0.6754, total loss: 50.27771985530853, average loss: 0.010055543971061707\n",
            "Epoch 309: accuracy: 0.8473111111111111, total loss: 182.73601466417313, average loss: 0.0040608003258705136\n",
            "Validation: accuracy: 0.6834, total loss: 48.50391936302185, average loss: 0.00970078387260437\n",
            "Epoch 310: accuracy: 0.8490222222222222, total loss: 182.6727715432644, average loss: 0.004059394923183653\n",
            "Validation: accuracy: 0.6756, total loss: 50.66284936666489, average loss: 0.010132569873332978\n",
            "Epoch 311: accuracy: 0.8496222222222222, total loss: 180.90878465771675, average loss: 0.004020195214615928\n",
            "Validation: accuracy: 0.677, total loss: 49.81060338020325, average loss: 0.00996212067604065\n",
            "Epoch 312: accuracy: 0.8467111111111111, total loss: 183.1266217827797, average loss: 0.004069480484061771\n",
            "Validation: accuracy: 0.6478, total loss: 54.8734410405159, average loss: 0.01097468820810318\n",
            "Epoch 313: accuracy: 0.8524, total loss: 179.15461161732674, average loss: 0.00398121359149615\n",
            "Validation: accuracy: 0.6722, total loss: 52.16211277246475, average loss: 0.01043242255449295\n",
            "Epoch 314: accuracy: 0.8478888888888889, total loss: 181.02129912376404, average loss: 0.004022695536083645\n",
            "Validation: accuracy: 0.6876, total loss: 48.40230840444565, average loss: 0.00968046168088913\n",
            "Epoch 315: accuracy: 0.8473777777777778, total loss: 181.30575788021088, average loss: 0.004029016841782464\n",
            "Validation: accuracy: 0.6532, total loss: 54.39319312572479, average loss: 0.010878638625144958\n",
            "Epoch 316: accuracy: 0.8477555555555556, total loss: 184.3560393154621, average loss: 0.004096800873676936\n",
            "Validation: accuracy: 0.6822, total loss: 50.245700895786285, average loss: 0.010049140179157257\n",
            "Epoch 317: accuracy: 0.8519333333333333, total loss: 176.05163577198982, average loss: 0.003912258572710885\n",
            "Validation: accuracy: 0.6656, total loss: 52.36501157283783, average loss: 0.010473002314567565\n",
            "Epoch 318: accuracy: 0.8480666666666666, total loss: 180.75081259012222, average loss: 0.004016684724224938\n",
            "Validation: accuracy: 0.6642, total loss: 52.224337458610535, average loss: 0.010444867491722108\n",
            "Epoch 319: accuracy: 0.8500666666666666, total loss: 181.7084765136242, average loss: 0.004037966144747204\n",
            "Validation: accuracy: 0.663, total loss: 53.808384120464325, average loss: 0.010761676824092865\n",
            "Epoch 320: accuracy: 0.8485111111111111, total loss: 181.4205371439457, average loss: 0.004031567492087682\n",
            "Validation: accuracy: 0.6662, total loss: 53.0432870388031, average loss: 0.010608657407760621\n",
            "Epoch 321: accuracy: 0.8436222222222223, total loss: 185.03780508041382, average loss: 0.004111951224009196\n",
            "Validation: accuracy: 0.688, total loss: 48.037944078445435, average loss: 0.009607588815689088\n",
            "Epoch 322: accuracy: 0.8518666666666667, total loss: 177.85820752382278, average loss: 0.003952404611640506\n",
            "Validation: accuracy: 0.649, total loss: 56.156794369220734, average loss: 0.011231358873844147\n",
            "Epoch 323: accuracy: 0.8481333333333333, total loss: 179.8535116314888, average loss: 0.003996744702921973\n",
            "Validation: accuracy: 0.6606, total loss: 54.03169137239456, average loss: 0.010806338274478912\n",
            "Epoch 324: accuracy: 0.8456222222222223, total loss: 185.28456330299377, average loss: 0.004117434740066529\n",
            "Validation: accuracy: 0.6408, total loss: 57.945371985435486, average loss: 0.011589074397087097\n",
            "Epoch 325: accuracy: 0.8494, total loss: 178.8023073375225, average loss: 0.0039733846075005\n",
            "Validation: accuracy: 0.6544, total loss: 53.527020037174225, average loss: 0.010705404007434844\n",
            "Epoch 326: accuracy: 0.8484222222222222, total loss: 181.06131234765053, average loss: 0.004023584718836679\n",
            "Validation: accuracy: 0.6464, total loss: 54.90643858909607, average loss: 0.010981287717819213\n",
            "Epoch 327: accuracy: 0.848, total loss: 179.38135194778442, average loss: 0.003986252265506321\n",
            "Validation: accuracy: 0.6794, total loss: 50.95284378528595, average loss: 0.01019056875705719\n",
            "Epoch 328: accuracy: 0.8484666666666667, total loss: 180.07270675897598, average loss: 0.004001615705755022\n",
            "Validation: accuracy: 0.6558, total loss: 54.94345164299011, average loss: 0.010988690328598023\n",
            "Epoch 329: accuracy: 0.8436666666666667, total loss: 186.07911014556885, average loss: 0.0041350913365681965\n",
            "Validation: accuracy: 0.6512, total loss: 54.876503348350525, average loss: 0.010975300669670104\n",
            "Epoch 330: accuracy: 0.8496666666666667, total loss: 179.82006089389324, average loss: 0.003996001353197627\n",
            "Validation: accuracy: 0.66, total loss: 54.120058715343475, average loss: 0.010824011743068695\n",
            "Checkpoint saved as `/weights/330.pth`\n",
            "Epoch 331: accuracy: 0.8485111111111111, total loss: 179.50281190872192, average loss: 0.003988951375749376\n",
            "Validation: accuracy: 0.6602, total loss: 53.02032786607742, average loss: 0.010604065573215485\n",
            "Epoch 332: accuracy: 0.8506222222222222, total loss: 178.4460383951664, average loss: 0.003965467519892587\n",
            "Validation: accuracy: 0.6842, total loss: 48.90726429224014, average loss: 0.00978145285844803\n",
            "Epoch 333: accuracy: 0.8498444444444444, total loss: 181.224913418293, average loss: 0.004027220298184289\n",
            "Validation: accuracy: 0.6616, total loss: 52.89401763677597, average loss: 0.010578803527355194\n",
            "Epoch 334: accuracy: 0.8528666666666667, total loss: 177.4750971198082, average loss: 0.0039438910471068485\n",
            "Validation: accuracy: 0.6732, total loss: 50.503817200660706, average loss: 0.010100763440132141\n",
            "Epoch 335: accuracy: 0.8474666666666667, total loss: 182.135055154562, average loss: 0.004047445670101377\n",
            "Validation: accuracy: 0.67, total loss: 51.549906611442566, average loss: 0.010309981322288513\n",
            "Epoch 336: accuracy: 0.8509777777777778, total loss: 180.09751322865486, average loss: 0.004002166960636775\n",
            "Validation: accuracy: 0.6648, total loss: 52.70675092935562, average loss: 0.010541350185871124\n",
            "Epoch 337: accuracy: 0.8517555555555556, total loss: 178.63473784923553, average loss: 0.003969660841094123\n",
            "Validation: accuracy: 0.6628, total loss: 52.184489130973816, average loss: 0.010436897826194762\n",
            "Epoch 338: accuracy: 0.8481333333333333, total loss: 181.6241111755371, average loss: 0.00403609135945638\n",
            "Validation: accuracy: 0.6692, total loss: 52.213218212127686, average loss: 0.010442643642425538\n",
            "Epoch 339: accuracy: 0.8501111111111112, total loss: 178.59275570511818, average loss: 0.003968727904558182\n",
            "Validation: accuracy: 0.6432, total loss: 56.294514298439026, average loss: 0.011258902859687805\n",
            "Epoch 340: accuracy: 0.8505111111111111, total loss: 179.95042967796326, average loss: 0.003998898437288073\n",
            "Validation: accuracy: 0.6664, total loss: 51.99036157131195, average loss: 0.01039807231426239\n",
            "Epoch 341: accuracy: 0.8508, total loss: 178.098337829113, average loss: 0.003957740840646956\n",
            "Validation: accuracy: 0.6818, total loss: 49.484032928943634, average loss: 0.009896806585788728\n",
            "Epoch 342: accuracy: 0.8496444444444444, total loss: 181.6184365451336, average loss: 0.004035965256558524\n",
            "Validation: accuracy: 0.6656, total loss: 52.28639209270477, average loss: 0.010457278418540954\n",
            "Epoch 343: accuracy: 0.8481777777777778, total loss: 181.07890751957893, average loss: 0.00402397572265731\n",
            "Validation: accuracy: 0.6408, total loss: 57.09368646144867, average loss: 0.011418737292289734\n",
            "Epoch 344: accuracy: 0.8532444444444445, total loss: 177.58002281188965, average loss: 0.0039462227291531034\n",
            "Validation: accuracy: 0.6738, total loss: 51.7075429558754, average loss: 0.010341508591175079\n",
            "Epoch 345: accuracy: 0.8514, total loss: 177.80640324950218, average loss: 0.003951253405544493\n",
            "Validation: accuracy: 0.6544, total loss: 53.85887944698334, average loss: 0.010771775889396668\n",
            "Epoch 346: accuracy: 0.8516444444444444, total loss: 178.1591520011425, average loss: 0.003959092266692056\n",
            "Validation: accuracy: 0.668, total loss: 54.265585064888, average loss: 0.0108531170129776\n",
            "Epoch 347: accuracy: 0.8519777777777777, total loss: 177.20891290903091, average loss: 0.003937975842422909\n",
            "Validation: accuracy: 0.6716, total loss: 51.743876576423645, average loss: 0.010348775315284728\n",
            "Epoch 348: accuracy: 0.8468, total loss: 182.76171439886093, average loss: 0.004061371431085798\n",
            "Validation: accuracy: 0.6534, total loss: 55.110922157764435, average loss: 0.011022184431552886\n",
            "Epoch 349: accuracy: 0.8517111111111111, total loss: 176.98359084129333, average loss: 0.003932968685362074\n",
            "Validation: accuracy: 0.6496, total loss: 55.727582573890686, average loss: 0.011145516514778138\n",
            "Epoch 350: accuracy: 0.8506222222222222, total loss: 180.74233683943748, average loss: 0.004016496374209722\n",
            "Validation: accuracy: 0.65, total loss: 55.76295220851898, average loss: 0.011152590441703797\n",
            "Epoch 351: accuracy: 0.8494444444444444, total loss: 179.8060920536518, average loss: 0.0039956909345255955\n",
            "Validation: accuracy: 0.669, total loss: 52.38757663965225, average loss: 0.01047751532793045\n",
            "Epoch 352: accuracy: 0.8487111111111111, total loss: 180.6457665860653, average loss: 0.004014350368579229\n",
            "Validation: accuracy: 0.6754, total loss: 53.20171910524368, average loss: 0.010640343821048736\n",
            "Epoch 353: accuracy: 0.8501333333333333, total loss: 180.5164326429367, average loss: 0.004011476280954149\n",
            "Validation: accuracy: 0.6712, total loss: 53.18125516176224, average loss: 0.010636251032352448\n",
            "Epoch 354: accuracy: 0.8497333333333333, total loss: 179.18854954838753, average loss: 0.003981967767741945\n",
            "Validation: accuracy: 0.6462, total loss: 57.72150659561157, average loss: 0.011544301319122315\n",
            "Epoch 355: accuracy: 0.8507111111111111, total loss: 179.43006420135498, average loss: 0.00398733476003011\n",
            "Validation: accuracy: 0.6424, total loss: 58.33015155792236, average loss: 0.011666030311584473\n",
            "Epoch 356: accuracy: 0.8501333333333333, total loss: 180.28424452245235, average loss: 0.004006316544943386\n",
            "Validation: accuracy: 0.6462, total loss: 55.66337335109711, average loss: 0.011132674670219422\n",
            "Epoch 357: accuracy: 0.8496888888888889, total loss: 180.00649851560593, average loss: 0.004000144411457909\n",
            "Validation: accuracy: 0.6644, total loss: 51.32055974006653, average loss: 0.010264111948013306\n",
            "Epoch 358: accuracy: 0.8542, total loss: 177.22466623783112, average loss: 0.003938325916396247\n",
            "Validation: accuracy: 0.6782, total loss: 50.05362042784691, average loss: 0.010010724085569382\n",
            "Epoch 359: accuracy: 0.8508444444444444, total loss: 179.05455338954926, average loss: 0.003978990075323317\n",
            "Validation: accuracy: 0.6646, total loss: 52.75699359178543, average loss: 0.010551398718357086\n",
            "Epoch 360: accuracy: 0.8500222222222222, total loss: 178.0599600672722, average loss: 0.003956888001494937\n",
            "Validation: accuracy: 0.6734, total loss: 52.373767375946045, average loss: 0.010474753475189209\n",
            "Checkpoint saved as `/weights/360.pth`\n",
            "Epoch 361: accuracy: 0.8515333333333334, total loss: 176.77773243188858, average loss: 0.003928394054041969\n",
            "Validation: accuracy: 0.6474, total loss: 57.053385853767395, average loss: 0.011410677170753478\n",
            "Epoch 362: accuracy: 0.8501333333333333, total loss: 178.87078434228897, average loss: 0.003974906318717533\n",
            "Validation: accuracy: 0.6424, total loss: 57.99925249814987, average loss: 0.011599850499629975\n",
            "Epoch 363: accuracy: 0.8519555555555556, total loss: 176.4199877679348, average loss: 0.003920444172620773\n",
            "Validation: accuracy: 0.6582, total loss: 55.663408160209656, average loss: 0.011132681632041931\n",
            "Epoch 364: accuracy: 0.8516444444444444, total loss: 178.7965050637722, average loss: 0.003973255668083826\n",
            "Validation: accuracy: 0.64, total loss: 57.63498640060425, average loss: 0.011526997280120849\n",
            "Epoch 365: accuracy: 0.8490666666666666, total loss: 180.25253224372864, average loss: 0.0040056118276384145\n",
            "Validation: accuracy: 0.6594, total loss: 52.41001236438751, average loss: 0.010482002472877502\n",
            "Epoch 366: accuracy: 0.8514, total loss: 178.54044416546822, average loss: 0.003967565425899293\n",
            "Validation: accuracy: 0.6836, total loss: 49.346249639987946, average loss: 0.009869249927997589\n",
            "Epoch 367: accuracy: 0.8536, total loss: 175.71218171715736, average loss: 0.0039047151492701635\n",
            "Validation: accuracy: 0.6448, total loss: 56.686758279800415, average loss: 0.011337351655960084\n",
            "Epoch 368: accuracy: 0.8535111111111111, total loss: 174.89053270220757, average loss: 0.003886456282271279\n",
            "Validation: accuracy: 0.6814, total loss: 48.498548328876495, average loss: 0.009699709665775299\n",
            "Epoch 369: accuracy: 0.8546444444444444, total loss: 176.54014725983143, average loss: 0.003923114383551809\n",
            "Validation: accuracy: 0.6508, total loss: 54.08503985404968, average loss: 0.010817007970809936\n",
            "Epoch 370: accuracy: 0.8525555555555555, total loss: 177.43644639849663, average loss: 0.003943032142188814\n",
            "Validation: accuracy: 0.6522, total loss: 57.18437194824219, average loss: 0.011436874389648438\n",
            "Epoch 371: accuracy: 0.8525333333333334, total loss: 176.79533299803734, average loss: 0.003928785177734163\n",
            "Validation: accuracy: 0.6776, total loss: 50.6718915104866, average loss: 0.01013437830209732\n",
            "Epoch 372: accuracy: 0.8546888888888889, total loss: 173.5445677638054, average loss: 0.0038565459503067862\n",
            "Validation: accuracy: 0.6166, total loss: 62.831602454185486, average loss: 0.012566320490837097\n",
            "Epoch 373: accuracy: 0.8498666666666667, total loss: 179.28824040293694, average loss: 0.0039841831200652655\n",
            "Validation: accuracy: 0.641, total loss: 56.35640478134155, average loss: 0.011271280956268311\n",
            "Epoch 374: accuracy: 0.851, total loss: 178.80604130029678, average loss: 0.003973467584451039\n",
            "Validation: accuracy: 0.6672, total loss: 54.44245934486389, average loss: 0.010888491868972778\n",
            "Epoch 375: accuracy: 0.8521333333333333, total loss: 177.9411543905735, average loss: 0.003954247875346078\n",
            "Validation: accuracy: 0.6676, total loss: 51.1781530380249, average loss: 0.010235630607604981\n",
            "Epoch 376: accuracy: 0.8546666666666667, total loss: 173.60723999142647, average loss: 0.003857938666476144\n",
            "Validation: accuracy: 0.6544, total loss: 54.49946731328964, average loss: 0.010899893462657928\n",
            "Epoch 377: accuracy: 0.8499333333333333, total loss: 179.4252794533968, average loss: 0.003987228432297707\n",
            "Validation: accuracy: 0.66, total loss: 53.87587088346481, average loss: 0.010775174176692963\n",
            "Epoch 378: accuracy: 0.8527333333333333, total loss: 176.0979544967413, average loss: 0.003913287877705362\n",
            "Validation: accuracy: 0.6532, total loss: 55.64774703979492, average loss: 0.011129549407958985\n",
            "Epoch 379: accuracy: 0.8506444444444444, total loss: 177.63179939985275, average loss: 0.003947373319996728\n",
            "Validation: accuracy: 0.6778, total loss: 51.160301089286804, average loss: 0.01023206021785736\n",
            "Epoch 380: accuracy: 0.8513111111111111, total loss: 178.75309282541275, average loss: 0.003972290951675839\n",
            "Validation: accuracy: 0.672, total loss: 52.296555519104004, average loss: 0.010459311103820802\n",
            "Epoch 381: accuracy: 0.8533333333333334, total loss: 175.2584199309349, average loss: 0.003894631554020776\n",
            "Validation: accuracy: 0.6884, total loss: 49.75734668970108, average loss: 0.009951469337940217\n",
            "Epoch 382: accuracy: 0.8520222222222222, total loss: 178.37357673048973, average loss: 0.003963857260677549\n",
            "Validation: accuracy: 0.6762, total loss: 51.23156899213791, average loss: 0.010246313798427581\n",
            "Epoch 383: accuracy: 0.8521777777777778, total loss: 174.8335984647274, average loss: 0.003885191076993942\n",
            "Validation: accuracy: 0.661, total loss: 53.16671979427338, average loss: 0.010633343958854674\n",
            "Epoch 384: accuracy: 0.8527777777777777, total loss: 177.70552864670753, average loss: 0.003949011747704612\n",
            "Validation: accuracy: 0.6562, total loss: 54.723414182662964, average loss: 0.010944682836532592\n",
            "Epoch 385: accuracy: 0.8547777777777777, total loss: 174.94708317518234, average loss: 0.0038877129594484965\n",
            "Validation: accuracy: 0.6754, total loss: 50.32890671491623, average loss: 0.010065781342983245\n",
            "Epoch 386: accuracy: 0.8525333333333334, total loss: 176.1327842026949, average loss: 0.003914061871170997\n",
            "Validation: accuracy: 0.6526, total loss: 57.41881227493286, average loss: 0.011483762454986573\n",
            "Epoch 387: accuracy: 0.8526888888888889, total loss: 176.5866974890232, average loss: 0.0039241488330894045\n",
            "Validation: accuracy: 0.679, total loss: 51.55499118566513, average loss: 0.010310998237133027\n",
            "Epoch 388: accuracy: 0.8533111111111111, total loss: 175.1327120512724, average loss: 0.003891838045583831\n",
            "Validation: accuracy: 0.6736, total loss: 51.40374666452408, average loss: 0.010280749332904815\n",
            "Epoch 389: accuracy: 0.8535555555555555, total loss: 176.87483069300652, average loss: 0.003930551793177923\n",
            "Validation: accuracy: 0.6626, total loss: 53.626679480075836, average loss: 0.010725335896015167\n",
            "Epoch 390: accuracy: 0.8520222222222222, total loss: 176.44235192239285, average loss: 0.003920941153830952\n",
            "Validation: accuracy: 0.6562, total loss: 54.89902472496033, average loss: 0.010979804944992065\n",
            "Checkpoint saved as `/weights/390.pth`\n",
            "Epoch 391: accuracy: 0.8518, total loss: 178.86071559786797, average loss: 0.003974682568841511\n",
            "Validation: accuracy: 0.6404, total loss: 56.72915160655975, average loss: 0.01134583032131195\n",
            "Epoch 392: accuracy: 0.8554, total loss: 172.72726421058178, average loss: 0.0038383836491240396\n",
            "Validation: accuracy: 0.6696, total loss: 52.30262917280197, average loss: 0.010460525834560395\n",
            "Epoch 393: accuracy: 0.8527777777777777, total loss: 177.27859687805176, average loss: 0.003939524375067817\n",
            "Validation: accuracy: 0.683, total loss: 48.08058679103851, average loss: 0.009616117358207703\n",
            "Epoch 394: accuracy: 0.8528, total loss: 177.02539232373238, average loss: 0.003933897607194052\n",
            "Validation: accuracy: 0.675, total loss: 53.29732781648636, average loss: 0.010659465563297272\n",
            "Epoch 395: accuracy: 0.8513111111111111, total loss: 178.1325051188469, average loss: 0.003958500113752153\n",
            "Validation: accuracy: 0.6688, total loss: 51.416283428668976, average loss: 0.010283256685733795\n",
            "Epoch 396: accuracy: 0.8538, total loss: 176.16155138611794, average loss: 0.003914701141913732\n",
            "Validation: accuracy: 0.6478, total loss: 57.67888045310974, average loss: 0.011535776090621949\n",
            "Epoch 397: accuracy: 0.8565555555555555, total loss: 172.97864520549774, average loss: 0.0038439698934555054\n",
            "Validation: accuracy: 0.6498, total loss: 57.336695194244385, average loss: 0.011467339038848877\n",
            "Epoch 398: accuracy: 0.8507555555555556, total loss: 177.2622337937355, average loss: 0.0039391607509719\n",
            "Validation: accuracy: 0.6642, total loss: 55.145575165748596, average loss: 0.01102911503314972\n",
            "Epoch 399: accuracy: 0.8546, total loss: 174.78860026597977, average loss: 0.0038841911170217724\n",
            "Validation: accuracy: 0.6696, total loss: 52.73918694257736, average loss: 0.010547837388515472\n",
            "Epoch 400: accuracy: 0.8552, total loss: 172.48868361115456, average loss: 0.003833081858025657\n",
            "Validation: accuracy: 0.6476, total loss: 56.92162108421326, average loss: 0.011384324216842651\n",
            "Epoch 401: accuracy: 0.8545555555555555, total loss: 174.61135306954384, average loss: 0.0038802522904343075\n",
            "Validation: accuracy: 0.6608, total loss: 54.973002433776855, average loss: 0.01099460048675537\n",
            "Epoch 402: accuracy: 0.8557555555555556, total loss: 173.06299471855164, average loss: 0.0038458443270789253\n",
            "Validation: accuracy: 0.6634, total loss: 54.43837821483612, average loss: 0.010887675642967224\n",
            "Epoch 403: accuracy: 0.8534444444444444, total loss: 175.66957001388073, average loss: 0.003903768222530683\n",
            "Validation: accuracy: 0.6884, total loss: 49.208373844623566, average loss: 0.009841674768924714\n",
            "Epoch 404: accuracy: 0.8558, total loss: 174.02327582240105, average loss: 0.003867183907164468\n",
            "Validation: accuracy: 0.6792, total loss: 50.02602553367615, average loss: 0.01000520510673523\n",
            "Epoch 405: accuracy: 0.8536222222222222, total loss: 173.62298783659935, average loss: 0.003858288618591097\n",
            "Validation: accuracy: 0.6674, total loss: 52.29086101055145, average loss: 0.01045817220211029\n",
            "Epoch 406: accuracy: 0.8521555555555556, total loss: 179.27250760793686, average loss: 0.003983833502398597\n",
            "Validation: accuracy: 0.6062, total loss: 64.5060156583786, average loss: 0.01290120313167572\n",
            "Epoch 407: accuracy: 0.8548888888888889, total loss: 175.56113561987877, average loss: 0.0039013585693306392\n",
            "Validation: accuracy: 0.6642, total loss: 52.60718059539795, average loss: 0.01052143611907959\n",
            "Epoch 408: accuracy: 0.8551111111111112, total loss: 174.7690392434597, average loss: 0.0038837564276324376\n",
            "Validation: accuracy: 0.6586, total loss: 53.18718993663788, average loss: 0.010637437987327575\n",
            "Epoch 409: accuracy: 0.8537777777777777, total loss: 174.7751027047634, average loss: 0.0038838911712169646\n",
            "Validation: accuracy: 0.673, total loss: 52.285990715026855, average loss: 0.01045719814300537\n",
            "Epoch 410: accuracy: 0.8525555555555555, total loss: 176.74112251400948, average loss: 0.003927580500311322\n",
            "Validation: accuracy: 0.6538, total loss: 54.28373110294342, average loss: 0.010856746220588685\n",
            "Epoch 411: accuracy: 0.8538888888888889, total loss: 174.81900662183762, average loss: 0.003884866813818614\n",
            "Validation: accuracy: 0.6652, total loss: 53.26959586143494, average loss: 0.010653919172286988\n",
            "Epoch 412: accuracy: 0.8561333333333333, total loss: 174.90115723013878, average loss: 0.0038866923828919727\n",
            "Validation: accuracy: 0.6434, total loss: 55.556195855140686, average loss: 0.011111239171028138\n",
            "Epoch 413: accuracy: 0.8526, total loss: 175.95772798359394, average loss: 0.003910171732968754\n",
            "Validation: accuracy: 0.6508, total loss: 53.43611866235733, average loss: 0.010687223732471466\n",
            "Epoch 414: accuracy: 0.8557111111111111, total loss: 174.5548369884491, average loss: 0.0038789963775210912\n",
            "Validation: accuracy: 0.6462, total loss: 56.88960802555084, average loss: 0.011377921605110168\n",
            "Epoch 415: accuracy: 0.8559333333333333, total loss: 174.15573209524155, average loss: 0.003870127379894257\n",
            "Validation: accuracy: 0.656, total loss: 55.728567361831665, average loss: 0.011145713472366333\n",
            "Epoch 416: accuracy: 0.8522666666666666, total loss: 177.9208409488201, average loss: 0.0039537964655293354\n",
            "Validation: accuracy: 0.6698, total loss: 50.39802569150925, average loss: 0.01007960513830185\n",
            "Epoch 417: accuracy: 0.8541777777777778, total loss: 174.49812361598015, average loss: 0.0038777360803551146\n",
            "Validation: accuracy: 0.6576, total loss: 55.74086672067642, average loss: 0.011148173344135284\n",
            "Epoch 418: accuracy: 0.8562666666666666, total loss: 174.7228747010231, average loss: 0.0038827305489116245\n",
            "Validation: accuracy: 0.6592, total loss: 54.906162202358246, average loss: 0.01098123244047165\n",
            "Epoch 419: accuracy: 0.8557333333333333, total loss: 173.08771395683289, average loss: 0.0038463936434851753\n",
            "Validation: accuracy: 0.6638, total loss: 53.19560715556145, average loss: 0.01063912143111229\n",
            "Epoch 420: accuracy: 0.8578666666666667, total loss: 169.7094415873289, average loss: 0.0037713209241628646\n",
            "Validation: accuracy: 0.6662, total loss: 54.011051416397095, average loss: 0.010802210283279419\n",
            "Checkpoint saved as `/weights/420.pth`\n",
            "Epoch 421: accuracy: 0.8539333333333333, total loss: 174.9501111805439, average loss: 0.003887780248456531\n",
            "Validation: accuracy: 0.6764, total loss: 52.09794795513153, average loss: 0.010419589591026306\n",
            "Epoch 422: accuracy: 0.8492888888888889, total loss: 180.87583589553833, average loss: 0.004019463019900852\n",
            "Validation: accuracy: 0.6402, total loss: 57.67124605178833, average loss: 0.011534249210357665\n",
            "Epoch 423: accuracy: 0.8532666666666666, total loss: 174.69141879677773, average loss: 0.0038820315288172827\n",
            "Validation: accuracy: 0.6594, total loss: 53.89005249738693, average loss: 0.010778010499477386\n",
            "Epoch 424: accuracy: 0.8546222222222222, total loss: 174.37269434332848, average loss: 0.0038749487631850772\n",
            "Validation: accuracy: 0.6764, total loss: 51.065169632434845, average loss: 0.010213033926486969\n",
            "Epoch 425: accuracy: 0.8516888888888889, total loss: 177.47761225700378, average loss: 0.003943946939044528\n",
            "Validation: accuracy: 0.6458, total loss: 57.5905921459198, average loss: 0.01151811842918396\n",
            "Epoch 426: accuracy: 0.8541777777777778, total loss: 175.021608710289, average loss: 0.0038893690824508666\n",
            "Validation: accuracy: 0.6672, total loss: 51.27088153362274, average loss: 0.010254176306724549\n",
            "Epoch 427: accuracy: 0.8550666666666666, total loss: 174.66878373920918, average loss: 0.0038815285275379816\n",
            "Validation: accuracy: 0.6514, total loss: 58.43727898597717, average loss: 0.011687455797195434\n",
            "Epoch 428: accuracy: 0.8521555555555556, total loss: 176.0541554391384, average loss: 0.003912314565314187\n",
            "Validation: accuracy: 0.658, total loss: 55.51523756980896, average loss: 0.011103047513961793\n",
            "Epoch 429: accuracy: 0.8527111111111111, total loss: 175.92599561810493, average loss: 0.00390946656929122\n",
            "Validation: accuracy: 0.6684, total loss: 52.71855807304382, average loss: 0.010543711614608764\n",
            "Epoch 430: accuracy: 0.8550666666666666, total loss: 171.71393033862114, average loss: 0.0038158651186360254\n",
            "Validation: accuracy: 0.6544, total loss: 54.3787242770195, average loss: 0.0108757448554039\n",
            "Epoch 431: accuracy: 0.8543555555555555, total loss: 175.1448693871498, average loss: 0.003892108208603329\n",
            "Validation: accuracy: 0.672, total loss: 50.307977080345154, average loss: 0.010061595416069031\n",
            "Epoch 432: accuracy: 0.8550666666666666, total loss: 173.60239273309708, average loss: 0.0038578309496243796\n",
            "Validation: accuracy: 0.6598, total loss: 54.3037725687027, average loss: 0.01086075451374054\n",
            "Epoch 433: accuracy: 0.8541111111111112, total loss: 174.63666486740112, average loss: 0.003880814774831136\n",
            "Validation: accuracy: 0.685, total loss: 49.061037480831146, average loss: 0.00981220749616623\n",
            "Epoch 434: accuracy: 0.8543111111111111, total loss: 173.89520582556725, average loss: 0.003864337907234828\n",
            "Validation: accuracy: 0.6912, total loss: 47.43334859609604, average loss: 0.009486669719219208\n",
            "Epoch 435: accuracy: 0.8550666666666666, total loss: 172.7301539182663, average loss: 0.003838447864850362\n",
            "Validation: accuracy: 0.6482, total loss: 55.23407602310181, average loss: 0.011046815204620362\n",
            "Epoch 436: accuracy: 0.8576666666666667, total loss: 170.93503531813622, average loss: 0.003798556340403027\n",
            "Validation: accuracy: 0.6222, total loss: 60.69122099876404, average loss: 0.012138244199752807\n",
            "Epoch 437: accuracy: 0.8552666666666666, total loss: 174.364048615098, average loss: 0.003874756635891067\n",
            "Validation: accuracy: 0.6896, total loss: 48.12286180257797, average loss: 0.009624572360515594\n",
            "Epoch 438: accuracy: 0.8547555555555556, total loss: 174.1711349785328, average loss: 0.0038704696661896174\n",
            "Validation: accuracy: 0.672, total loss: 52.43881821632385, average loss: 0.01048776364326477\n",
            "Epoch 439: accuracy: 0.8544, total loss: 175.9987354874611, average loss: 0.003911083010832469\n",
            "Validation: accuracy: 0.638, total loss: 58.23447370529175, average loss: 0.01164689474105835\n",
            "Epoch 440: accuracy: 0.8556666666666667, total loss: 173.40629637241364, average loss: 0.003853473252720303\n",
            "Validation: accuracy: 0.6752, total loss: 51.832824528217316, average loss: 0.010366564905643463\n",
            "Epoch 441: accuracy: 0.8544222222222222, total loss: 173.2406765818596, average loss: 0.003849792812930213\n",
            "Validation: accuracy: 0.6404, total loss: 56.87548303604126, average loss: 0.011375096607208252\n",
            "Epoch 442: accuracy: 0.8525777777777778, total loss: 176.41922849416733, average loss: 0.003920427299870385\n",
            "Validation: accuracy: 0.6692, total loss: 51.60942530632019, average loss: 0.010321885061264038\n",
            "Epoch 443: accuracy: 0.8580888888888889, total loss: 172.42351186275482, average loss: 0.003831633596950107\n",
            "Validation: accuracy: 0.655, total loss: 55.462677121162415, average loss: 0.011092535424232483\n",
            "Epoch 444: accuracy: 0.8564222222222222, total loss: 173.0921515226364, average loss: 0.003846492256058587\n",
            "Validation: accuracy: 0.6838, total loss: 50.30166566371918, average loss: 0.010060333132743836\n",
            "Epoch 445: accuracy: 0.8507111111111111, total loss: 177.60824713110924, average loss: 0.003946849936246872\n",
            "Validation: accuracy: 0.6888, total loss: 47.49333292245865, average loss: 0.00949866658449173\n",
            "Epoch 446: accuracy: 0.8568444444444444, total loss: 172.86547881364822, average loss: 0.0038414550847477385\n",
            "Validation: accuracy: 0.6596, total loss: 54.25292545557022, average loss: 0.010850585091114045\n",
            "Epoch 447: accuracy: 0.8531333333333333, total loss: 174.2503478229046, average loss: 0.0038722299516201018\n",
            "Validation: accuracy: 0.6664, total loss: 53.375414311885834, average loss: 0.010675082862377167\n",
            "Epoch 448: accuracy: 0.8525555555555555, total loss: 177.96910986304283, average loss: 0.003954869108067619\n",
            "Validation: accuracy: 0.637, total loss: 60.28816258907318, average loss: 0.012057632517814637\n",
            "Epoch 449: accuracy: 0.8567777777777777, total loss: 170.70910796523094, average loss: 0.0037935357325606877\n",
            "Validation: accuracy: 0.6804, total loss: 48.37279415130615, average loss: 0.009674558830261231\n",
            "Epoch 450: accuracy: 0.8569777777777777, total loss: 171.87387132644653, average loss: 0.003819419362809923\n",
            "Validation: accuracy: 0.6834, total loss: 50.10825473070145, average loss: 0.01002165094614029\n",
            "Checkpoint saved as `/weights/450.pth`\n",
            "Epoch 451: accuracy: 0.8534, total loss: 174.55082434415817, average loss: 0.0038789072076479596\n",
            "Validation: accuracy: 0.6794, total loss: 50.40666711330414, average loss: 0.010081333422660828\n",
            "Epoch 452: accuracy: 0.8558888888888889, total loss: 171.9924191236496, average loss: 0.0038220537583033244\n",
            "Validation: accuracy: 0.6872, total loss: 48.33467626571655, average loss: 0.00966693525314331\n",
            "Epoch 453: accuracy: 0.8592888888888889, total loss: 168.87413921952248, average loss: 0.0037527586493227216\n",
            "Validation: accuracy: 0.6844, total loss: 48.13985073566437, average loss: 0.009627970147132873\n",
            "Epoch 454: accuracy: 0.8582222222222222, total loss: 169.40789517760277, average loss: 0.003764619892835617\n",
            "Validation: accuracy: 0.6704, total loss: 53.41009485721588, average loss: 0.010682018971443176\n",
            "Epoch 455: accuracy: 0.8552888888888889, total loss: 172.64275541901588, average loss: 0.0038365056759781307\n",
            "Validation: accuracy: 0.6686, total loss: 52.00124531984329, average loss: 0.010400249063968659\n",
            "Epoch 456: accuracy: 0.8578666666666667, total loss: 171.8669751882553, average loss: 0.0038192661152945625\n",
            "Validation: accuracy: 0.67, total loss: 52.39215499162674, average loss: 0.010478430998325348\n",
            "Epoch 457: accuracy: 0.8574444444444445, total loss: 171.13460952043533, average loss: 0.003802991322676341\n",
            "Validation: accuracy: 0.673, total loss: 51.99243777990341, average loss: 0.010398487555980683\n",
            "Epoch 458: accuracy: 0.8592666666666666, total loss: 171.39193323254585, average loss: 0.003808709627389908\n",
            "Validation: accuracy: 0.658, total loss: 54.129930555820465, average loss: 0.010825986111164093\n",
            "Epoch 459: accuracy: 0.8563777777777778, total loss: 172.20477136969566, average loss: 0.003826772697104348\n",
            "Validation: accuracy: 0.6654, total loss: 52.16021555662155, average loss: 0.010432043111324311\n",
            "Epoch 460: accuracy: 0.8573777777777778, total loss: 172.5545804798603, average loss: 0.0038345462328857848\n",
            "Validation: accuracy: 0.66, total loss: 54.43304789066315, average loss: 0.010886609578132629\n",
            "Epoch 461: accuracy: 0.8576, total loss: 171.8523522466421, average loss: 0.0038189411610364916\n",
            "Validation: accuracy: 0.6698, total loss: 53.284587383270264, average loss: 0.010656917476654053\n",
            "Epoch 462: accuracy: 0.8551111111111112, total loss: 174.57236996293068, average loss: 0.0038793859991762374\n",
            "Validation: accuracy: 0.65, total loss: 56.16702377796173, average loss: 0.011233404755592346\n",
            "Epoch 463: accuracy: 0.8570888888888889, total loss: 172.14125536382198, average loss: 0.003825361230307155\n",
            "Validation: accuracy: 0.6694, total loss: 52.510315001010895, average loss: 0.01050206300020218\n",
            "Epoch 464: accuracy: 0.8581111111111112, total loss: 170.1977488398552, average loss: 0.0037821721964412264\n",
            "Validation: accuracy: 0.6604, total loss: 54.83618438243866, average loss: 0.010967236876487731\n",
            "Epoch 465: accuracy: 0.8561333333333333, total loss: 172.63882276415825, average loss: 0.003836418283647961\n",
            "Validation: accuracy: 0.675, total loss: 51.58821505308151, average loss: 0.010317643010616303\n",
            "Epoch 466: accuracy: 0.8540444444444445, total loss: 172.83381900191307, average loss: 0.003840751533375846\n",
            "Validation: accuracy: 0.6644, total loss: 52.72576403617859, average loss: 0.010545152807235718\n",
            "Epoch 467: accuracy: 0.8584, total loss: 170.94524313509464, average loss: 0.003798783180779881\n",
            "Validation: accuracy: 0.662, total loss: 54.78320449590683, average loss: 0.010956640899181367\n",
            "Epoch 468: accuracy: 0.8551777777777778, total loss: 173.44204780459404, average loss: 0.0038542677289909785\n",
            "Validation: accuracy: 0.6726, total loss: 51.83206379413605, average loss: 0.01036641275882721\n",
            "Epoch 469: accuracy: 0.8542444444444445, total loss: 173.3064389526844, average loss: 0.0038512541989485422\n",
            "Validation: accuracy: 0.6356, total loss: 59.433603048324585, average loss: 0.011886720609664917\n",
            "Epoch 470: accuracy: 0.859, total loss: 169.13624307513237, average loss: 0.003758583179447386\n",
            "Validation: accuracy: 0.6684, total loss: 52.6817347407341, average loss: 0.01053634694814682\n",
            "Epoch 471: accuracy: 0.8583111111111111, total loss: 169.5517950952053, average loss: 0.00376781766878234\n",
            "Validation: accuracy: 0.6608, total loss: 53.9987975358963, average loss: 0.01079975950717926\n",
            "Epoch 472: accuracy: 0.8549333333333333, total loss: 173.0815708041191, average loss: 0.0038462571289804245\n",
            "Validation: accuracy: 0.6736, total loss: 51.04915416240692, average loss: 0.010209830832481384\n",
            "Epoch 473: accuracy: 0.8562, total loss: 171.34188923239708, average loss: 0.003807597538497713\n",
            "Validation: accuracy: 0.6894, total loss: 47.587598383426666, average loss: 0.009517519676685333\n",
            "Epoch 474: accuracy: 0.8544666666666667, total loss: 174.66455958783627, average loss: 0.0038814346575074725\n",
            "Validation: accuracy: 0.6542, total loss: 55.63451266288757, average loss: 0.011126902532577514\n",
            "Epoch 475: accuracy: 0.8607555555555556, total loss: 167.6321346461773, average loss: 0.0037251585476928287\n",
            "Validation: accuracy: 0.6538, total loss: 54.49942284822464, average loss: 0.010899884569644929\n",
            "Epoch 476: accuracy: 0.8556888888888889, total loss: 173.2901450395584, average loss: 0.003850892111990187\n",
            "Validation: accuracy: 0.6832, total loss: 50.32040935754776, average loss: 0.010064081871509553\n",
            "Epoch 477: accuracy: 0.8558444444444444, total loss: 172.2113764435053, average loss: 0.0038269194765223396\n",
            "Validation: accuracy: 0.6638, total loss: 52.556129693984985, average loss: 0.010511225938796998\n",
            "Epoch 478: accuracy: 0.8574888888888889, total loss: 170.7879909723997, average loss: 0.0037952886882755494\n",
            "Validation: accuracy: 0.6644, total loss: 54.098002910614014, average loss: 0.010819600582122803\n",
            "Epoch 479: accuracy: 0.8600666666666666, total loss: 168.91542191803455, average loss: 0.0037536760426229903\n",
            "Validation: accuracy: 0.6824, total loss: 49.24411857128143, average loss: 0.009848823714256287\n",
            "Epoch 480: accuracy: 0.8557333333333333, total loss: 173.98447966575623, average loss: 0.0038663217703501385\n",
            "Validation: accuracy: 0.643, total loss: 57.70555067062378, average loss: 0.011541110134124757\n",
            "Checkpoint saved as `/weights/480.pth`\n",
            "Epoch 481: accuracy: 0.8566444444444444, total loss: 170.7212575674057, average loss: 0.0037938057237201267\n",
            "Validation: accuracy: 0.6476, total loss: 57.1634464263916, average loss: 0.011432689285278321\n",
            "Epoch 482: accuracy: 0.8558444444444444, total loss: 172.36091059446335, average loss: 0.003830242457654741\n",
            "Validation: accuracy: 0.674, total loss: 48.91008321940899, average loss: 0.009782016643881798\n",
            "Epoch 483: accuracy: 0.8563555555555555, total loss: 171.8477002978325, average loss: 0.0038188377843962774\n",
            "Validation: accuracy: 0.6548, total loss: 54.67197513580322, average loss: 0.010934395027160645\n",
            "Epoch 484: accuracy: 0.8561111111111112, total loss: 173.0675424337387, average loss: 0.0038459453874164157\n",
            "Validation: accuracy: 0.6678, total loss: 52.54118776321411, average loss: 0.010508237552642822\n",
            "Epoch 485: accuracy: 0.8574, total loss: 171.45701664686203, average loss: 0.003810155925485823\n",
            "Validation: accuracy: 0.6602, total loss: 51.654118835926056, average loss: 0.01033082376718521\n",
            "Epoch 486: accuracy: 0.8570444444444445, total loss: 171.42159029841423, average loss: 0.003809368673298094\n",
            "Validation: accuracy: 0.6462, total loss: 57.555965423583984, average loss: 0.011511193084716797\n",
            "Epoch 487: accuracy: 0.8558, total loss: 171.48930096626282, average loss: 0.0038108733548058403\n",
            "Validation: accuracy: 0.6678, total loss: 50.62613499164581, average loss: 0.010125226998329163\n",
            "Epoch 488: accuracy: 0.8584666666666667, total loss: 168.1903548836708, average loss: 0.0037375634418593514\n",
            "Validation: accuracy: 0.647, total loss: 55.8946373462677, average loss: 0.01117892746925354\n",
            "Epoch 489: accuracy: 0.8569555555555556, total loss: 173.0798351019621, average loss: 0.0038462185578213796\n",
            "Validation: accuracy: 0.6684, total loss: 52.97822558879852, average loss: 0.010595645117759704\n",
            "Epoch 490: accuracy: 0.8569777777777777, total loss: 172.18313390016556, average loss: 0.0038262918644481237\n",
            "Validation: accuracy: 0.6762, total loss: 50.67680186033249, average loss: 0.010135360372066499\n",
            "Epoch 491: accuracy: 0.8616, total loss: 167.9732974767685, average loss: 0.0037327399439281888\n",
            "Validation: accuracy: 0.6184, total loss: 64.2736668586731, average loss: 0.012854733371734619\n",
            "Epoch 492: accuracy: 0.8571777777777778, total loss: 170.16047835350037, average loss: 0.003781343963411119\n",
            "Validation: accuracy: 0.6356, total loss: 59.29483938217163, average loss: 0.011858967876434326\n",
            "Epoch 493: accuracy: 0.8559555555555556, total loss: 172.65032157301903, average loss: 0.0038366738127337562\n",
            "Validation: accuracy: 0.6792, total loss: 48.798606276512146, average loss: 0.00975972125530243\n",
            "Epoch 494: accuracy: 0.8582444444444445, total loss: 171.5100341439247, average loss: 0.003811334092087216\n",
            "Validation: accuracy: 0.6622, total loss: 53.09313189983368, average loss: 0.010618626379966736\n",
            "Epoch 495: accuracy: 0.8597111111111111, total loss: 169.81416453421116, average loss: 0.003773648100760248\n",
            "Validation: accuracy: 0.6792, total loss: 49.40593618154526, average loss: 0.009881187236309052\n",
            "Epoch 496: accuracy: 0.8598444444444444, total loss: 169.45839013159275, average loss: 0.0037657420029242835\n",
            "Validation: accuracy: 0.6756, total loss: 50.06659460067749, average loss: 0.010013318920135498\n",
            "Epoch 497: accuracy: 0.8570444444444445, total loss: 170.34020698070526, average loss: 0.0037853379329045615\n",
            "Validation: accuracy: 0.657, total loss: 53.48264044523239, average loss: 0.010696528089046479\n",
            "Epoch 498: accuracy: 0.8568222222222223, total loss: 171.72691723704338, average loss: 0.003816153716378742\n",
            "Validation: accuracy: 0.676, total loss: 51.24230372905731, average loss: 0.010248460745811462\n",
            "Epoch 499: accuracy: 0.8573555555555555, total loss: 170.08707191050053, average loss: 0.0037797127091222338\n",
            "Validation: accuracy: 0.6898, total loss: 47.787831008434296, average loss: 0.00955756620168686\n",
            "Epoch 500: accuracy: 0.8589333333333333, total loss: 169.40026304125786, average loss: 0.00376445028980573\n",
            "Validation: accuracy: 0.6688, total loss: 51.26181608438492, average loss: 0.010252363216876984\n",
            "Epoch 501: accuracy: 0.8552444444444445, total loss: 170.98094822466373, average loss: 0.0037995766272147496\n",
            "Validation: accuracy: 0.6398, total loss: 57.561540365219116, average loss: 0.011512308073043823\n",
            "Epoch 502: accuracy: 0.8590444444444445, total loss: 170.3841810822487, average loss: 0.003786315135161082\n",
            "Validation: accuracy: 0.669, total loss: 52.12100434303284, average loss: 0.010424200868606567\n",
            "Epoch 503: accuracy: 0.8546666666666667, total loss: 173.4040784239769, average loss: 0.0038534239649772642\n",
            "Validation: accuracy: 0.6768, total loss: 50.9542373418808, average loss: 0.01019084746837616\n",
            "Epoch 504: accuracy: 0.8575111111111111, total loss: 170.0644870698452, average loss: 0.0037792108237743377\n",
            "Validation: accuracy: 0.6372, total loss: 57.333460569381714, average loss: 0.011466692113876343\n",
            "Epoch 505: accuracy: 0.8557555555555556, total loss: 171.18678227066994, average loss: 0.0038041507171259984\n",
            "Validation: accuracy: 0.6854, total loss: 50.212204813957214, average loss: 0.010042440962791443\n",
            "Epoch 506: accuracy: 0.8578, total loss: 169.4386985898018, average loss: 0.0037653044131067065\n",
            "Validation: accuracy: 0.6502, total loss: 57.29732120037079, average loss: 0.011459464240074158\n",
            "Epoch 507: accuracy: 0.8601111111111112, total loss: 168.02023649215698, average loss: 0.003733783033159044\n",
            "Validation: accuracy: 0.6414, total loss: 55.98409575223923, average loss: 0.011196819150447845\n",
            "Epoch 508: accuracy: 0.8632222222222222, total loss: 166.27036049962044, average loss: 0.0036948968999915652\n",
            "Validation: accuracy: 0.6714, total loss: 52.43946272134781, average loss: 0.010487892544269562\n",
            "Epoch 509: accuracy: 0.8600888888888889, total loss: 168.5554355084896, average loss: 0.0037456763446331023\n",
            "Validation: accuracy: 0.6498, total loss: 55.377458930015564, average loss: 0.011075491786003113\n",
            "Epoch 510: accuracy: 0.8566666666666667, total loss: 171.78463596105576, average loss: 0.003817436354690128\n",
            "Validation: accuracy: 0.6398, total loss: 58.26622140407562, average loss: 0.011653244280815125\n",
            "Checkpoint saved as `/weights/510.pth`\n",
            "Epoch 511: accuracy: 0.8551111111111112, total loss: 172.22414237260818, average loss: 0.0038272031638357375\n",
            "Validation: accuracy: 0.6244, total loss: 62.6340366601944, average loss: 0.01252680733203888\n",
            "Epoch 512: accuracy: 0.8617111111111111, total loss: 167.72413527965546, average loss: 0.003727203006214566\n",
            "Validation: accuracy: 0.655, total loss: 55.4955393075943, average loss: 0.01109910786151886\n",
            "Epoch 513: accuracy: 0.8538444444444444, total loss: 175.5255985558033, average loss: 0.003900568856795629\n",
            "Validation: accuracy: 0.6808, total loss: 50.626061499118805, average loss: 0.010125212299823761\n",
            "Epoch 514: accuracy: 0.8571555555555556, total loss: 171.32116390764713, average loss: 0.0038071369757254917\n",
            "Validation: accuracy: 0.6668, total loss: 52.714843451976776, average loss: 0.010542968690395355\n",
            "Epoch 515: accuracy: 0.8585777777777778, total loss: 173.05935686826706, average loss: 0.00384576348596149\n",
            "Validation: accuracy: 0.6804, total loss: 50.28024113178253, average loss: 0.010056048226356506\n",
            "Epoch 516: accuracy: 0.8613333333333333, total loss: 167.74234694242477, average loss: 0.0037276077098316617\n",
            "Validation: accuracy: 0.689, total loss: 47.81478440761566, average loss: 0.009562956881523131\n",
            "Epoch 517: accuracy: 0.8607777777777778, total loss: 168.50309033691883, average loss: 0.003744513118598196\n",
            "Validation: accuracy: 0.6846, total loss: 50.372681975364685, average loss: 0.010074536395072936\n",
            "Epoch 518: accuracy: 0.8560888888888889, total loss: 171.94374778866768, average loss: 0.003820972173081504\n",
            "Validation: accuracy: 0.625, total loss: 61.13843262195587, average loss: 0.012227686524391174\n",
            "Epoch 519: accuracy: 0.856, total loss: 171.92209649085999, average loss: 0.0038204910331302217\n",
            "Validation: accuracy: 0.6614, total loss: 52.96600866317749, average loss: 0.010593201732635497\n",
            "Epoch 520: accuracy: 0.8590222222222222, total loss: 170.2279949337244, average loss: 0.003782844331860542\n",
            "Validation: accuracy: 0.6678, total loss: 54.459276139736176, average loss: 0.010891855227947235\n",
            "Epoch 521: accuracy: 0.8570666666666666, total loss: 171.73315557837486, average loss: 0.003816292346186108\n",
            "Validation: accuracy: 0.624, total loss: 61.61277151107788, average loss: 0.012322554302215577\n",
            "Epoch 522: accuracy: 0.8614, total loss: 168.06782627105713, average loss: 0.0037348405838012696\n",
            "Validation: accuracy: 0.6812, total loss: 50.64373689889908, average loss: 0.010128747379779816\n",
            "Epoch 523: accuracy: 0.8600666666666666, total loss: 169.5023930966854, average loss: 0.003766719846593009\n",
            "Validation: accuracy: 0.6712, total loss: 52.67653852701187, average loss: 0.010535307705402374\n",
            "Epoch 524: accuracy: 0.8594, total loss: 169.23822185397148, average loss: 0.0037608493745326997\n",
            "Validation: accuracy: 0.5716, total loss: 77.58798587322235, average loss: 0.01551759717464447\n",
            "Epoch 525: accuracy: 0.8582444444444445, total loss: 171.5975430905819, average loss: 0.003813278735346264\n",
            "Validation: accuracy: 0.6792, total loss: 51.04863399267197, average loss: 0.010209726798534394\n",
            "Epoch 526: accuracy: 0.8577111111111111, total loss: 169.63440641760826, average loss: 0.00376965347594685\n",
            "Validation: accuracy: 0.6554, total loss: 56.24584949016571, average loss: 0.011249169898033142\n",
            "Epoch 527: accuracy: 0.8600444444444444, total loss: 168.63062842190266, average loss: 0.0037473472982645036\n",
            "Validation: accuracy: 0.668, total loss: 52.40748488903046, average loss: 0.010481496977806091\n",
            "Epoch 528: accuracy: 0.8556666666666667, total loss: 172.79152804613113, average loss: 0.0038398117343584697\n",
            "Validation: accuracy: 0.6556, total loss: 55.02317827939987, average loss: 0.011004635655879974\n",
            "Epoch 529: accuracy: 0.8556222222222222, total loss: 173.78865967690945, average loss: 0.003861970215042432\n",
            "Validation: accuracy: 0.6614, total loss: 54.31971484422684, average loss: 0.010863942968845368\n",
            "Epoch 530: accuracy: 0.8585555555555555, total loss: 169.34462064504623, average loss: 0.0037632137921121385\n",
            "Validation: accuracy: 0.6832, total loss: 50.389371156692505, average loss: 0.010077874231338501\n",
            "Epoch 531: accuracy: 0.8593111111111111, total loss: 169.12326909601688, average loss: 0.003758294868800375\n",
            "Validation: accuracy: 0.6632, total loss: 54.11406707763672, average loss: 0.010822813415527344\n",
            "Epoch 532: accuracy: 0.8623333333333333, total loss: 165.97191202640533, average loss: 0.0036882647116978965\n",
            "Validation: accuracy: 0.6634, total loss: 53.593482196331024, average loss: 0.010718696439266205\n",
            "Epoch 533: accuracy: 0.8607111111111111, total loss: 167.92511519789696, average loss: 0.0037316692266199324\n",
            "Validation: accuracy: 0.6444, total loss: 59.37592375278473, average loss: 0.011875184750556945\n",
            "Epoch 534: accuracy: 0.8583333333333333, total loss: 169.21356078982353, average loss: 0.0037603013508849675\n",
            "Validation: accuracy: 0.6584, total loss: 54.2045978307724, average loss: 0.01084091956615448\n",
            "Epoch 535: accuracy: 0.8562, total loss: 172.54864284396172, average loss: 0.0038344142854213714\n",
            "Validation: accuracy: 0.6664, total loss: 53.899937748909, average loss: 0.0107799875497818\n",
            "Epoch 536: accuracy: 0.8632222222222222, total loss: 164.58984252810478, average loss: 0.0036575520561801063\n",
            "Validation: accuracy: 0.6422, total loss: 58.778958678245544, average loss: 0.01175579173564911\n",
            "Epoch 537: accuracy: 0.8589333333333333, total loss: 170.18312260508537, average loss: 0.003781847169001897\n",
            "Validation: accuracy: 0.6768, total loss: 50.31181353330612, average loss: 0.010062362706661224\n",
            "Epoch 538: accuracy: 0.8598666666666667, total loss: 167.83499601483345, average loss: 0.00372966657810741\n",
            "Validation: accuracy: 0.6536, total loss: 54.32399308681488, average loss: 0.010864798617362977\n",
            "Epoch 539: accuracy: 0.8628666666666667, total loss: 165.04957497119904, average loss: 0.0036677683326933118\n",
            "Validation: accuracy: 0.6506, total loss: 55.99452877044678, average loss: 0.011198905754089356\n",
            "Epoch 540: accuracy: 0.8576, total loss: 169.85683631896973, average loss: 0.0037745963626437716\n",
            "Validation: accuracy: 0.6774, total loss: 50.84638625383377, average loss: 0.010169277250766754\n",
            "Checkpoint saved as `/weights/540.pth`\n",
            "Epoch 541: accuracy: 0.8591111111111112, total loss: 170.9933890402317, average loss: 0.003799853089782927\n",
            "Validation: accuracy: 0.6616, total loss: 52.72224950790405, average loss: 0.010544449901580811\n",
            "Epoch 542: accuracy: 0.8566, total loss: 170.05844341218472, average loss: 0.0037790765202707715\n",
            "Validation: accuracy: 0.671, total loss: 50.71389091014862, average loss: 0.010142778182029724\n",
            "Epoch 543: accuracy: 0.8594888888888889, total loss: 169.21299022436142, average loss: 0.003760288671652476\n",
            "Validation: accuracy: 0.6616, total loss: 53.170437693595886, average loss: 0.010634087538719177\n",
            "Epoch 544: accuracy: 0.8602, total loss: 168.17855769395828, average loss: 0.0037373012820879617\n",
            "Validation: accuracy: 0.669, total loss: 52.64468163251877, average loss: 0.010528936326503754\n",
            "Epoch 545: accuracy: 0.8543111111111111, total loss: 174.5587618947029, average loss: 0.003879083597660065\n",
            "Validation: accuracy: 0.646, total loss: 55.51805257797241, average loss: 0.011103610515594482\n",
            "Epoch 546: accuracy: 0.8584888888888889, total loss: 169.77729678153992, average loss: 0.0037728288173675536\n",
            "Validation: accuracy: 0.643, total loss: 58.212879836559296, average loss: 0.01164257596731186\n",
            "Epoch 547: accuracy: 0.861, total loss: 166.63471072912216, average loss: 0.0037029935717582704\n",
            "Validation: accuracy: 0.6578, total loss: 53.56608581542969, average loss: 0.010713217163085938\n",
            "Epoch 548: accuracy: 0.8613555555555555, total loss: 167.01955604553223, average loss: 0.003711545689900716\n",
            "Validation: accuracy: 0.6258, total loss: 63.644129276275635, average loss: 0.012728825855255126\n",
            "Epoch 549: accuracy: 0.8573777777777778, total loss: 168.97755979001522, average loss: 0.0037550568842225603\n",
            "Validation: accuracy: 0.651, total loss: 58.69263136386871, average loss: 0.011738526272773742\n",
            "Epoch 550: accuracy: 0.8574888888888889, total loss: 171.94100341200829, average loss: 0.0038209111869335175\n",
            "Validation: accuracy: 0.6388, total loss: 58.068716526031494, average loss: 0.011613743305206299\n",
            "Epoch 551: accuracy: 0.8553777777777778, total loss: 172.44579783082008, average loss: 0.003832128840684891\n",
            "Validation: accuracy: 0.6454, total loss: 57.29381024837494, average loss: 0.011458762049674987\n",
            "Epoch 552: accuracy: 0.8600888888888889, total loss: 170.34775558114052, average loss: 0.0037855056795809004\n",
            "Validation: accuracy: 0.6332, total loss: 58.0850750207901, average loss: 0.01161701500415802\n",
            "Epoch 553: accuracy: 0.8614444444444445, total loss: 167.64026993513107, average loss: 0.0037253393318918016\n",
            "Validation: accuracy: 0.6442, total loss: 55.45251554250717, average loss: 0.011090503108501435\n",
            "Epoch 554: accuracy: 0.8602, total loss: 167.70737862586975, average loss: 0.003726830636130439\n",
            "Validation: accuracy: 0.6752, total loss: 52.06656616926193, average loss: 0.010413313233852386\n",
            "Epoch 555: accuracy: 0.8602222222222222, total loss: 167.55085653066635, average loss: 0.003723352367348141\n",
            "Validation: accuracy: 0.6434, total loss: 58.36159682273865, average loss: 0.01167231936454773\n",
            "Epoch 556: accuracy: 0.8598, total loss: 168.041271597147, average loss: 0.0037342504799365997\n",
            "Validation: accuracy: 0.6696, total loss: 50.372859597206116, average loss: 0.010074571919441224\n",
            "Epoch 557: accuracy: 0.8628666666666667, total loss: 164.57523009181023, average loss: 0.0036572273353735604\n",
            "Validation: accuracy: 0.6472, total loss: 57.882755398750305, average loss: 0.011576551079750062\n",
            "Epoch 558: accuracy: 0.8580444444444445, total loss: 170.86587089300156, average loss: 0.0037970193531778125\n",
            "Validation: accuracy: 0.6576, total loss: 55.340951800346375, average loss: 0.011068190360069275\n",
            "Epoch 559: accuracy: 0.8600888888888889, total loss: 167.95311918854713, average loss: 0.00373229153752327\n",
            "Validation: accuracy: 0.6636, total loss: 51.808370649814606, average loss: 0.010361674129962922\n",
            "Epoch 560: accuracy: 0.8584, total loss: 171.38470840454102, average loss: 0.003808549075656467\n",
            "Validation: accuracy: 0.6358, total loss: 57.105621099472046, average loss: 0.01142112421989441\n",
            "Epoch 561: accuracy: 0.8623111111111111, total loss: 167.4953289628029, average loss: 0.0037221184213956197\n",
            "Validation: accuracy: 0.6538, total loss: 54.72094523906708, average loss: 0.010944189047813415\n",
            "Epoch 562: accuracy: 0.8604666666666667, total loss: 167.74058492481709, average loss: 0.003727568553884824\n",
            "Validation: accuracy: 0.6418, total loss: 57.743435859680176, average loss: 0.011548687171936035\n",
            "Epoch 563: accuracy: 0.8628666666666667, total loss: 165.04388228058815, average loss: 0.0036676418284575145\n",
            "Validation: accuracy: 0.6348, total loss: 59.86016058921814, average loss: 0.011972032117843628\n",
            "Epoch 564: accuracy: 0.8597777777777778, total loss: 168.33931431174278, average loss: 0.003740873651372062\n",
            "Validation: accuracy: 0.646, total loss: 55.80799615383148, average loss: 0.011161599230766296\n",
            "Epoch 565: accuracy: 0.8590222222222222, total loss: 168.86315608024597, average loss: 0.0037525145795610218\n",
            "Validation: accuracy: 0.6528, total loss: 55.59391963481903, average loss: 0.011118783926963806\n",
            "Epoch 566: accuracy: 0.8578222222222223, total loss: 171.04174089431763, average loss: 0.0038009275754292806\n",
            "Validation: accuracy: 0.6774, total loss: 50.25640320777893, average loss: 0.010051280641555787\n",
            "Epoch 567: accuracy: 0.8600666666666666, total loss: 167.75184080004692, average loss: 0.003727818684445487\n",
            "Validation: accuracy: 0.6776, total loss: 51.27117121219635, average loss: 0.01025423424243927\n",
            "Epoch 568: accuracy: 0.8617777777777778, total loss: 165.67174777388573, average loss: 0.0036815943949752386\n",
            "Validation: accuracy: 0.6616, total loss: 53.35355821251869, average loss: 0.010670711642503739\n",
            "Epoch 569: accuracy: 0.8600444444444444, total loss: 168.11504313349724, average loss: 0.00373588984741105\n",
            "Validation: accuracy: 0.6694, total loss: 53.38896590471268, average loss: 0.010677793180942535\n",
            "Epoch 570: accuracy: 0.8602666666666666, total loss: 168.5260769724846, average loss: 0.00374502393272188\n",
            "Validation: accuracy: 0.654, total loss: 55.347576916217804, average loss: 0.01106951538324356\n",
            "Checkpoint saved as `/weights/570.pth`\n",
            "Epoch 571: accuracy: 0.8597111111111111, total loss: 168.7034456282854, average loss: 0.0037489654584063423\n",
            "Validation: accuracy: 0.6702, total loss: 51.86366426944733, average loss: 0.010372732853889465\n",
            "Epoch 572: accuracy: 0.8602888888888889, total loss: 168.0600673854351, average loss: 0.00373466816412078\n",
            "Validation: accuracy: 0.6274, total loss: 60.16918194293976, average loss: 0.012033836388587951\n",
            "Epoch 573: accuracy: 0.8611333333333333, total loss: 166.0712187588215, average loss: 0.003690471527973811\n",
            "Validation: accuracy: 0.6538, total loss: 56.316049218177795, average loss: 0.011263209843635559\n",
            "Epoch 574: accuracy: 0.8580888888888889, total loss: 169.29302947223186, average loss: 0.0037620673216051527\n",
            "Validation: accuracy: 0.6498, total loss: 56.79044735431671, average loss: 0.011358089470863343\n",
            "Epoch 575: accuracy: 0.8611555555555556, total loss: 165.75140514969826, average loss: 0.0036833645588821833\n",
            "Validation: accuracy: 0.6552, total loss: 55.55439168214798, average loss: 0.011110878336429595\n",
            "Epoch 576: accuracy: 0.8577111111111111, total loss: 171.0113926678896, average loss: 0.0038002531703975467\n",
            "Validation: accuracy: 0.6586, total loss: 52.062363266944885, average loss: 0.010412472653388977\n",
            "Epoch 577: accuracy: 0.8597111111111111, total loss: 168.79905880987644, average loss: 0.003751090195775032\n",
            "Validation: accuracy: 0.6626, total loss: 53.58438217639923, average loss: 0.010716876435279845\n",
            "Epoch 578: accuracy: 0.8619333333333333, total loss: 165.03229501843452, average loss: 0.0036673843337429895\n",
            "Validation: accuracy: 0.6622, total loss: 53.428592920303345, average loss: 0.010685718584060668\n",
            "Epoch 579: accuracy: 0.8586, total loss: 169.90671038627625, average loss: 0.0037757046752505833\n",
            "Validation: accuracy: 0.6798, total loss: 51.27354574203491, average loss: 0.010254709148406982\n",
            "Epoch 580: accuracy: 0.8588444444444444, total loss: 171.50463807582855, average loss: 0.0038112141794628567\n",
            "Validation: accuracy: 0.6782, total loss: 49.35175210237503, average loss: 0.009870350420475005\n",
            "Epoch 581: accuracy: 0.8578888888888889, total loss: 171.05018450319767, average loss: 0.0038011152111821706\n",
            "Validation: accuracy: 0.6526, total loss: 54.00865280628204, average loss: 0.010801730561256408\n",
            "Epoch 582: accuracy: 0.8569777777777777, total loss: 170.60201585292816, average loss: 0.0037911559078428483\n",
            "Validation: accuracy: 0.661, total loss: 53.225019097328186, average loss: 0.010645003819465638\n",
            "Epoch 583: accuracy: 0.8613777777777778, total loss: 165.45593410730362, average loss: 0.0036767985357178583\n",
            "Validation: accuracy: 0.6664, total loss: 51.68716216087341, average loss: 0.010337432432174682\n",
            "Epoch 584: accuracy: 0.8599333333333333, total loss: 167.44060848653316, average loss: 0.003720902410811848\n",
            "Validation: accuracy: 0.6448, total loss: 60.509488344192505, average loss: 0.0121018976688385\n",
            "Epoch 585: accuracy: 0.8625555555555555, total loss: 165.58852729201317, average loss: 0.003679745050933626\n",
            "Validation: accuracy: 0.671, total loss: 51.59271687269211, average loss: 0.010318543374538422\n",
            "Epoch 586: accuracy: 0.8589333333333333, total loss: 170.68476757407188, average loss: 0.003792994834979375\n",
            "Validation: accuracy: 0.6202, total loss: 62.92380380630493, average loss: 0.012584760761260987\n",
            "Epoch 587: accuracy: 0.8602, total loss: 169.20200034976006, average loss: 0.00376004445221689\n",
            "Validation: accuracy: 0.6434, total loss: 57.05794298648834, average loss: 0.011411588597297668\n",
            "Epoch 588: accuracy: 0.8588222222222223, total loss: 168.75133614242077, average loss: 0.0037500296920537947\n",
            "Validation: accuracy: 0.6512, total loss: 54.68818378448486, average loss: 0.010937636756896973\n",
            "Epoch 589: accuracy: 0.8594, total loss: 168.41878524422646, average loss: 0.003742639672093921\n",
            "Validation: accuracy: 0.6712, total loss: 51.38235950469971, average loss: 0.010276471900939941\n",
            "Epoch 590: accuracy: 0.8605111111111111, total loss: 168.34133830666542, average loss: 0.0037409186290370093\n",
            "Validation: accuracy: 0.6742, total loss: 51.21413105726242, average loss: 0.010242826211452484\n",
            "Epoch 591: accuracy: 0.8624, total loss: 166.33004461228848, average loss: 0.0036962232136064106\n",
            "Validation: accuracy: 0.639, total loss: 58.58220994472504, average loss: 0.011716441988945007\n",
            "Epoch 592: accuracy: 0.8620666666666666, total loss: 166.25300931930542, average loss: 0.003694511318206787\n",
            "Validation: accuracy: 0.6738, total loss: 51.95658242702484, average loss: 0.010391316485404969\n",
            "Epoch 593: accuracy: 0.8622888888888889, total loss: 167.0827986896038, average loss: 0.003712951081991196\n",
            "Validation: accuracy: 0.6402, total loss: 57.7506308555603, average loss: 0.011550126171112061\n",
            "Epoch 594: accuracy: 0.8594, total loss: 169.25809460878372, average loss: 0.003761290991306305\n",
            "Validation: accuracy: 0.6694, total loss: 53.4672486782074, average loss: 0.010693449735641479\n",
            "Epoch 595: accuracy: 0.8609555555555556, total loss: 165.33828973770142, average loss: 0.0036741842163933646\n",
            "Validation: accuracy: 0.657, total loss: 53.67864316701889, average loss: 0.010735728633403777\n",
            "Epoch 596: accuracy: 0.8634666666666667, total loss: 165.1772522032261, average loss: 0.0036706056045161355\n",
            "Validation: accuracy: 0.6666, total loss: 54.17906999588013, average loss: 0.010835813999176025\n",
            "Epoch 597: accuracy: 0.8603555555555555, total loss: 170.02706791460514, average loss: 0.003778379286991225\n",
            "Validation: accuracy: 0.6608, total loss: 54.56309390068054, average loss: 0.01091261878013611\n",
            "Epoch 598: accuracy: 0.8611555555555556, total loss: 167.9709987640381, average loss: 0.003732688861423069\n",
            "Validation: accuracy: 0.6794, total loss: 49.11006236076355, average loss: 0.00982201247215271\n",
            "Epoch 599: accuracy: 0.8632666666666666, total loss: 165.35624332726002, average loss: 0.0036745831850502225\n",
            "Validation: accuracy: 0.6474, total loss: 58.03024089336395, average loss: 0.01160604817867279\n",
            "Epoch 600: accuracy: 0.8592888888888889, total loss: 169.02046358585358, average loss: 0.0037560103019078574\n",
            "Validation: accuracy: 0.6636, total loss: 52.75699484348297, average loss: 0.010551398968696593\n",
            "Checkpoint saved as `/weights/600.pth`\n",
            "Epoch 601: accuracy: 0.8602222222222222, total loss: 168.09414511919022, average loss: 0.003735425447093116\n",
            "Validation: accuracy: 0.6548, total loss: 55.47126269340515, average loss: 0.011094252538681031\n",
            "Epoch 602: accuracy: 0.8600222222222222, total loss: 167.79378047585487, average loss: 0.0037287506772412196\n",
            "Validation: accuracy: 0.6144, total loss: 63.28823268413544, average loss: 0.012657646536827088\n",
            "Epoch 603: accuracy: 0.8598666666666667, total loss: 168.66003274917603, average loss: 0.0037480007277594673\n",
            "Validation: accuracy: 0.6578, total loss: 55.18016600608826, average loss: 0.01103603320121765\n",
            "Epoch 604: accuracy: 0.8615333333333334, total loss: 165.63913595676422, average loss: 0.0036808696879280936\n",
            "Validation: accuracy: 0.6776, total loss: 49.359238505363464, average loss: 0.009871847701072693\n",
            "Epoch 605: accuracy: 0.8657111111111111, total loss: 163.70017510652542, average loss: 0.0036377816690338982\n",
            "Validation: accuracy: 0.6714, total loss: 50.81340789794922, average loss: 0.010162681579589843\n",
            "Epoch 606: accuracy: 0.8593333333333333, total loss: 167.44052819907665, average loss: 0.003720900626646148\n",
            "Validation: accuracy: 0.6674, total loss: 51.39902287721634, average loss: 0.010279804575443268\n",
            "Epoch 607: accuracy: 0.8577555555555556, total loss: 170.5148719549179, average loss: 0.0037892193767759537\n",
            "Validation: accuracy: 0.678, total loss: 49.245914041996, average loss: 0.0098491828083992\n",
            "Epoch 608: accuracy: 0.8618, total loss: 166.82712949812412, average loss: 0.003707269544402758\n",
            "Validation: accuracy: 0.667, total loss: 53.106881499290466, average loss: 0.010621376299858093\n",
            "Epoch 609: accuracy: 0.8609333333333333, total loss: 167.5176422894001, average loss: 0.00372261427309778\n",
            "Validation: accuracy: 0.6582, total loss: 53.570835292339325, average loss: 0.010714167058467864\n",
            "Epoch 610: accuracy: 0.8618666666666667, total loss: 166.19620747864246, average loss: 0.0036932490550809437\n",
            "Validation: accuracy: 0.682, total loss: 50.42904752492905, average loss: 0.01008580950498581\n",
            "Epoch 611: accuracy: 0.8627333333333334, total loss: 165.6849828660488, average loss: 0.003681888508134418\n",
            "Validation: accuracy: 0.6784, total loss: 51.032806754112244, average loss: 0.010206561350822449\n",
            "Epoch 612: accuracy: 0.8626444444444444, total loss: 166.38469389081, average loss: 0.0036974376420180004\n",
            "Validation: accuracy: 0.6588, total loss: 52.84905219078064, average loss: 0.010569810438156128\n",
            "Epoch 613: accuracy: 0.8556444444444444, total loss: 171.55931210517883, average loss: 0.003812429157892863\n",
            "Validation: accuracy: 0.6412, total loss: 58.41094100475311, average loss: 0.011682188200950622\n",
            "Epoch 614: accuracy: 0.8608888888888889, total loss: 168.43803749978542, average loss: 0.0037430674999952315\n",
            "Validation: accuracy: 0.6186, total loss: 64.25238049030304, average loss: 0.012850476098060608\n",
            "Epoch 615: accuracy: 0.8579555555555556, total loss: 172.0823214650154, average loss: 0.0038240515881114534\n",
            "Validation: accuracy: 0.6142, total loss: 62.10699701309204, average loss: 0.012421399402618409\n",
            "Epoch 616: accuracy: 0.8608222222222223, total loss: 166.40689179301262, average loss: 0.0036979309287336138\n",
            "Validation: accuracy: 0.6174, total loss: 62.64088332653046, average loss: 0.012528176665306092\n",
            "Epoch 617: accuracy: 0.8641111111111112, total loss: 164.34186029434204, average loss: 0.0036520413398742676\n",
            "Validation: accuracy: 0.6712, total loss: 51.01795941591263, average loss: 0.010203591883182525\n",
            "Epoch 618: accuracy: 0.8640444444444444, total loss: 165.31307756900787, average loss: 0.0036736239459779526\n",
            "Validation: accuracy: 0.6642, total loss: 52.55406874418259, average loss: 0.010510813748836518\n",
            "Epoch 619: accuracy: 0.8595555555555555, total loss: 168.0492137670517, average loss: 0.003734426972601149\n",
            "Validation: accuracy: 0.6736, total loss: 52.14341336488724, average loss: 0.010428682672977448\n",
            "Epoch 620: accuracy: 0.8663333333333333, total loss: 162.63598257303238, average loss: 0.0036141329460673863\n",
            "Validation: accuracy: 0.6732, total loss: 51.35895490646362, average loss: 0.010271790981292725\n",
            "Epoch 621: accuracy: 0.8592, total loss: 169.20147502422333, average loss: 0.003760032778316074\n",
            "Validation: accuracy: 0.6662, total loss: 52.49877625703812, average loss: 0.010499755251407623\n",
            "Epoch 622: accuracy: 0.8601555555555556, total loss: 168.90394115447998, average loss: 0.0037534209145439995\n",
            "Validation: accuracy: 0.6272, total loss: 59.07862627506256, average loss: 0.011815725255012512\n",
            "Epoch 623: accuracy: 0.8603777777777778, total loss: 168.4516627341509, average loss: 0.003743370282981131\n",
            "Validation: accuracy: 0.6524, total loss: 56.239653289318085, average loss: 0.011247930657863617\n",
            "Epoch 624: accuracy: 0.8660444444444444, total loss: 162.30095264315605, average loss: 0.003606687836514579\n",
            "Validation: accuracy: 0.6632, total loss: 52.21903860569, average loss: 0.010443807721138001\n",
            "Epoch 625: accuracy: 0.8621333333333333, total loss: 166.58031085133553, average loss: 0.003701784685585234\n",
            "Validation: accuracy: 0.6584, total loss: 55.88571685552597, average loss: 0.011177143371105194\n",
            "Epoch 626: accuracy: 0.8614222222222222, total loss: 165.18023923039436, average loss: 0.0036706719828976525\n",
            "Validation: accuracy: 0.677, total loss: 50.363284170627594, average loss: 0.010072656834125518\n",
            "Epoch 627: accuracy: 0.863, total loss: 165.0035549402237, average loss: 0.0036667456653383043\n",
            "Validation: accuracy: 0.6614, total loss: 54.902136623859406, average loss: 0.01098042732477188\n",
            "Epoch 628: accuracy: 0.8627333333333334, total loss: 165.1147265136242, average loss: 0.003669216144747204\n",
            "Validation: accuracy: 0.653, total loss: 56.75432080030441, average loss: 0.011350864160060883\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-2bacd983182d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7c2ee978-57b6-45ea-8f9b-240365d602fa",
        "id": "bsu0HnKNoAJb"
      },
      "source": [
        "\n",
        "checkpoint_save_dir='/weights/'\n",
        "\n",
        "CHECKPOINT_SAVE_EPOCH_INTERVAL=40\n",
        "if (not os.path.exists(checkpoint_save_dir)):\n",
        "        os.makedirs(checkpoint_save_dir)\n",
        "        print(f\"Created checkpoint save dir `{checkpoint_save_dir}`.\")\n",
        "elif (os.path.isdir(checkpoint_save_dir)):\n",
        "        print(f\"Using existing checkpoint save dir `{checkpoint_save_dir}`.\")\n",
        "        existing_filenames = os.listdir(checkpoint_save_dir)\n",
        "        if existing_filenames:\n",
        "            print(f\"Existing checkpoint files in this directory will be overwritten.\")\n",
        "else:\n",
        "        raise FileExistsError(f\"Checkpoint save dir `{checkpoint_save_dir}` is not a folder.\")   \n",
        "########################################################################\n",
        "# 4. Train the network\n",
        "# ^^^^^^^^^^^^^^^^^^^^\n",
        "#\n",
        "# We simply have to loop over our data iterator, and feed the inputs to the\n",
        "# network and optimize. We evaluate the validation accuracy at each\n",
        "# epoch and plot these values over the number of epochs\n",
        "# Nothing to change here\n",
        "# -----------------------------\n",
        "train_loss_visual=[]\n",
        "valid_loss_visual=[]\n",
        "\n",
        "\n",
        "VAL_EPOCH_INTERVAL=1\n",
        "\n",
        "# for epoch in range(EPOCHS):  # loop over the dataset multiple times\n",
        "\n",
        "#     running_loss = 0.0\n",
        "#     for i, data in enumerate(trainloader, 0):\n",
        "#         # get the inputs\n",
        "#         inputs, labels = data\n",
        "\n",
        "#         if IS_GPU:\n",
        "#             inputs = inputs.cuda()\n",
        "#             labels = labels.cuda()\n",
        "\n",
        "#         # wrap them in Variable\n",
        "#         inputs, labels = Variable(inputs), Variable(labels)\n",
        "\n",
        "#         # zero the parameter gradients\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         # forward + backward + optimize\n",
        "#         outputs = net(inputs)\n",
        "#         loss = criterion(outputs, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         # print statistics\n",
        "#         running_loss += loss.item()\n",
        "    \n",
        "#     # Normalizing the loss by the total number of train batches\n",
        "#     running_loss/=len(trainloader)\n",
        "#     print('[%d] loss: %.3f' %\n",
        "#           (epoch + 1, running_loss))\n",
        "\n",
        "#     # Scale of 0.0 to 100.0\n",
        "#     # Calculate validation set accuracy of the existing model\n",
        "#     val_accuracy, val_classwise_accuracy = \n",
        "#         calculate_val_accuracy(valloader, IS_GPU)\n",
        "#     print('Accuracy of the network on the val images: %d %%' % (val_accuracy))\n",
        "\n",
        "#     # # Optionally print classwise accuracies\n",
        "#     # for c_i in range(TOTAL_CLASSES):\n",
        "#     #     print('Accuracy of %5s : %2d %%' % (\n",
        "#     #         classes[c_i], 100 * val_classwise_accuracy[c_i]))\n",
        "\n",
        "#     train_loss_over_epochs.append(running_loss)\n",
        "#     val_accuracy_over_epochs.append(val_accuracy)\n",
        "# # -----------------------------\n",
        "#     # MARK: Save checkpoint\n",
        "#     if ((epoch % CHECKPOINT_SAVE_EPOCH_INTERVAL) == 0):\n",
        "#         checkpoint_filename = os.path.join(checkpoint_save_dir, f\"{epoch}.pth\")\n",
        "#         torch.save(net.state_dict(), checkpoint_filename)\n",
        "#         print(f\"Checkpoint saved as `{checkpoint_filename}`\")\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "trainloss=[]\n",
        "validloss=[]\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):  # loop over the dataset multiple times\n",
        "        net.train()\n",
        "        # running_loss = 0.0\n",
        "        running_loss = 0.0\n",
        "        train_loss=0.\n",
        "        train_count=0\n",
        "        train_correct_count = 0\n",
        "        for image, label in trainloader:\n",
        "\n",
        "            if IS_GPU:\n",
        "                image = image.cuda()\n",
        "                label = label.cuda()\n",
        "            image, label = Variable(image), Variable(label)\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            # forward + backward + optimize\n",
        "            net=net.cuda()\n",
        "\n",
        "            outputs = net(image)\n",
        "       \n",
        "            # loss = F.nll_loss(outputs, label)\n",
        "            loss = loss_function(outputs, label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "        #     running_loss += loss.item()\n",
        "        # running_loss/=len(trainloader)\n",
        "        # print('[%d] loss: %.3f' %\n",
        "        # (epoch + 1, running_loss))\n",
        "        # train_loss_visual.append(running_loss)\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            train_count += image.shape[0]\n",
        "\n",
        "            _, max_prediction_indices = torch.max(outputs, -1)\n",
        "            train_correct_count += torch.sum(max_prediction_indices == label).item()\n",
        "\n",
        "        print(f\"Epoch {epoch}: accuracy: {train_correct_count / train_count}, total loss: {train_loss}, average loss: {train_loss / train_count}\")\n",
        "        train_loss_visual.append(train_loss)\n",
        "        trainloss.append(train_loss)\n",
        "        \n",
        "        # Normalizing the loss by the total number of train batches\n",
        "        # running_loss/=len(trainloader)\n",
        "        # print('[%d] loss: %.3f' %\n",
        "        #     (epoch + 1, running_loss))\n",
        "\n",
        "        # Scale of 0.0 to 100.0\n",
        "        # Calculate validation set accuracy of the existing model\n",
        "        # val_accuracy, val_classwise_accuracy = \\\n",
        "        #     calculate_val_accuracy(valloader, IS_GPU)\n",
        "        # print('Accuracy of the network on the val images: %d %%' % (val_accuracy))\n",
        "\n",
        "        # # Optionally print classwise accuracies\n",
        "        # for c_i in range(TOTAL_CLASSES):\n",
        "        #     print('Accuracy of %5s : %2d %%' % (\n",
        "        #         classes[c_i], 100 * val_classwise_accuracy[c_i]))\n",
        "        if (epoch % VAL_EPOCH_INTERVAL == 0):\n",
        "           \n",
        "            net.eval()\n",
        "            validation_loss = 0.\n",
        "            validation_count = 0\n",
        "            validation_correct_count = 0\n",
        "            with torch.no_grad():    # [Very important] Reduce memory usage.\n",
        "                for image, label in valloader :\n",
        "                    image = image.cuda()\n",
        "                    label = label.cuda()\n",
        "\n",
        "                    prediction = net(image)\n",
        "                    # loss = F.nll_loss(prediction, label)\n",
        "                    \n",
        "                    loss = loss_function(prediction, label)\n",
        "                    validation_loss += loss.item()\n",
        "                    validation_count += image.shape[0]\n",
        "\n",
        "                    _, max_prediction_indices = torch.max(prediction, -1)\n",
        "                    validation_correct_count += torch.sum(max_prediction_indices == label).item()\n",
        "\n",
        "       \n",
        "            print(f\"Validation: accuracy: {validation_correct_count / validation_count}, total loss: {validation_loss}, average loss: {validation_loss / validation_count}\")\n",
        "            valid_loss_visual.append(validation_loss)\n",
        "            validloss.append(validation_loss)\n",
        "              # Calculate validation set accuracy of the existing model\n",
        "            # temp=calculate_val_accuracy(valloader, IS_GPU)\n",
        "            # val_accuracy, val_classwise_accuracy = temp\n",
        "            # print('Accuracy of the network on the val images: %d %%' % (val_accuracy))\n",
        "\n",
        "    # # # Optionally print classwise accuracies\n",
        "    #     train_loss_over_epochs.append(running_loss)\n",
        "    #     val_accuracy_over_epochs.append(val_accuracy)\n",
        "\n",
        "\n",
        "    # -----------------------------\n",
        "        # MARK: Save checkpoint\n",
        "        if ((epoch % CHECKPOINT_SAVE_EPOCH_INTERVAL) == 0):\n",
        "        \n",
        "            checkpoint_filename = os.path.join(checkpoint_save_dir, f\"{epoch}.pth\")\n",
        "            torch.save(net.state_dict(), checkpoint_filename)\n",
        "            print(f\"Checkpoint saved as `{checkpoint_filename}`\")\n",
        "\n",
        "        adjust_opt('sgd',optimizer, epoch)\n",
        "\n",
        "trainlossnp=np.array(trainloss)\n",
        "validlossnp=np.array(validloss)\n",
        "plt.plot(trainlossnp)\n",
        "plt.plot(validlossnp)\n",
        "plt.show()\n",
        "plt.savefig('different_loss.png')\n",
        "\n",
        "\n",
        "# Plot train loss over epochs and val set accuracy over epochs\n",
        "# Nothing to change here\n",
        "# -------------\n",
        "# plt.subplot(2, 1, 1)\n",
        "# plt.ylabel('Train loss')\n",
        "# plt.plot(np.arange(EPOCHS), train_loss_over_epochs, 'k-')\n",
        "# plt.title('train loss and val accuracy')\n",
        "# plt.xticks(np.arange(EPOCHS, dtype=int))\n",
        "# plt.grid(True)\n",
        "\n",
        "# plt.subplot(2, 1, 2)\n",
        "# val_accuracy_over_epochs = torch.tensor(val_accuracy_over_epochs, device = 'cpu')\n",
        "# plt.plot(np.arange(EPOCHS), val_accuracy_over_epochs, 'b-')\n",
        "# plt.ylabel('Val accuracy')\n",
        "# plt.xlabel('Epochs')\n",
        "# plt.xticks(np.arange(EPOCHS, dtype=int))\n",
        "# plt.grid(True)\n",
        "# plt.savefig(\"plot.png\")\n",
        "# plt.close(fig)\n",
        "# print('Finished Training')\n",
        "# -------------\n",
        "\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using existing checkpoint save dir `/weights/`.\n",
            "Existing checkpoint files in this directory will be overwritten.\n",
            "Epoch 0: accuracy: 0.08273333333333334, total loss: 1407.1718492507935, average loss: 0.031270485538906524\n",
            "Validation: accuracy: 0.1334, total loss: 145.76429343223572, average loss: 0.029152858686447142\n",
            "Checkpoint saved as `/weights/0.pth`\n",
            "Epoch 1: accuracy: 0.16173333333333334, total loss: 1222.250857591629, average loss: 0.027161130168702866\n",
            "Validation: accuracy: 0.189, total loss: 131.47013092041016, average loss: 0.026294026184082032\n",
            "Epoch 2: accuracy: 0.22791111111111112, total loss: 1088.2772047519684, average loss: 0.024183937883377075\n",
            "Validation: accuracy: 0.2768, total loss: 114.87057399749756, average loss: 0.022974114799499512\n",
            "Epoch 3: accuracy: 0.2844888888888889, total loss: 976.1606216430664, average loss: 0.02169245825873481\n",
            "Validation: accuracy: 0.3222, total loss: 104.00015068054199, average loss: 0.0208000301361084\n",
            "Epoch 4: accuracy: 0.33795555555555556, total loss: 888.0152344703674, average loss: 0.019733671877119278\n",
            "Validation: accuracy: 0.361, total loss: 94.09779691696167, average loss: 0.018819559383392335\n",
            "Epoch 5: accuracy: 0.37653333333333333, total loss: 821.8335124254227, average loss: 0.01826296694278717\n",
            "Validation: accuracy: 0.36, total loss: 97.29567229747772, average loss: 0.019459134459495545\n",
            "Epoch 6: accuracy: 0.4045111111111111, total loss: 776.5384261608124, average loss: 0.017256409470240274\n",
            "Validation: accuracy: 0.3938, total loss: 90.10377204418182, average loss: 0.018020754408836364\n",
            "Epoch 7: accuracy: 0.4323777777777778, total loss: 738.7862229347229, average loss: 0.01641747162077162\n",
            "Validation: accuracy: 0.4488, total loss: 80.55626225471497, average loss: 0.016111252450942994\n",
            "Epoch 8: accuracy: 0.4473111111111111, total loss: 710.1781259775162, average loss: 0.015781736132833693\n",
            "Validation: accuracy: 0.433, total loss: 85.64392042160034, average loss: 0.01712878408432007\n",
            "Epoch 9: accuracy: 0.4633333333333333, total loss: 687.1285392045975, average loss: 0.015269523093435499\n",
            "Validation: accuracy: 0.4342, total loss: 83.43128550052643, average loss: 0.016686257100105285\n",
            "Epoch 10: accuracy: 0.4727777777777778, total loss: 673.1907893419266, average loss: 0.014959795318709479\n",
            "Validation: accuracy: 0.429, total loss: 85.9675681591034, average loss: 0.01719351363182068\n",
            "Epoch 11: accuracy: 0.48244444444444445, total loss: 660.2735617160797, average loss: 0.014672745815912883\n",
            "Validation: accuracy: 0.4426, total loss: 84.81891679763794, average loss: 0.016963783359527588\n",
            "Epoch 12: accuracy: 0.49348888888888887, total loss: 644.4694901704788, average loss: 0.014321544226010641\n",
            "Validation: accuracy: 0.4312, total loss: 85.87282705307007, average loss: 0.017174565410614014\n",
            "Epoch 13: accuracy: 0.49746666666666667, total loss: 636.1999391317368, average loss: 0.014137776425149705\n",
            "Validation: accuracy: 0.4928, total loss: 74.41532778739929, average loss: 0.014883065557479858\n",
            "Epoch 14: accuracy: 0.5087333333333334, total loss: 624.5673475265503, average loss: 0.013879274389478896\n",
            "Validation: accuracy: 0.4742, total loss: 78.79633295536041, average loss: 0.01575926659107208\n",
            "Epoch 15: accuracy: 0.5157333333333334, total loss: 615.5619115829468, average loss: 0.01367915359073215\n",
            "Validation: accuracy: 0.4144, total loss: 91.38376700878143, average loss: 0.018276753401756288\n",
            "Epoch 16: accuracy: 0.5227111111111111, total loss: 606.9190802574158, average loss: 0.013487090672387018\n",
            "Validation: accuracy: 0.4406, total loss: 84.14421153068542, average loss: 0.016828842306137085\n",
            "Epoch 17: accuracy: 0.5235777777777778, total loss: 602.2595643997192, average loss: 0.013383545875549317\n",
            "Validation: accuracy: 0.4866, total loss: 76.1149240732193, average loss: 0.01522298481464386\n",
            "Epoch 18: accuracy: 0.5311777777777777, total loss: 596.9461815357208, average loss: 0.013265470700793796\n",
            "Validation: accuracy: 0.4778, total loss: 78.65796792507172, average loss: 0.015731593585014345\n",
            "Epoch 19: accuracy: 0.5326666666666666, total loss: 590.496621131897, average loss: 0.013122147136264377\n",
            "Validation: accuracy: 0.4846, total loss: 77.0033757686615, average loss: 0.0154006751537323\n",
            "Epoch 20: accuracy: 0.5379111111111111, total loss: 584.7558113336563, average loss: 0.012994573585192362\n",
            "Validation: accuracy: 0.5178, total loss: 69.9517377614975, average loss: 0.0139903475522995\n",
            "Epoch 21: accuracy: 0.5405777777777778, total loss: 581.1829682588577, average loss: 0.01291517707241906\n",
            "Validation: accuracy: 0.4796, total loss: 78.12989521026611, average loss: 0.015625979042053224\n",
            "Epoch 22: accuracy: 0.5412888888888889, total loss: 580.0552499294281, average loss: 0.012890116665098402\n",
            "Validation: accuracy: 0.5194, total loss: 71.47187733650208, average loss: 0.014294375467300415\n",
            "Epoch 23: accuracy: 0.5434666666666667, total loss: 574.055743098259, average loss: 0.012756794291072421\n",
            "Validation: accuracy: 0.4856, total loss: 78.10741639137268, average loss: 0.015621483278274536\n",
            "Epoch 24: accuracy: 0.5468888888888889, total loss: 571.8548996448517, average loss: 0.012707886658774483\n",
            "Validation: accuracy: 0.496, total loss: 76.6035305261612, average loss: 0.01532070610523224\n",
            "Epoch 25: accuracy: 0.5506666666666666, total loss: 567.429344534874, average loss: 0.012609540989663865\n",
            "Validation: accuracy: 0.4906, total loss: 75.53662538528442, average loss: 0.015107325077056885\n",
            "Epoch 26: accuracy: 0.5487777777777778, total loss: 566.8227785825729, average loss: 0.012596061746279398\n",
            "Validation: accuracy: 0.5188, total loss: 71.79407119750977, average loss: 0.014358814239501954\n",
            "Epoch 27: accuracy: 0.5540666666666667, total loss: 565.8674149513245, average loss: 0.012574831443362766\n",
            "Validation: accuracy: 0.5162, total loss: 71.73039019107819, average loss: 0.014346078038215637\n",
            "Epoch 28: accuracy: 0.5543555555555556, total loss: 561.5703492164612, average loss: 0.012479341093699137\n",
            "Validation: accuracy: 0.534, total loss: 68.29639744758606, average loss: 0.013659279489517211\n",
            "Epoch 29: accuracy: 0.5586222222222222, total loss: 560.9184712171555, average loss: 0.012464854915936787\n",
            "Validation: accuracy: 0.512, total loss: 72.42213928699493, average loss: 0.014484427857398986\n",
            "Epoch 30: accuracy: 0.5589333333333333, total loss: 555.424502491951, average loss: 0.012342766722043355\n",
            "Validation: accuracy: 0.4952, total loss: 75.89965295791626, average loss: 0.015179930591583252\n",
            "Epoch 31: accuracy: 0.5611333333333334, total loss: 556.7551507949829, average loss: 0.012372336684332954\n",
            "Validation: accuracy: 0.4848, total loss: 78.0337324142456, average loss: 0.015606746482849121\n",
            "Epoch 32: accuracy: 0.5652888888888888, total loss: 549.9119458198547, average loss: 0.012220265462663438\n",
            "Validation: accuracy: 0.5298, total loss: 69.7670556306839, average loss: 0.01395341112613678\n",
            "Epoch 33: accuracy: 0.5631555555555555, total loss: 549.1589295864105, average loss: 0.0122035317685869\n",
            "Validation: accuracy: 0.5088, total loss: 74.24363315105438, average loss: 0.014848726630210876\n",
            "Epoch 34: accuracy: 0.5652888888888888, total loss: 549.1203212738037, average loss: 0.012202673806084526\n",
            "Validation: accuracy: 0.4874, total loss: 79.94170141220093, average loss: 0.015988340282440186\n",
            "Epoch 35: accuracy: 0.5672888888888888, total loss: 547.4962124824524, average loss: 0.012166582499610053\n",
            "Validation: accuracy: 0.5172, total loss: 72.0648992061615, average loss: 0.0144129798412323\n",
            "Epoch 36: accuracy: 0.5686, total loss: 546.5427116155624, average loss: 0.012145393591456943\n",
            "Validation: accuracy: 0.4906, total loss: 77.2476863861084, average loss: 0.015449537277221679\n",
            "Epoch 37: accuracy: 0.5701111111111111, total loss: 545.7700877189636, average loss: 0.012128224171532524\n",
            "Validation: accuracy: 0.5392, total loss: 67.7502989768982, average loss: 0.013550059795379639\n",
            "Epoch 38: accuracy: 0.5677111111111112, total loss: 545.1391268968582, average loss: 0.012114202819930182\n",
            "Validation: accuracy: 0.5204, total loss: 72.73425757884979, average loss: 0.01454685151576996\n",
            "Epoch 39: accuracy: 0.5733777777777778, total loss: 539.5113232135773, average loss: 0.011989140515857272\n",
            "Validation: accuracy: 0.4822, total loss: 78.2534362077713, average loss: 0.01565068724155426\n",
            "Epoch 40: accuracy: 0.5751333333333334, total loss: 540.7818151712418, average loss: 0.012017373670472039\n",
            "Validation: accuracy: 0.513, total loss: 73.27355921268463, average loss: 0.014654711842536926\n",
            "Checkpoint saved as `/weights/40.pth`\n",
            "Epoch 41: accuracy: 0.5764888888888889, total loss: 536.7323890924454, average loss: 0.011927386424276563\n",
            "Validation: accuracy: 0.4658, total loss: 87.04005086421967, average loss: 0.01740801017284393\n",
            "Epoch 42: accuracy: 0.5697555555555556, total loss: 541.0539392232895, average loss: 0.012023420871628655\n",
            "Validation: accuracy: 0.4996, total loss: 75.66153311729431, average loss: 0.015132306623458863\n",
            "Epoch 43: accuracy: 0.5732444444444444, total loss: 538.842023730278, average loss: 0.011974267194006178\n",
            "Validation: accuracy: 0.5018, total loss: 74.96035623550415, average loss: 0.01499207124710083\n",
            "Epoch 44: accuracy: 0.5762444444444444, total loss: 535.7279051542282, average loss: 0.01190506455898285\n",
            "Validation: accuracy: 0.4998, total loss: 77.31638085842133, average loss: 0.015463276171684266\n",
            "Epoch 45: accuracy: 0.5748666666666666, total loss: 537.1534558534622, average loss: 0.011936743463410272\n",
            "Validation: accuracy: 0.5034, total loss: 74.86704111099243, average loss: 0.014973408222198486\n",
            "Epoch 46: accuracy: 0.5788666666666666, total loss: 533.8851262331009, average loss: 0.01186411391629113\n",
            "Validation: accuracy: 0.524, total loss: 71.95181155204773, average loss: 0.014390362310409546\n",
            "Epoch 47: accuracy: 0.5771777777777778, total loss: 532.5207260847092, average loss: 0.011833793912993537\n",
            "Validation: accuracy: 0.5344, total loss: 68.59308052062988, average loss: 0.013718616104125976\n",
            "Epoch 48: accuracy: 0.5786888888888889, total loss: 532.892660856247, average loss: 0.011842059130138822\n",
            "Validation: accuracy: 0.4742, total loss: 83.16470515727997, average loss: 0.016632941031455992\n",
            "Epoch 49: accuracy: 0.5784222222222222, total loss: 530.8108580112457, average loss: 0.01179579684469435\n",
            "Validation: accuracy: 0.5282, total loss: 70.36693572998047, average loss: 0.014073387145996093\n",
            "Epoch 50: accuracy: 0.5823777777777778, total loss: 531.9033926129341, average loss: 0.011820075391398536\n",
            "Validation: accuracy: 0.5082, total loss: 73.3763016462326, average loss: 0.014675260329246521\n",
            "Epoch 51: accuracy: 0.5765555555555556, total loss: 532.4479818344116, average loss: 0.011832177374098037\n",
            "Validation: accuracy: 0.5016, total loss: 76.50139605998993, average loss: 0.015300279211997985\n",
            "Epoch 52: accuracy: 0.5816444444444444, total loss: 529.557039141655, average loss: 0.011767934203147888\n",
            "Validation: accuracy: 0.5132, total loss: 73.38100171089172, average loss: 0.014676200342178345\n",
            "Epoch 53: accuracy: 0.5810888888888889, total loss: 528.10382771492, average loss: 0.011735640615887112\n",
            "Validation: accuracy: 0.5134, total loss: 71.39351284503937, average loss: 0.014278702569007873\n",
            "Epoch 54: accuracy: 0.58, total loss: 531.0683619976044, average loss: 0.01180151915550232\n",
            "Validation: accuracy: 0.5442, total loss: 66.31315398216248, average loss: 0.013262630796432496\n",
            "Epoch 55: accuracy: 0.5821777777777778, total loss: 528.9433991909027, average loss: 0.011754297759797838\n",
            "Validation: accuracy: 0.4728, total loss: 82.46377384662628, average loss: 0.016492754769325255\n",
            "Epoch 56: accuracy: 0.5831333333333333, total loss: 527.3802646398544, average loss: 0.01171956143644121\n",
            "Validation: accuracy: 0.521, total loss: 72.22713780403137, average loss: 0.014445427560806275\n",
            "Epoch 57: accuracy: 0.5861777777777778, total loss: 525.7996315956116, average loss: 0.011684436257680257\n",
            "Validation: accuracy: 0.4868, total loss: 78.1416244506836, average loss: 0.01562832489013672\n",
            "Epoch 58: accuracy: 0.5842666666666667, total loss: 527.7565929889679, average loss: 0.011727924288643731\n",
            "Validation: accuracy: 0.5132, total loss: 73.26163482666016, average loss: 0.014652326965332032\n",
            "Epoch 59: accuracy: 0.5848444444444444, total loss: 525.2882544994354, average loss: 0.011673072322209677\n",
            "Validation: accuracy: 0.5238, total loss: 71.66933500766754, average loss: 0.014333867001533508\n",
            "Epoch 60: accuracy: 0.5861111111111111, total loss: 523.4496109485626, average loss: 0.011632213576634725\n",
            "Validation: accuracy: 0.5244, total loss: 70.9898431301117, average loss: 0.014197968626022338\n",
            "Epoch 61: accuracy: 0.5849777777777778, total loss: 522.9791307449341, average loss: 0.011621758460998536\n",
            "Validation: accuracy: 0.5214, total loss: 70.91031646728516, average loss: 0.014182063293457031\n",
            "Epoch 62: accuracy: 0.5870888888888889, total loss: 523.996400475502, average loss: 0.011644364455011156\n",
            "Validation: accuracy: 0.4756, total loss: 82.5361555814743, average loss: 0.01650723111629486\n",
            "Epoch 63: accuracy: 0.5847555555555556, total loss: 523.6182792186737, average loss: 0.01163596176041497\n",
            "Validation: accuracy: 0.5164, total loss: 74.76755559444427, average loss: 0.014953511118888855\n",
            "Epoch 64: accuracy: 0.5829333333333333, total loss: 526.3270428180695, average loss: 0.01169615650706821\n",
            "Validation: accuracy: 0.532, total loss: 70.29030644893646, average loss: 0.014058061289787293\n",
            "Epoch 65: accuracy: 0.5821777777777778, total loss: 525.0595074892044, average loss: 0.011667989055315653\n",
            "Validation: accuracy: 0.5726, total loss: 63.438501715660095, average loss: 0.012687700343132019\n",
            "Epoch 66: accuracy: 0.5932, total loss: 517.6378637552261, average loss: 0.011503063639005025\n",
            "Validation: accuracy: 0.517, total loss: 72.29266154766083, average loss: 0.014458532309532165\n",
            "Epoch 67: accuracy: 0.5854444444444444, total loss: 524.1510953903198, average loss: 0.011647802119784886\n",
            "Validation: accuracy: 0.51, total loss: 71.33441078662872, average loss: 0.014266882157325744\n",
            "Epoch 68: accuracy: 0.5907333333333333, total loss: 518.4342913627625, average loss: 0.01152076203028361\n",
            "Validation: accuracy: 0.5282, total loss: 71.62961983680725, average loss: 0.01432592396736145\n",
            "Epoch 69: accuracy: 0.5849777777777778, total loss: 524.5019886493683, average loss: 0.011655599747763739\n",
            "Validation: accuracy: 0.5518, total loss: 67.46059846878052, average loss: 0.013492119693756103\n",
            "Epoch 70: accuracy: 0.5844222222222222, total loss: 524.250430226326, average loss: 0.011650009560585022\n",
            "Validation: accuracy: 0.5142, total loss: 72.0976927280426, average loss: 0.014419538545608521\n",
            "Epoch 71: accuracy: 0.5881111111111111, total loss: 517.9100239276886, average loss: 0.011509111642837525\n",
            "Validation: accuracy: 0.5326, total loss: 70.7235654592514, average loss: 0.01414471309185028\n",
            "Epoch 72: accuracy: 0.5852222222222222, total loss: 522.9356325864792, average loss: 0.011620791835255093\n",
            "Validation: accuracy: 0.5272, total loss: 70.36293935775757, average loss: 0.014072587871551514\n",
            "Epoch 73: accuracy: 0.5879555555555556, total loss: 517.672239780426, average loss: 0.011503827550676134\n",
            "Validation: accuracy: 0.5122, total loss: 74.9843077659607, average loss: 0.014996861553192138\n",
            "Epoch 74: accuracy: 0.5862888888888889, total loss: 520.8425160646439, average loss: 0.011574278134769864\n",
            "Validation: accuracy: 0.4982, total loss: 77.66123640537262, average loss: 0.015532247281074524\n",
            "Epoch 75: accuracy: 0.5899333333333333, total loss: 520.6568670272827, average loss: 0.011570152600606282\n",
            "Validation: accuracy: 0.5032, total loss: 75.91989362239838, average loss: 0.015183978724479675\n",
            "Epoch 76: accuracy: 0.5878444444444444, total loss: 521.0499591827393, average loss: 0.01157888798183865\n",
            "Validation: accuracy: 0.5332, total loss: 70.5388275384903, average loss: 0.014107765507698058\n",
            "Epoch 77: accuracy: 0.5892444444444445, total loss: 518.1591869592667, average loss: 0.011514648599094814\n",
            "Validation: accuracy: 0.5132, total loss: 76.17577481269836, average loss: 0.015235154962539674\n",
            "Epoch 78: accuracy: 0.5927333333333333, total loss: 514.0992982387543, average loss: 0.011424428849750095\n",
            "Validation: accuracy: 0.5236, total loss: 71.03135240077972, average loss: 0.014206270480155945\n",
            "Epoch 79: accuracy: 0.5907111111111111, total loss: 515.9369587898254, average loss: 0.01146526575088501\n",
            "Validation: accuracy: 0.5168, total loss: 74.3621016740799, average loss: 0.01487242033481598\n",
            "Epoch 80: accuracy: 0.5902444444444445, total loss: 518.4102684259415, average loss: 0.011520228187243143\n",
            "Validation: accuracy: 0.522, total loss: 73.58926153182983, average loss: 0.014717852306365967\n",
            "Checkpoint saved as `/weights/80.pth`\n",
            "Epoch 81: accuracy: 0.5931555555555555, total loss: 512.7538615465164, average loss: 0.011394530256589253\n",
            "Validation: accuracy: 0.5264, total loss: 74.13042604923248, average loss: 0.014826085209846497\n",
            "Epoch 82: accuracy: 0.5894, total loss: 520.278079867363, average loss: 0.011561735108163622\n",
            "Validation: accuracy: 0.5304, total loss: 72.18352782726288, average loss: 0.014436705565452576\n",
            "Epoch 83: accuracy: 0.5928666666666667, total loss: 516.7809863090515, average loss: 0.011484021917978923\n",
            "Validation: accuracy: 0.5338, total loss: 70.59730327129364, average loss: 0.014119460654258727\n",
            "Epoch 84: accuracy: 0.5938444444444444, total loss: 513.5008215904236, average loss: 0.01141112936867608\n",
            "Validation: accuracy: 0.5188, total loss: 71.35837292671204, average loss: 0.014271674585342408\n",
            "Epoch 85: accuracy: 0.5917111111111111, total loss: 515.6390945911407, average loss: 0.011458646546469794\n",
            "Validation: accuracy: 0.511, total loss: 74.3223147392273, average loss: 0.014864462947845459\n",
            "Epoch 86: accuracy: 0.5921555555555555, total loss: 514.3540464639664, average loss: 0.011430089921421475\n",
            "Validation: accuracy: 0.5304, total loss: 72.16418099403381, average loss: 0.014432836198806762\n",
            "Epoch 87: accuracy: 0.5921777777777778, total loss: 516.6710534095764, average loss: 0.011481578964657254\n",
            "Validation: accuracy: 0.5408, total loss: 67.77016746997833, average loss: 0.013554033493995667\n",
            "Epoch 88: accuracy: 0.5922222222222222, total loss: 513.7901240587234, average loss: 0.011417558312416077\n",
            "Validation: accuracy: 0.5286, total loss: 71.26132023334503, average loss: 0.014252264046669006\n",
            "Epoch 89: accuracy: 0.5958666666666667, total loss: 512.2114382982254, average loss: 0.011382476406627231\n",
            "Validation: accuracy: 0.5334, total loss: 69.08854639530182, average loss: 0.013817709279060364\n",
            "Epoch 90: accuracy: 0.5930888888888889, total loss: 514.3837398290634, average loss: 0.011430749773979187\n",
            "Validation: accuracy: 0.5328, total loss: 69.03226447105408, average loss: 0.013806452894210815\n",
            "Epoch 91: accuracy: 0.5941777777777778, total loss: 513.4693672657013, average loss: 0.011410430383682252\n",
            "Validation: accuracy: 0.5226, total loss: 71.9315733909607, average loss: 0.014386314678192139\n",
            "Epoch 92: accuracy: 0.594, total loss: 514.855462372303, average loss: 0.01144123249716229\n",
            "Validation: accuracy: 0.5218, total loss: 72.41081094741821, average loss: 0.014482162189483642\n",
            "Epoch 93: accuracy: 0.5916666666666667, total loss: 516.853835940361, average loss: 0.011485640798674689\n",
            "Validation: accuracy: 0.5158, total loss: 74.0177788734436, average loss: 0.01480355577468872\n",
            "Epoch 94: accuracy: 0.5944666666666667, total loss: 514.4490520954132, average loss: 0.01143220115767585\n",
            "Validation: accuracy: 0.4942, total loss: 77.97236800193787, average loss: 0.015594473600387574\n",
            "Epoch 95: accuracy: 0.5970666666666666, total loss: 511.10858738422394, average loss: 0.01135796860853831\n",
            "Validation: accuracy: 0.5072, total loss: 75.49668514728546, average loss: 0.015099337029457092\n",
            "Epoch 96: accuracy: 0.5936444444444444, total loss: 515.8472231626511, average loss: 0.01146327162583669\n",
            "Validation: accuracy: 0.5502, total loss: 67.4357488155365, average loss: 0.0134871497631073\n",
            "Epoch 97: accuracy: 0.5964444444444444, total loss: 511.8332276940346, average loss: 0.011374071726534101\n",
            "Validation: accuracy: 0.5162, total loss: 75.39420783519745, average loss: 0.015078841567039489\n",
            "Epoch 98: accuracy: 0.5986222222222222, total loss: 511.6132332086563, average loss: 0.011369182960192362\n",
            "Validation: accuracy: 0.544, total loss: 67.53706860542297, average loss: 0.013507413721084594\n",
            "Epoch 99: accuracy: 0.5951333333333333, total loss: 512.6645671129227, average loss: 0.011392545935842727\n",
            "Validation: accuracy: 0.5034, total loss: 74.34652054309845, average loss: 0.01486930410861969\n",
            "Epoch 100: accuracy: 0.5933111111111111, total loss: 513.2665829658508, average loss: 0.011405924065907796\n",
            "Validation: accuracy: 0.5136, total loss: 71.33748495578766, average loss: 0.014267496991157532\n",
            "Epoch 101: accuracy: 0.5952222222222222, total loss: 511.4134656190872, average loss: 0.01136474368042416\n",
            "Validation: accuracy: 0.4996, total loss: 77.10317766666412, average loss: 0.015420635533332824\n",
            "Epoch 102: accuracy: 0.5946666666666667, total loss: 513.5005601644516, average loss: 0.011411123559210036\n",
            "Validation: accuracy: 0.559, total loss: 67.38713216781616, average loss: 0.013477426433563233\n",
            "Epoch 103: accuracy: 0.5956666666666667, total loss: 513.2156329154968, average loss: 0.011404791842566596\n",
            "Validation: accuracy: 0.504, total loss: 76.0899007320404, average loss: 0.01521798014640808\n",
            "Epoch 104: accuracy: 0.596, total loss: 510.17920184135437, average loss: 0.011337315596474542\n",
            "Validation: accuracy: 0.543, total loss: 68.39703249931335, average loss: 0.01367940649986267\n",
            "Epoch 105: accuracy: 0.5999111111111111, total loss: 505.75705939531326, average loss: 0.011239045764340296\n",
            "Validation: accuracy: 0.5298, total loss: 72.00820112228394, average loss: 0.014401640224456787\n",
            "Epoch 106: accuracy: 0.5975777777777778, total loss: 511.40074050426483, average loss: 0.011364460900094775\n",
            "Validation: accuracy: 0.5158, total loss: 72.88952076435089, average loss: 0.014577904152870177\n",
            "Epoch 107: accuracy: 0.5956, total loss: 510.3340697288513, average loss: 0.011340757105085584\n",
            "Validation: accuracy: 0.5128, total loss: 75.90722477436066, average loss: 0.015181444954872132\n",
            "Epoch 108: accuracy: 0.5968444444444444, total loss: 511.8109920024872, average loss: 0.01137357760005527\n",
            "Validation: accuracy: 0.4886, total loss: 76.28027498722076, average loss: 0.015256054997444153\n",
            "Epoch 109: accuracy: 0.5976222222222223, total loss: 509.06111109256744, average loss: 0.011312469135390388\n",
            "Validation: accuracy: 0.5208, total loss: 74.29167640209198, average loss: 0.014858335280418395\n",
            "Epoch 110: accuracy: 0.5958666666666667, total loss: 511.0019608736038, average loss: 0.01135559913052453\n",
            "Validation: accuracy: 0.543, total loss: 69.34002423286438, average loss: 0.013868004846572876\n",
            "Epoch 111: accuracy: 0.5940666666666666, total loss: 513.1688435077667, average loss: 0.011403752077950371\n",
            "Validation: accuracy: 0.5418, total loss: 67.84777987003326, average loss: 0.013569555974006654\n",
            "Epoch 112: accuracy: 0.5991111111111111, total loss: 508.34183526039124, average loss: 0.011296485228008694\n",
            "Validation: accuracy: 0.5452, total loss: 66.98619389533997, average loss: 0.013397238779067993\n",
            "Epoch 113: accuracy: 0.5954888888888888, total loss: 513.1382368803024, average loss: 0.011403071930673388\n",
            "Validation: accuracy: 0.5216, total loss: 71.88308358192444, average loss: 0.014376616716384887\n",
            "Epoch 114: accuracy: 0.5990888888888889, total loss: 504.53277015686035, average loss: 0.011211839336819119\n",
            "Validation: accuracy: 0.568, total loss: 65.27712559700012, average loss: 0.013055425119400024\n",
            "Epoch 115: accuracy: 0.5975111111111111, total loss: 505.8611597418785, average loss: 0.011241359105375078\n",
            "Validation: accuracy: 0.52, total loss: 74.50073766708374, average loss: 0.014900147533416747\n",
            "Epoch 116: accuracy: 0.5988666666666667, total loss: 507.25823426246643, average loss: 0.011272405205832588\n",
            "Validation: accuracy: 0.545, total loss: 67.84249091148376, average loss: 0.013568498182296752\n",
            "Epoch 117: accuracy: 0.5966222222222223, total loss: 509.3281272649765, average loss: 0.011318402828110589\n",
            "Validation: accuracy: 0.5504, total loss: 68.18077725172043, average loss: 0.013636155450344085\n",
            "Epoch 118: accuracy: 0.5989777777777778, total loss: 506.7235372066498, average loss: 0.011260523049036662\n",
            "Validation: accuracy: 0.4986, total loss: 76.73116743564606, average loss: 0.015346233487129211\n",
            "Epoch 119: accuracy: 0.5956666666666667, total loss: 510.612686753273, average loss: 0.011346948594517178\n",
            "Validation: accuracy: 0.5058, total loss: 75.96321558952332, average loss: 0.015192643117904664\n",
            "Epoch 120: accuracy: 0.5964222222222222, total loss: 509.6296705007553, average loss: 0.011325103788905673\n",
            "Validation: accuracy: 0.5364, total loss: 68.8447870016098, average loss: 0.01376895740032196\n",
            "Checkpoint saved as `/weights/120.pth`\n",
            "Epoch 121: accuracy: 0.5961555555555555, total loss: 509.874892950058, average loss: 0.011330553176667955\n",
            "Validation: accuracy: 0.5308, total loss: 70.67348253726959, average loss: 0.014134696507453918\n",
            "Epoch 122: accuracy: 0.5996666666666667, total loss: 508.34636092185974, average loss: 0.01129658579826355\n",
            "Validation: accuracy: 0.5666, total loss: 64.41434502601624, average loss: 0.012882869005203247\n",
            "Epoch 123: accuracy: 0.5992, total loss: 507.30350279808044, average loss: 0.011273411173290676\n",
            "Validation: accuracy: 0.532, total loss: 70.85528516769409, average loss: 0.014171057033538818\n",
            "Epoch 124: accuracy: 0.5935777777777778, total loss: 512.5103571414948, average loss: 0.011389119047588773\n",
            "Validation: accuracy: 0.515, total loss: 73.30962181091309, average loss: 0.014661924362182617\n",
            "Epoch 125: accuracy: 0.5981333333333333, total loss: 506.2367957830429, average loss: 0.011249706572956509\n",
            "Validation: accuracy: 0.5236, total loss: 72.33023393154144, average loss: 0.014466046786308289\n",
            "Epoch 126: accuracy: 0.6017777777777777, total loss: 504.9774340391159, average loss: 0.011221720756424797\n",
            "Validation: accuracy: 0.5284, total loss: 74.4289162158966, average loss: 0.014885783243179322\n",
            "Epoch 127: accuracy: 0.5969555555555556, total loss: 506.18108344078064, average loss: 0.011248468520906236\n",
            "Validation: accuracy: 0.4772, total loss: 83.39087808132172, average loss: 0.01667817561626434\n",
            "Epoch 128: accuracy: 0.5992666666666666, total loss: 509.8285356760025, average loss: 0.011329523015022278\n",
            "Validation: accuracy: 0.496, total loss: 79.75964558124542, average loss: 0.015951929116249083\n",
            "Epoch 129: accuracy: 0.5971111111111111, total loss: 509.61095106601715, average loss: 0.011324687801467047\n",
            "Validation: accuracy: 0.5688, total loss: 63.71774923801422, average loss: 0.012743549847602844\n",
            "Epoch 130: accuracy: 0.5971555555555556, total loss: 506.8124796152115, average loss: 0.011262499547004699\n",
            "Validation: accuracy: 0.552, total loss: 66.51308715343475, average loss: 0.01330261743068695\n",
            "Epoch 131: accuracy: 0.5982666666666666, total loss: 506.2131620645523, average loss: 0.011249181379212273\n",
            "Validation: accuracy: 0.4948, total loss: 77.52681255340576, average loss: 0.015505362510681152\n",
            "Epoch 132: accuracy: 0.5983555555555555, total loss: 507.547513961792, average loss: 0.011278833643595377\n",
            "Validation: accuracy: 0.561, total loss: 64.15874230861664, average loss: 0.012831748461723328\n",
            "Epoch 133: accuracy: 0.5985111111111111, total loss: 507.17033421993256, average loss: 0.011270451871554057\n",
            "Validation: accuracy: 0.5364, total loss: 69.75705778598785, average loss: 0.013951411557197571\n",
            "Epoch 134: accuracy: 0.6028222222222223, total loss: 504.8422759771347, average loss: 0.011218717243936327\n",
            "Validation: accuracy: 0.5114, total loss: 74.30928874015808, average loss: 0.014861857748031617\n",
            "Epoch 135: accuracy: 0.6012888888888889, total loss: 504.7156660556793, average loss: 0.011215903690126207\n",
            "Validation: accuracy: 0.5266, total loss: 73.85184836387634, average loss: 0.014770369672775268\n",
            "Epoch 136: accuracy: 0.5980888888888889, total loss: 508.577993452549, average loss: 0.011301733187834421\n",
            "Validation: accuracy: 0.5034, total loss: 76.48882222175598, average loss: 0.015297764444351197\n",
            "Epoch 137: accuracy: 0.5987777777777777, total loss: 507.30362820625305, average loss: 0.011273413960138957\n",
            "Validation: accuracy: 0.5286, total loss: 69.8882452249527, average loss: 0.013977649044990539\n",
            "Epoch 138: accuracy: 0.5985777777777778, total loss: 507.3682070970535, average loss: 0.01127484904660119\n",
            "Validation: accuracy: 0.5522, total loss: 66.19417655467987, average loss: 0.013238835310935974\n",
            "Epoch 139: accuracy: 0.6013111111111111, total loss: 505.16008853912354, average loss: 0.011225779745313856\n",
            "Validation: accuracy: 0.5092, total loss: 75.76329386234283, average loss: 0.015152658772468568\n",
            "Epoch 140: accuracy: 0.5974444444444444, total loss: 507.8866333961487, average loss: 0.011286369631025526\n",
            "Validation: accuracy: 0.5474, total loss: 68.0941099524498, average loss: 0.013618821990489959\n",
            "Epoch 141: accuracy: 0.5992888888888889, total loss: 508.5250151157379, average loss: 0.011300555891460843\n",
            "Validation: accuracy: 0.5584, total loss: 65.83798545598984, average loss: 0.013167597091197967\n",
            "Epoch 142: accuracy: 0.5992222222222222, total loss: 504.94915187358856, average loss: 0.011221092263857523\n",
            "Validation: accuracy: 0.5494, total loss: 65.32880008220673, average loss: 0.013065760016441346\n",
            "Epoch 143: accuracy: 0.5964666666666667, total loss: 510.16632187366486, average loss: 0.01133702937497033\n",
            "Validation: accuracy: 0.5166, total loss: 73.64055597782135, average loss: 0.01472811119556427\n",
            "Epoch 144: accuracy: 0.5974222222222222, total loss: 508.52498883008957, average loss: 0.011300555307335324\n",
            "Validation: accuracy: 0.5452, total loss: 66.42540204524994, average loss: 0.013285080409049988\n",
            "Epoch 145: accuracy: 0.6002222222222222, total loss: 504.3114497065544, average loss: 0.011206921104590099\n",
            "Validation: accuracy: 0.5456, total loss: 67.89932131767273, average loss: 0.013579864263534545\n",
            "Epoch 146: accuracy: 0.5984222222222222, total loss: 507.28156208992004, average loss: 0.011272923601998223\n",
            "Validation: accuracy: 0.527, total loss: 70.89756941795349, average loss: 0.014179513883590698\n",
            "Epoch 147: accuracy: 0.6022888888888889, total loss: 504.7387046813965, average loss: 0.011216415659586589\n",
            "Validation: accuracy: 0.5222, total loss: 71.58614206314087, average loss: 0.014317228412628174\n",
            "Epoch 148: accuracy: 0.6002444444444445, total loss: 505.9022784233093, average loss: 0.011242272853851319\n",
            "Validation: accuracy: 0.5344, total loss: 69.60029888153076, average loss: 0.013920059776306152\n",
            "Epoch 149: accuracy: 0.6012444444444445, total loss: 504.6242427825928, average loss: 0.011213872061835394\n",
            "Validation: accuracy: 0.5234, total loss: 71.40994071960449, average loss: 0.014281988143920898\n",
            "Epoch 150: accuracy: 0.5982222222222222, total loss: 504.76466512680054, average loss: 0.011216992558373346\n",
            "Validation: accuracy: 0.543, total loss: 69.78037750720978, average loss: 0.013956075501441956\n",
            "Epoch 151: accuracy: 0.7077111111111111, total loss: 358.1100751757622, average loss: 0.007958001670572493\n",
            "Validation: accuracy: 0.7014, total loss: 41.00542050600052, average loss: 0.008201084101200103\n",
            "Epoch 152: accuracy: 0.7477777777777778, total loss: 305.2159831523895, average loss: 0.006782577403386434\n",
            "Validation: accuracy: 0.7148, total loss: 39.292563676834106, average loss: 0.007858512735366821\n",
            "Epoch 153: accuracy: 0.7613111111111112, total loss: 284.27001333236694, average loss: 0.006317111407385932\n",
            "Validation: accuracy: 0.721, total loss: 38.35344696044922, average loss: 0.007670689392089844\n",
            "Epoch 154: accuracy: 0.7710222222222223, total loss: 270.6922152042389, average loss: 0.006015382560094197\n",
            "Validation: accuracy: 0.722, total loss: 38.02128875255585, average loss: 0.007604257750511169\n",
            "Epoch 155: accuracy: 0.7824888888888889, total loss: 255.53147557377815, average loss: 0.005678477234972848\n",
            "Validation: accuracy: 0.729, total loss: 38.098579317331314, average loss: 0.007619715863466263\n",
            "Epoch 156: accuracy: 0.7898222222222222, total loss: 247.02186501026154, average loss: 0.005489374778005812\n",
            "Validation: accuracy: 0.7274, total loss: 37.992325872182846, average loss: 0.007598465174436569\n",
            "Epoch 157: accuracy: 0.7966, total loss: 238.08573698997498, average loss: 0.005290794155332778\n",
            "Validation: accuracy: 0.7288, total loss: 37.91604596376419, average loss: 0.0075832091927528385\n",
            "Epoch 158: accuracy: 0.8036666666666666, total loss: 229.50127911567688, average loss: 0.005100028424792819\n",
            "Validation: accuracy: 0.7256, total loss: 38.11438137292862, average loss: 0.007622876274585724\n",
            "Epoch 159: accuracy: 0.8105111111111111, total loss: 221.13475903868675, average loss: 0.004914105756415261\n",
            "Validation: accuracy: 0.7284, total loss: 37.956372022628784, average loss: 0.007591274404525757\n",
            "Epoch 160: accuracy: 0.8141111111111111, total loss: 215.08940103650093, average loss: 0.004779764467477798\n",
            "Validation: accuracy: 0.736, total loss: 37.62515339255333, average loss: 0.007525030678510666\n",
            "Checkpoint saved as `/weights/160.pth`\n",
            "Epoch 161: accuracy: 0.8213777777777778, total loss: 206.81040903925896, average loss: 0.004595786867539088\n",
            "Validation: accuracy: 0.73, total loss: 38.679980993270874, average loss: 0.007735996198654175\n",
            "Epoch 162: accuracy: 0.8237111111111111, total loss: 202.48962944746017, average loss: 0.004499769543276893\n",
            "Validation: accuracy: 0.7284, total loss: 38.1392462849617, average loss: 0.0076278492569923405\n",
            "Epoch 163: accuracy: 0.8285111111111111, total loss: 196.56419432163239, average loss: 0.004368093207147386\n",
            "Validation: accuracy: 0.7272, total loss: 38.51091679930687, average loss: 0.007702183359861374\n",
            "Epoch 164: accuracy: 0.8340888888888889, total loss: 192.61739966273308, average loss: 0.004280386659171846\n",
            "Validation: accuracy: 0.7322, total loss: 38.336080849170685, average loss: 0.007667216169834137\n",
            "Epoch 165: accuracy: 0.8370222222222222, total loss: 187.02401265501976, average loss: 0.00415608917011155\n",
            "Validation: accuracy: 0.7356, total loss: 37.61431837081909, average loss: 0.007522863674163818\n",
            "Epoch 166: accuracy: 0.8370222222222222, total loss: 185.3935824930668, average loss: 0.004119857388734817\n",
            "Validation: accuracy: 0.7216, total loss: 40.14441478252411, average loss: 0.008028882956504823\n",
            "Epoch 167: accuracy: 0.8404666666666667, total loss: 183.82099559903145, average loss: 0.00408491101331181\n",
            "Validation: accuracy: 0.7318, total loss: 38.503810584545135, average loss: 0.0077007621169090275\n",
            "Epoch 168: accuracy: 0.8440888888888889, total loss: 177.30288472771645, average loss: 0.003940064105060366\n",
            "Validation: accuracy: 0.7268, total loss: 39.08160716295242, average loss: 0.007816321432590485\n",
            "Epoch 169: accuracy: 0.8450222222222222, total loss: 177.2473036646843, average loss: 0.003938828970326318\n",
            "Validation: accuracy: 0.7206, total loss: 40.29213559627533, average loss: 0.008058427119255066\n",
            "Epoch 170: accuracy: 0.8471777777777778, total loss: 174.42334187030792, average loss: 0.0038760742637846206\n",
            "Validation: accuracy: 0.7258, total loss: 41.44784468412399, average loss: 0.008289568936824799\n",
            "Epoch 171: accuracy: 0.8497555555555556, total loss: 171.89212760329247, average loss: 0.003819825057850944\n",
            "Validation: accuracy: 0.7146, total loss: 42.50513982772827, average loss: 0.008501027965545655\n",
            "Epoch 172: accuracy: 0.8499333333333333, total loss: 170.88580814003944, average loss: 0.0037974624031119875\n",
            "Validation: accuracy: 0.715, total loss: 41.344153970479965, average loss: 0.008268830794095993\n",
            "Epoch 173: accuracy: 0.8543111111111111, total loss: 166.65913370251656, average loss: 0.003703536304500368\n",
            "Validation: accuracy: 0.7212, total loss: 41.39870873093605, average loss: 0.00827974174618721\n",
            "Epoch 174: accuracy: 0.8517111111111111, total loss: 168.44445684552193, average loss: 0.0037432101521227097\n",
            "Validation: accuracy: 0.7192, total loss: 41.14580577611923, average loss: 0.008229161155223846\n",
            "Epoch 175: accuracy: 0.8559777777777777, total loss: 166.2076060473919, average loss: 0.0036935023566087087\n",
            "Validation: accuracy: 0.7274, total loss: 40.77104890346527, average loss: 0.008154209780693054\n",
            "Epoch 176: accuracy: 0.8524888888888889, total loss: 168.1183359324932, average loss: 0.0037359630207220713\n",
            "Validation: accuracy: 0.7206, total loss: 41.31513133645058, average loss: 0.008263026267290115\n",
            "Epoch 177: accuracy: 0.8575111111111111, total loss: 163.9897238612175, average loss: 0.0036442160858048333\n",
            "Validation: accuracy: 0.7134, total loss: 41.945661783218384, average loss: 0.008389132356643677\n",
            "Epoch 178: accuracy: 0.8550888888888889, total loss: 164.57080161571503, average loss: 0.0036571289247936674\n",
            "Validation: accuracy: 0.7182, total loss: 41.995739102363586, average loss: 0.008399147820472717\n",
            "Epoch 179: accuracy: 0.8574222222222222, total loss: 163.77542784810066, average loss: 0.003639453952180015\n",
            "Validation: accuracy: 0.7122, total loss: 42.63761132955551, average loss: 0.008527522265911103\n",
            "Epoch 180: accuracy: 0.8574444444444445, total loss: 162.27531054615974, average loss: 0.0036061180121368832\n",
            "Validation: accuracy: 0.7084, total loss: 43.2250859439373, average loss: 0.008645017188787461\n",
            "Epoch 181: accuracy: 0.8570222222222222, total loss: 161.22969348728657, average loss: 0.003582882077495257\n",
            "Validation: accuracy: 0.7134, total loss: 42.8390057682991, average loss: 0.008567801153659821\n",
            "Epoch 182: accuracy: 0.8578888888888889, total loss: 161.67972683906555, average loss: 0.003592882818645901\n",
            "Validation: accuracy: 0.7188, total loss: 41.94080048799515, average loss: 0.00838816009759903\n",
            "Epoch 183: accuracy: 0.8610666666666666, total loss: 159.1464630663395, average loss: 0.0035365880681408777\n",
            "Validation: accuracy: 0.7228, total loss: 42.30058312416077, average loss: 0.008460116624832153\n",
            "Epoch 184: accuracy: 0.8572222222222222, total loss: 162.0009872019291, average loss: 0.0036000219378206464\n",
            "Validation: accuracy: 0.6952, total loss: 46.401426911354065, average loss: 0.009280285382270813\n",
            "Epoch 185: accuracy: 0.8581555555555556, total loss: 162.86300683021545, average loss: 0.0036191779295603435\n",
            "Validation: accuracy: 0.7146, total loss: 42.21383345127106, average loss: 0.008442766690254212\n",
            "Epoch 186: accuracy: 0.8602, total loss: 160.38034717738628, average loss: 0.0035640077150530286\n",
            "Validation: accuracy: 0.7152, total loss: 42.83762675523758, average loss: 0.008567525351047515\n",
            "Epoch 187: accuracy: 0.8589777777777777, total loss: 160.16246484220028, average loss: 0.0035591658853822285\n",
            "Validation: accuracy: 0.71, total loss: 44.07864195108414, average loss: 0.008815728390216828\n",
            "Epoch 188: accuracy: 0.8564666666666667, total loss: 162.86891946196556, average loss: 0.0036193093213770125\n",
            "Validation: accuracy: 0.7058, total loss: 45.105399668216705, average loss: 0.009021079933643341\n",
            "Epoch 189: accuracy: 0.8614, total loss: 158.14907011389732, average loss: 0.0035144237803088293\n",
            "Validation: accuracy: 0.7232, total loss: 42.585137724876404, average loss: 0.00851702754497528\n",
            "Epoch 190: accuracy: 0.8610666666666666, total loss: 158.85015200078487, average loss: 0.0035300033777952195\n",
            "Validation: accuracy: 0.707, total loss: 45.55054885149002, average loss: 0.009110109770298004\n",
            "Epoch 191: accuracy: 0.8650666666666667, total loss: 154.26420667767525, average loss: 0.0034280934817261166\n",
            "Validation: accuracy: 0.703, total loss: 45.364252269268036, average loss: 0.009072850453853608\n",
            "Epoch 192: accuracy: 0.8590666666666666, total loss: 160.3809445798397, average loss: 0.0035640209906631047\n",
            "Validation: accuracy: 0.706, total loss: 46.22225338220596, average loss: 0.009244450676441193\n",
            "Epoch 193: accuracy: 0.8628888888888889, total loss: 155.91479848325253, average loss: 0.003464773299627834\n",
            "Validation: accuracy: 0.6994, total loss: 45.72289901971817, average loss: 0.009144579803943634\n",
            "Epoch 194: accuracy: 0.8614222222222222, total loss: 159.0247192978859, average loss: 0.003533882651064131\n",
            "Validation: accuracy: 0.7176, total loss: 43.350711703300476, average loss: 0.008670142340660094\n",
            "Epoch 195: accuracy: 0.8622666666666666, total loss: 156.71824364364147, average loss: 0.003482627636525366\n",
            "Validation: accuracy: 0.7158, total loss: 43.302528619766235, average loss: 0.008660505723953247\n",
            "Epoch 196: accuracy: 0.8628888888888889, total loss: 156.20360615849495, average loss: 0.0034711912479665546\n",
            "Validation: accuracy: 0.7178, total loss: 43.14930957555771, average loss: 0.008629861915111541\n",
            "Epoch 197: accuracy: 0.8648666666666667, total loss: 154.55775287747383, average loss: 0.0034346167306105297\n",
            "Validation: accuracy: 0.7116, total loss: 43.42340636253357, average loss: 0.008684681272506713\n",
            "Epoch 198: accuracy: 0.8625111111111111, total loss: 156.0663617402315, average loss: 0.0034681413720051447\n",
            "Validation: accuracy: 0.7088, total loss: 44.589944660663605, average loss: 0.008917988932132722\n",
            "Epoch 199: accuracy: 0.8672666666666666, total loss: 152.24330048263073, average loss: 0.0033831844551695718\n",
            "Validation: accuracy: 0.712, total loss: 44.97357851266861, average loss: 0.008994715702533723\n",
            "Epoch 200: accuracy: 0.8660666666666667, total loss: 153.01533852517605, average loss: 0.0034003408561150235\n",
            "Validation: accuracy: 0.7104, total loss: 44.703239381313324, average loss: 0.008940647876262664\n",
            "Checkpoint saved as `/weights/200.pth`\n",
            "Epoch 201: accuracy: 0.8664444444444445, total loss: 153.2391654253006, average loss: 0.0034053147872289023\n",
            "Validation: accuracy: 0.6974, total loss: 46.77473706007004, average loss: 0.009354947412014007\n",
            "Epoch 202: accuracy: 0.8632444444444445, total loss: 155.97188445925713, average loss: 0.0034660418768723807\n",
            "Validation: accuracy: 0.7078, total loss: 45.274320900440216, average loss: 0.009054864180088043\n",
            "Epoch 203: accuracy: 0.8646666666666667, total loss: 154.5957606136799, average loss: 0.0034354613469706643\n",
            "Validation: accuracy: 0.7026, total loss: 45.983871042728424, average loss: 0.009196774208545686\n",
            "Epoch 204: accuracy: 0.8664888888888889, total loss: 151.59236052632332, average loss: 0.003368719122807185\n",
            "Validation: accuracy: 0.7002, total loss: 45.643154978752136, average loss: 0.009128630995750427\n",
            "Epoch 205: accuracy: 0.8678888888888889, total loss: 150.6036070883274, average loss: 0.0033467468241850534\n",
            "Validation: accuracy: 0.7128, total loss: 44.67425096035004, average loss: 0.008934850192070007\n",
            "Epoch 206: accuracy: 0.8662888888888889, total loss: 152.73706977069378, average loss: 0.0033941571060154174\n",
            "Validation: accuracy: 0.7156, total loss: 43.60995942354202, average loss: 0.008721991884708404\n",
            "Epoch 207: accuracy: 0.8697555555555555, total loss: 150.25353445112705, average loss: 0.003338967432247268\n",
            "Validation: accuracy: 0.7028, total loss: 45.23620027303696, average loss: 0.009047240054607391\n",
            "Epoch 208: accuracy: 0.8654888888888889, total loss: 153.47710445523262, average loss: 0.0034106023212273915\n",
            "Validation: accuracy: 0.6972, total loss: 47.22643738985062, average loss: 0.009445287477970124\n",
            "Epoch 209: accuracy: 0.8682, total loss: 151.6604132205248, average loss: 0.003370231404900551\n",
            "Validation: accuracy: 0.7006, total loss: 46.344040870666504, average loss: 0.009268808174133301\n",
            "Epoch 210: accuracy: 0.8695777777777778, total loss: 148.5276349335909, average loss: 0.003300614109635353\n",
            "Validation: accuracy: 0.7084, total loss: 44.40793514251709, average loss: 0.008881587028503418\n",
            "Epoch 211: accuracy: 0.8663111111111111, total loss: 151.0613778680563, average loss: 0.0033569195081790287\n",
            "Validation: accuracy: 0.7104, total loss: 45.31741577386856, average loss: 0.009063483154773712\n",
            "Epoch 212: accuracy: 0.8682222222222222, total loss: 151.04637244343758, average loss: 0.003356586054298613\n",
            "Validation: accuracy: 0.7082, total loss: 45.72514981031418, average loss: 0.009145029962062836\n",
            "Epoch 213: accuracy: 0.8695555555555555, total loss: 149.76567183434963, average loss: 0.0033281260407633253\n",
            "Validation: accuracy: 0.7088, total loss: 45.67075175046921, average loss: 0.009134150350093842\n",
            "Epoch 214: accuracy: 0.8700666666666667, total loss: 146.95984245836735, average loss: 0.0032657742768526076\n",
            "Validation: accuracy: 0.7142, total loss: 44.86485701799393, average loss: 0.008972971403598786\n",
            "Epoch 215: accuracy: 0.8681555555555556, total loss: 150.5478623956442, average loss: 0.0033455080532365376\n",
            "Validation: accuracy: 0.7064, total loss: 46.80257749557495, average loss: 0.00936051549911499\n",
            "Epoch 216: accuracy: 0.8678666666666667, total loss: 149.49353943765163, average loss: 0.0033220786541700363\n",
            "Validation: accuracy: 0.709, total loss: 46.056621968746185, average loss: 0.009211324393749236\n",
            "Epoch 217: accuracy: 0.8715111111111111, total loss: 146.37979350984097, average loss: 0.003252884300218688\n",
            "Validation: accuracy: 0.7072, total loss: 46.966059029102325, average loss: 0.009393211805820464\n",
            "Epoch 218: accuracy: 0.8737555555555555, total loss: 144.02443827688694, average loss: 0.0032005430728197096\n",
            "Validation: accuracy: 0.705, total loss: 45.32447800040245, average loss: 0.00906489560008049\n",
            "Epoch 219: accuracy: 0.8744, total loss: 143.95953872799873, average loss: 0.003199100860622194\n",
            "Validation: accuracy: 0.7114, total loss: 45.018716275691986, average loss: 0.009003743255138398\n",
            "Epoch 220: accuracy: 0.8723555555555556, total loss: 145.41863989830017, average loss: 0.003231525331073337\n",
            "Validation: accuracy: 0.7076, total loss: 47.2922847867012, average loss: 0.00945845695734024\n",
            "Epoch 221: accuracy: 0.8729555555555556, total loss: 144.89548425376415, average loss: 0.003219899650083648\n",
            "Validation: accuracy: 0.7142, total loss: 44.387567803263664, average loss: 0.008877513560652733\n",
            "Epoch 222: accuracy: 0.8754666666666666, total loss: 143.19191034138203, average loss: 0.003182042452030712\n",
            "Validation: accuracy: 0.7034, total loss: 47.41035282611847, average loss: 0.009482070565223695\n",
            "Epoch 223: accuracy: 0.8752666666666666, total loss: 144.33128710091114, average loss: 0.003207361935575803\n",
            "Validation: accuracy: 0.7184, total loss: 44.6236789226532, average loss: 0.00892473578453064\n",
            "Epoch 224: accuracy: 0.8796444444444445, total loss: 136.69462610781193, average loss: 0.0030376583579513764\n",
            "Validation: accuracy: 0.7202, total loss: 45.25300467014313, average loss: 0.009050600934028626\n",
            "Epoch 225: accuracy: 0.8769777777777777, total loss: 141.88294659554958, average loss: 0.0031529543687899906\n",
            "Validation: accuracy: 0.6946, total loss: 48.90663552284241, average loss: 0.009781327104568482\n",
            "Epoch 226: accuracy: 0.9128222222222222, total loss: 102.57871575653553, average loss: 0.0022795270168119008\n",
            "Validation: accuracy: 0.7518, total loss: 37.78316089510918, average loss: 0.007556632179021835\n",
            "Epoch 227: accuracy: 0.9327555555555556, total loss: 80.18620995432138, average loss: 0.0017819157767626974\n",
            "Validation: accuracy: 0.7536, total loss: 37.14902317523956, average loss: 0.007429804635047913\n",
            "Epoch 228: accuracy: 0.9376444444444444, total loss: 74.37979544699192, average loss: 0.0016528843432664871\n",
            "Validation: accuracy: 0.753, total loss: 37.06122273206711, average loss: 0.007412244546413422\n",
            "Epoch 229: accuracy: 0.9430888888888889, total loss: 68.4953696206212, average loss: 0.0015221193249026935\n",
            "Validation: accuracy: 0.7566, total loss: 36.87792582809925, average loss: 0.00737558516561985\n",
            "Epoch 230: accuracy: 0.9472, total loss: 63.83979307860136, average loss: 0.0014186620684133635\n",
            "Validation: accuracy: 0.758, total loss: 36.95367035269737, average loss: 0.007390734070539475\n",
            "Epoch 231: accuracy: 0.9498666666666666, total loss: 61.22156263887882, average loss: 0.0013604791697528627\n",
            "Validation: accuracy: 0.7574, total loss: 36.87875582277775, average loss: 0.00737575116455555\n",
            "Epoch 232: accuracy: 0.9518, total loss: 58.14593169093132, average loss: 0.0012921318153540293\n",
            "Validation: accuracy: 0.7598, total loss: 36.67515566945076, average loss: 0.007335031133890152\n",
            "Epoch 233: accuracy: 0.9524888888888889, total loss: 56.987363919615746, average loss: 0.00126638586488035\n",
            "Validation: accuracy: 0.7592, total loss: 37.04651294648647, average loss: 0.0074093025892972945\n",
            "Epoch 234: accuracy: 0.9549333333333333, total loss: 54.828212812542915, average loss: 0.0012184047291676204\n",
            "Validation: accuracy: 0.7598, total loss: 37.128369599580765, average loss: 0.007425673919916153\n",
            "Epoch 235: accuracy: 0.9558888888888889, total loss: 54.13907906413078, average loss: 0.0012030906458695729\n",
            "Validation: accuracy: 0.7622, total loss: 36.91364370286465, average loss: 0.007382728740572929\n",
            "Epoch 236: accuracy: 0.9590888888888889, total loss: 50.939455680549145, average loss: 0.0011319879040122033\n",
            "Validation: accuracy: 0.7608, total loss: 37.26536726951599, average loss: 0.007453073453903198\n",
            "Epoch 237: accuracy: 0.9596222222222223, total loss: 49.5032513551414, average loss: 0.0011000722523364756\n",
            "Validation: accuracy: 0.7584, total loss: 37.036452278494835, average loss: 0.007407290455698967\n",
            "Epoch 238: accuracy: 0.9603777777777778, total loss: 48.49536767229438, average loss: 0.0010776748371620973\n",
            "Validation: accuracy: 0.7636, total loss: 37.28609263896942, average loss: 0.007457218527793884\n",
            "Epoch 239: accuracy: 0.9616, total loss: 47.848117649555206, average loss: 0.001063291503323449\n",
            "Validation: accuracy: 0.76, total loss: 37.0363706946373, average loss: 0.00740727413892746\n",
            "Epoch 240: accuracy: 0.9623111111111111, total loss: 47.34926953539252, average loss: 0.0010522059896753895\n",
            "Validation: accuracy: 0.7592, total loss: 37.394123047590256, average loss: 0.007478824609518051\n",
            "Checkpoint saved as `/weights/240.pth`\n",
            "Epoch 241: accuracy: 0.9611111111111111, total loss: 46.10657470300794, average loss: 0.001024590548955732\n",
            "Validation: accuracy: 0.7618, total loss: 37.467943876981735, average loss: 0.007493588775396347\n",
            "Epoch 242: accuracy: 0.9620666666666666, total loss: 45.80451104789972, average loss: 0.0010178780232866604\n",
            "Validation: accuracy: 0.7622, total loss: 37.38562276959419, average loss: 0.007477124553918839\n",
            "Epoch 243: accuracy: 0.9649333333333333, total loss: 43.60297177359462, average loss: 0.0009689549283021026\n",
            "Validation: accuracy: 0.7612, total loss: 37.52650445699692, average loss: 0.0075053008913993835\n",
            "Epoch 244: accuracy: 0.9642444444444445, total loss: 43.93727292865515, average loss: 0.0009763838428590032\n",
            "Validation: accuracy: 0.7624, total loss: 37.215680703520775, average loss: 0.007443136140704155\n",
            "Epoch 245: accuracy: 0.966, total loss: 42.83903123438358, average loss: 0.0009519784718751907\n",
            "Validation: accuracy: 0.7594, total loss: 37.48586179316044, average loss: 0.007497172358632087\n",
            "Epoch 246: accuracy: 0.9668666666666667, total loss: 41.48241537436843, average loss: 0.0009218314527637429\n",
            "Validation: accuracy: 0.7602, total loss: 37.46631273627281, average loss: 0.0074932625472545625\n",
            "Epoch 247: accuracy: 0.9669333333333333, total loss: 41.21976291015744, average loss: 0.0009159947313368321\n",
            "Validation: accuracy: 0.7652, total loss: 37.61036144196987, average loss: 0.007522072288393974\n",
            "Epoch 248: accuracy: 0.9691777777777778, total loss: 40.602851297706366, average loss: 0.0009022855843934748\n",
            "Validation: accuracy: 0.7624, total loss: 37.65672332048416, average loss: 0.007531344664096832\n",
            "Epoch 249: accuracy: 0.9682222222222222, total loss: 39.64229464530945, average loss: 0.0008809398810068767\n",
            "Validation: accuracy: 0.761, total loss: 37.48917476832867, average loss: 0.007497834953665733\n",
            "Epoch 250: accuracy: 0.9691555555555555, total loss: 38.90996315330267, average loss: 0.0008646658478511705\n",
            "Validation: accuracy: 0.7626, total loss: 37.635619789361954, average loss: 0.007527123957872391\n",
            "Epoch 251: accuracy: 0.9684666666666667, total loss: 39.417182590812445, average loss: 0.0008759373909069432\n",
            "Validation: accuracy: 0.7642, total loss: 37.71119776368141, average loss: 0.007542239552736282\n",
            "Epoch 252: accuracy: 0.9688888888888889, total loss: 38.76958531141281, average loss: 0.0008615463402536181\n",
            "Validation: accuracy: 0.7624, total loss: 37.781867787241936, average loss: 0.007556373557448387\n",
            "Epoch 253: accuracy: 0.9713333333333334, total loss: 36.728638123720884, average loss: 0.0008161919583049086\n",
            "Validation: accuracy: 0.767, total loss: 37.4960405677557, average loss: 0.00749920811355114\n",
            "Epoch 254: accuracy: 0.9716, total loss: 36.74261749908328, average loss: 0.0008165026110907395\n",
            "Validation: accuracy: 0.764, total loss: 37.52125737071037, average loss: 0.0075042514741420745\n",
            "Epoch 255: accuracy: 0.9706222222222223, total loss: 36.98535047471523, average loss: 0.0008218966772158941\n",
            "Validation: accuracy: 0.7638, total loss: 37.681512638926506, average loss: 0.0075363025277853015\n",
            "Epoch 256: accuracy: 0.9716444444444444, total loss: 35.510595962405205, average loss: 0.0007891243547201157\n",
            "Validation: accuracy: 0.7636, total loss: 37.89108994603157, average loss: 0.007578217989206314\n",
            "Epoch 257: accuracy: 0.9727555555555556, total loss: 34.66033748537302, average loss: 0.0007702297218971782\n",
            "Validation: accuracy: 0.765, total loss: 37.93295216560364, average loss: 0.0075865904331207275\n",
            "Epoch 258: accuracy: 0.9733555555555555, total loss: 35.007370095700026, average loss: 0.0007779415576822228\n",
            "Validation: accuracy: 0.766, total loss: 38.06194508075714, average loss: 0.007612389016151428\n",
            "Epoch 259: accuracy: 0.9728888888888889, total loss: 34.610718332231045, average loss: 0.0007691270740495787\n",
            "Validation: accuracy: 0.767, total loss: 38.04256993532181, average loss: 0.007608513987064362\n",
            "Epoch 260: accuracy: 0.9732, total loss: 33.883950755000114, average loss: 0.000752976683444447\n",
            "Validation: accuracy: 0.7628, total loss: 38.17754065990448, average loss: 0.007635508131980896\n",
            "Epoch 261: accuracy: 0.974, total loss: 32.920090060681105, average loss: 0.0007315575569040245\n",
            "Validation: accuracy: 0.7662, total loss: 38.27833101153374, average loss: 0.007655666202306747\n",
            "Epoch 262: accuracy: 0.9746444444444444, total loss: 33.22006069496274, average loss: 0.000738223570999172\n",
            "Validation: accuracy: 0.768, total loss: 38.046665728092194, average loss: 0.0076093331456184386\n",
            "Epoch 263: accuracy: 0.9746444444444444, total loss: 32.87405972741544, average loss: 0.000730534660609232\n",
            "Validation: accuracy: 0.7642, total loss: 38.1742479801178, average loss: 0.0076348495960235595\n",
            "Epoch 264: accuracy: 0.9742, total loss: 33.261087795719504, average loss: 0.0007391352843493223\n",
            "Validation: accuracy: 0.7654, total loss: 38.56728604435921, average loss: 0.007713457208871842\n",
            "Epoch 265: accuracy: 0.9756888888888889, total loss: 31.118436062708497, average loss: 0.0006915208013935221\n",
            "Validation: accuracy: 0.7664, total loss: 38.07176664471626, average loss: 0.007614353328943253\n",
            "Epoch 266: accuracy: 0.9757333333333333, total loss: 31.0523423217237, average loss: 0.00069005205159386\n",
            "Validation: accuracy: 0.7664, total loss: 38.183677315711975, average loss: 0.007636735463142395\n",
            "Epoch 267: accuracy: 0.9751111111111112, total loss: 31.293371856212616, average loss: 0.0006954082634713914\n",
            "Validation: accuracy: 0.7664, total loss: 38.752470284700394, average loss: 0.007750494056940079\n",
            "Epoch 268: accuracy: 0.9754, total loss: 31.45500777475536, average loss: 0.0006990001727723413\n",
            "Validation: accuracy: 0.7646, total loss: 38.497126162052155, average loss: 0.0076994252324104305\n",
            "Epoch 269: accuracy: 0.9760444444444445, total loss: 30.790047334507108, average loss: 0.0006842232741001579\n",
            "Validation: accuracy: 0.7676, total loss: 38.386678248643875, average loss: 0.007677335649728775\n",
            "Epoch 270: accuracy: 0.9773111111111111, total loss: 29.626582372933626, average loss: 0.0006583684971763028\n",
            "Validation: accuracy: 0.7656, total loss: 38.49686127901077, average loss: 0.007699372255802155\n",
            "Epoch 271: accuracy: 0.9766444444444444, total loss: 29.882983319461346, average loss: 0.0006640662959880299\n",
            "Validation: accuracy: 0.7678, total loss: 38.300612419843674, average loss: 0.007660122483968735\n",
            "Epoch 272: accuracy: 0.9778222222222223, total loss: 29.32307979464531, average loss: 0.0006516239954365624\n",
            "Validation: accuracy: 0.7676, total loss: 38.403574615716934, average loss: 0.007680714923143387\n",
            "Epoch 273: accuracy: 0.977, total loss: 28.842824010178447, average loss: 0.0006409516446706322\n",
            "Validation: accuracy: 0.766, total loss: 38.89737066626549, average loss: 0.0077794741332530976\n",
            "Epoch 274: accuracy: 0.9783777777777778, total loss: 28.553993763402104, average loss: 0.000634533194742269\n",
            "Validation: accuracy: 0.7686, total loss: 38.51605433225632, average loss: 0.007703210866451263\n",
            "Epoch 275: accuracy: 0.9779333333333333, total loss: 29.275176480412483, average loss: 0.0006505594773424996\n",
            "Validation: accuracy: 0.771, total loss: 38.35876989364624, average loss: 0.007671753978729248\n",
            "Epoch 276: accuracy: 0.9782666666666666, total loss: 28.20777218043804, average loss: 0.000626839381787512\n",
            "Validation: accuracy: 0.766, total loss: 38.65702477097511, average loss: 0.007731404954195022\n",
            "Epoch 277: accuracy: 0.9774444444444444, total loss: 28.861969178542495, average loss: 0.0006413770928564999\n",
            "Validation: accuracy: 0.77, total loss: 38.62312608957291, average loss: 0.007724625217914581\n",
            "Epoch 278: accuracy: 0.9797777777777777, total loss: 27.024397956207395, average loss: 0.0006005421768046087\n",
            "Validation: accuracy: 0.772, total loss: 38.43414142727852, average loss: 0.007686828285455704\n",
            "Epoch 279: accuracy: 0.9794666666666667, total loss: 27.57682180777192, average loss: 0.0006128182623949316\n",
            "Validation: accuracy: 0.772, total loss: 38.54554386436939, average loss: 0.007709108772873879\n",
            "Epoch 280: accuracy: 0.9811111111111112, total loss: 25.99004198051989, average loss: 0.0005775564884559976\n",
            "Validation: accuracy: 0.7694, total loss: 38.62477907538414, average loss: 0.007724955815076828\n",
            "Checkpoint saved as `/weights/280.pth`\n",
            "Epoch 281: accuracy: 0.98, total loss: 27.34969907440245, average loss: 0.0006077710905422767\n",
            "Validation: accuracy: 0.7686, total loss: 38.5492891818285, average loss: 0.0077098578363656994\n",
            "Epoch 282: accuracy: 0.9809333333333333, total loss: 25.92784238420427, average loss: 0.0005761742752045393\n",
            "Validation: accuracy: 0.7668, total loss: 38.86039899289608, average loss: 0.007772079798579216\n",
            "Epoch 283: accuracy: 0.9807333333333333, total loss: 26.51968852058053, average loss: 0.0005893264115684562\n",
            "Validation: accuracy: 0.7696, total loss: 38.34762233495712, average loss: 0.007669524466991424\n",
            "Epoch 284: accuracy: 0.9798222222222223, total loss: 26.594330463558435, average loss: 0.0005909851214124097\n",
            "Validation: accuracy: 0.771, total loss: 38.41725638508797, average loss: 0.007683451277017594\n",
            "Epoch 285: accuracy: 0.9808888888888889, total loss: 26.143277367576957, average loss: 0.0005809617192794879\n",
            "Validation: accuracy: 0.7664, total loss: 38.671869575977325, average loss: 0.007734373915195465\n",
            "Epoch 286: accuracy: 0.9806, total loss: 25.32746567763388, average loss: 0.0005628325706140862\n",
            "Validation: accuracy: 0.7662, total loss: 39.03783133625984, average loss: 0.007807566267251968\n",
            "Epoch 287: accuracy: 0.9810666666666666, total loss: 25.52099948376417, average loss: 0.000567133321861426\n",
            "Validation: accuracy: 0.7684, total loss: 38.734384179115295, average loss: 0.007746876835823059\n",
            "Epoch 288: accuracy: 0.9801555555555556, total loss: 25.93144504725933, average loss: 0.0005762543343835407\n",
            "Validation: accuracy: 0.7672, total loss: 39.36274212598801, average loss: 0.007872548425197601\n",
            "Epoch 289: accuracy: 0.9818888888888889, total loss: 24.920193066820502, average loss: 0.0005537820681515667\n",
            "Validation: accuracy: 0.7686, total loss: 39.154487282037735, average loss: 0.007830897456407547\n",
            "Epoch 290: accuracy: 0.9827111111111111, total loss: 24.43976415321231, average loss: 0.0005431058700713846\n",
            "Validation: accuracy: 0.7702, total loss: 38.667673736810684, average loss: 0.007733534747362137\n",
            "Epoch 291: accuracy: 0.9813333333333333, total loss: 24.309630688279867, average loss: 0.0005402140152951082\n",
            "Validation: accuracy: 0.7676, total loss: 39.23556464910507, average loss: 0.007847112929821015\n",
            "Epoch 292: accuracy: 0.9815555555555555, total loss: 25.02708804421127, average loss: 0.0005561575120935837\n",
            "Validation: accuracy: 0.7686, total loss: 39.19660526514053, average loss: 0.007839321053028106\n",
            "Epoch 293: accuracy: 0.9816666666666667, total loss: 24.23305076919496, average loss: 0.0005385122393154435\n",
            "Validation: accuracy: 0.7696, total loss: 39.389747977256775, average loss: 0.007877949595451355\n",
            "Epoch 294: accuracy: 0.9826888888888888, total loss: 23.88948943093419, average loss: 0.0005308775429096487\n",
            "Validation: accuracy: 0.7636, total loss: 39.21152928471565, average loss: 0.00784230585694313\n",
            "Epoch 295: accuracy: 0.9825333333333334, total loss: 23.47090557962656, average loss: 0.0005215756795472569\n",
            "Validation: accuracy: 0.768, total loss: 39.38021773099899, average loss: 0.007876043546199798\n",
            "Epoch 296: accuracy: 0.9821333333333333, total loss: 23.80357207544148, average loss: 0.000528968268343144\n",
            "Validation: accuracy: 0.767, total loss: 39.402692437171936, average loss: 0.007880538487434387\n",
            "Epoch 297: accuracy: 0.9814666666666667, total loss: 23.984653066843748, average loss: 0.0005329922903743055\n",
            "Validation: accuracy: 0.768, total loss: 39.4805144071579, average loss: 0.007896102881431579\n",
            "Epoch 298: accuracy: 0.9822222222222222, total loss: 23.759143060073256, average loss: 0.0005279809568905169\n",
            "Validation: accuracy: 0.768, total loss: 39.286923348903656, average loss: 0.007857384669780731\n",
            "Epoch 299: accuracy: 0.9821111111111112, total loss: 24.41360822878778, average loss: 0.0005425246273063952\n",
            "Validation: accuracy: 0.7634, total loss: 39.66350221633911, average loss: 0.007932700443267823\n",
            "Epoch 300: accuracy: 0.9829555555555556, total loss: 23.28621363081038, average loss: 0.0005174714140180085\n",
            "Validation: accuracy: 0.7698, total loss: 39.22932717204094, average loss: 0.007845865434408189\n",
            "Epoch 301: accuracy: 0.9828444444444444, total loss: 23.53922545723617, average loss: 0.0005230938990496926\n",
            "Validation: accuracy: 0.7702, total loss: 39.40898188948631, average loss: 0.007881796377897262\n",
            "Epoch 302: accuracy: 0.9828, total loss: 22.941056236624718, average loss: 0.0005098012497027715\n",
            "Validation: accuracy: 0.7658, total loss: 39.749764025211334, average loss: 0.007949952805042267\n",
            "Epoch 303: accuracy: 0.9829333333333333, total loss: 22.997777520678937, average loss: 0.0005110617226817541\n",
            "Validation: accuracy: 0.767, total loss: 39.33686502277851, average loss: 0.007867373004555702\n",
            "Epoch 304: accuracy: 0.9830222222222222, total loss: 22.896884124726057, average loss: 0.0005088196472161346\n",
            "Validation: accuracy: 0.7648, total loss: 39.647547483444214, average loss: 0.007929509496688842\n",
            "Epoch 305: accuracy: 0.9843555555555555, total loss: 21.69273974187672, average loss: 0.00048206088315281604\n",
            "Validation: accuracy: 0.7632, total loss: 39.542814150452614, average loss: 0.007908562830090523\n",
            "Epoch 306: accuracy: 0.9852, total loss: 21.62791113369167, average loss: 0.00048062024741537043\n",
            "Validation: accuracy: 0.7666, total loss: 39.48018801212311, average loss: 0.007896037602424622\n",
            "Epoch 307: accuracy: 0.9832666666666666, total loss: 22.229497157037258, average loss: 0.000493988825711939\n",
            "Validation: accuracy: 0.7646, total loss: 40.26461559534073, average loss: 0.008052923119068146\n",
            "Epoch 308: accuracy: 0.9836888888888888, total loss: 22.32757326029241, average loss: 0.0004961682946731647\n",
            "Validation: accuracy: 0.7658, total loss: 39.46042586863041, average loss: 0.007892085173726082\n",
            "Epoch 309: accuracy: 0.9839333333333333, total loss: 22.130229892209172, average loss: 0.0004917828864935372\n",
            "Validation: accuracy: 0.7666, total loss: 39.457235381007195, average loss: 0.007891447076201439\n",
            "Epoch 310: accuracy: 0.9838888888888889, total loss: 21.935662804171443, average loss: 0.0004874591734260321\n",
            "Validation: accuracy: 0.7694, total loss: 39.76468800008297, average loss: 0.007952937600016594\n",
            "Epoch 311: accuracy: 0.9841111111111112, total loss: 21.45394132193178, average loss: 0.000476754251598484\n",
            "Validation: accuracy: 0.7706, total loss: 39.44703868031502, average loss: 0.007889407736063003\n",
            "Epoch 312: accuracy: 0.9842888888888889, total loss: 20.995351189747453, average loss: 0.0004665633597721656\n",
            "Validation: accuracy: 0.7662, total loss: 39.61059358716011, average loss: 0.007922118717432022\n",
            "Epoch 313: accuracy: 0.9850222222222222, total loss: 21.54589436762035, average loss: 0.00047879765261378554\n",
            "Validation: accuracy: 0.7658, total loss: 40.041399627923965, average loss: 0.008008279925584793\n",
            "Epoch 314: accuracy: 0.9853111111111111, total loss: 20.933211689814925, average loss: 0.00046518248199588723\n",
            "Validation: accuracy: 0.7676, total loss: 40.116032749414444, average loss: 0.008023206549882889\n",
            "Epoch 315: accuracy: 0.9847555555555556, total loss: 20.918820065446198, average loss: 0.0004648626681210266\n",
            "Validation: accuracy: 0.7672, total loss: 39.88850739598274, average loss: 0.007977701479196549\n",
            "Epoch 316: accuracy: 0.9846, total loss: 21.9106537848711, average loss: 0.00048690341744158\n",
            "Validation: accuracy: 0.7686, total loss: 39.83583027124405, average loss: 0.00796716605424881\n",
            "Epoch 317: accuracy: 0.9843555555555555, total loss: 21.578747514635324, average loss: 0.00047952772254745166\n",
            "Validation: accuracy: 0.7626, total loss: 40.19133847951889, average loss: 0.008038267695903777\n",
            "Epoch 318: accuracy: 0.9860444444444444, total loss: 19.860838589258492, average loss: 0.0004413519686501887\n",
            "Validation: accuracy: 0.7648, total loss: 40.462617099285126, average loss: 0.008092523419857025\n",
            "Epoch 319: accuracy: 0.9861777777777778, total loss: 20.33612203411758, average loss: 0.00045191382298039067\n",
            "Validation: accuracy: 0.7674, total loss: 40.31274351477623, average loss: 0.008062548702955245\n",
            "Epoch 320: accuracy: 0.9865333333333334, total loss: 19.39358857832849, average loss: 0.00043096863507396645\n",
            "Validation: accuracy: 0.7664, total loss: 39.888471841812134, average loss: 0.007977694368362426\n",
            "Checkpoint saved as `/weights/320.pth`\n",
            "Epoch 321: accuracy: 0.9856888888888888, total loss: 20.35057293623686, average loss: 0.00045223495413859683\n",
            "Validation: accuracy: 0.7652, total loss: 40.26437444984913, average loss: 0.008052874889969825\n",
            "Epoch 322: accuracy: 0.9848666666666667, total loss: 21.182098688557744, average loss: 0.00047071330419017207\n",
            "Validation: accuracy: 0.7676, total loss: 39.78148838877678, average loss: 0.007956297677755356\n",
            "Epoch 323: accuracy: 0.9853333333333333, total loss: 20.368214080110192, average loss: 0.0004526269795580043\n",
            "Validation: accuracy: 0.769, total loss: 40.04129379987717, average loss: 0.008008258759975434\n",
            "Epoch 324: accuracy: 0.9855777777777778, total loss: 20.08584190439433, average loss: 0.00044635204231987396\n",
            "Validation: accuracy: 0.7708, total loss: 39.65669921785593, average loss: 0.007931339843571186\n",
            "Epoch 325: accuracy: 0.9854444444444445, total loss: 20.27398836426437, average loss: 0.0004505330747614304\n",
            "Validation: accuracy: 0.7684, total loss: 40.108655244112015, average loss: 0.008021731048822403\n",
            "Epoch 326: accuracy: 0.9857555555555556, total loss: 20.233171140775084, average loss: 0.00044962602535055744\n",
            "Validation: accuracy: 0.7638, total loss: 40.278965294361115, average loss: 0.008055793058872223\n",
            "Epoch 327: accuracy: 0.9862, total loss: 19.171777044422925, average loss: 0.00042603948987606497\n",
            "Validation: accuracy: 0.7656, total loss: 40.48108443617821, average loss: 0.00809621688723564\n",
            "Epoch 328: accuracy: 0.9859111111111111, total loss: 20.221657114103436, average loss: 0.0004493701580911875\n",
            "Validation: accuracy: 0.7642, total loss: 40.90807294845581, average loss: 0.008181614589691162\n",
            "Epoch 329: accuracy: 0.9859555555555556, total loss: 19.823596733622253, average loss: 0.0004405243718582723\n",
            "Validation: accuracy: 0.7684, total loss: 40.47840139269829, average loss: 0.008095680278539658\n",
            "Epoch 330: accuracy: 0.9865777777777778, total loss: 19.112813347019255, average loss: 0.00042472918548931677\n",
            "Validation: accuracy: 0.7676, total loss: 40.86237323284149, average loss: 0.008172474646568298\n",
            "Epoch 331: accuracy: 0.9875555555555555, total loss: 18.739490089938045, average loss: 0.0004164331131097343\n",
            "Validation: accuracy: 0.7674, total loss: 40.3437939286232, average loss: 0.00806875878572464\n",
            "Epoch 332: accuracy: 0.9857777777777778, total loss: 19.832944387570024, average loss: 0.0004407320975015561\n",
            "Validation: accuracy: 0.7684, total loss: 39.8991294503212, average loss: 0.007979825890064239\n",
            "Epoch 333: accuracy: 0.9864666666666667, total loss: 19.628830088302493, average loss: 0.00043619622418449987\n",
            "Validation: accuracy: 0.7632, total loss: 39.77258017659187, average loss: 0.007954516035318375\n",
            "Epoch 334: accuracy: 0.9856222222222222, total loss: 19.710923944599926, average loss: 0.0004380205321022206\n",
            "Validation: accuracy: 0.7652, total loss: 40.449380457401276, average loss: 0.008089876091480255\n",
            "Epoch 335: accuracy: 0.9881333333333333, total loss: 18.449980840086937, average loss: 0.00040999957422415415\n",
            "Validation: accuracy: 0.7626, total loss: 40.458493649959564, average loss: 0.008091698729991912\n",
            "Epoch 336: accuracy: 0.9867777777777778, total loss: 18.70648338086903, average loss: 0.0004156996306859785\n",
            "Validation: accuracy: 0.7644, total loss: 40.52088788151741, average loss: 0.008104177576303481\n",
            "Epoch 337: accuracy: 0.9874, total loss: 18.425922548398376, average loss: 0.0004094649455199639\n",
            "Validation: accuracy: 0.762, total loss: 40.277276054024696, average loss: 0.008055455210804939\n",
            "Epoch 338: accuracy: 0.9872222222222222, total loss: 19.158299507573247, average loss: 0.0004257399890571833\n",
            "Validation: accuracy: 0.7634, total loss: 40.58884906768799, average loss: 0.008117769813537597\n",
            "Epoch 339: accuracy: 0.9866, total loss: 18.942034538835287, average loss: 0.0004209341008630064\n",
            "Validation: accuracy: 0.764, total loss: 40.52217283844948, average loss: 0.008104434567689896\n",
            "Epoch 340: accuracy: 0.9870444444444444, total loss: 18.509262651205063, average loss: 0.00041131694780455696\n",
            "Validation: accuracy: 0.7674, total loss: 39.85186389088631, average loss: 0.007970372778177262\n",
            "Epoch 341: accuracy: 0.9873111111111111, total loss: 18.424262261018157, average loss: 0.0004094280502448479\n",
            "Validation: accuracy: 0.7664, total loss: 39.95227111876011, average loss: 0.007990454223752021\n",
            "Epoch 342: accuracy: 0.9867111111111111, total loss: 18.866790695115924, average loss: 0.0004192620154470205\n",
            "Validation: accuracy: 0.763, total loss: 40.24544022977352, average loss: 0.008049088045954705\n",
            "Epoch 343: accuracy: 0.9873777777777778, total loss: 18.551419461145997, average loss: 0.0004122537658032444\n",
            "Validation: accuracy: 0.7622, total loss: 40.44704467058182, average loss: 0.008089408934116363\n",
            "Epoch 344: accuracy: 0.9881555555555556, total loss: 17.57683499995619, average loss: 0.0003905963333323598\n",
            "Validation: accuracy: 0.7648, total loss: 40.13556718826294, average loss: 0.008027113437652588\n",
            "Epoch 345: accuracy: 0.9878666666666667, total loss: 17.67724669445306, average loss: 0.0003928277043211791\n",
            "Validation: accuracy: 0.765, total loss: 40.52761037647724, average loss: 0.008105522075295449\n",
            "Epoch 346: accuracy: 0.9883555555555555, total loss: 17.43932588212192, average loss: 0.0003875405751582649\n",
            "Validation: accuracy: 0.7674, total loss: 40.52575731277466, average loss: 0.008105151462554932\n",
            "Epoch 347: accuracy: 0.9880888888888889, total loss: 17.67661593668163, average loss: 0.000392813687481814\n",
            "Validation: accuracy: 0.7638, total loss: 40.3116190135479, average loss: 0.008062323802709579\n",
            "Epoch 348: accuracy: 0.9873111111111111, total loss: 17.727334497496486, average loss: 0.000393940766611033\n",
            "Validation: accuracy: 0.7624, total loss: 40.64272165298462, average loss: 0.008128544330596925\n",
            "Epoch 349: accuracy: 0.9884444444444445, total loss: 17.663842330686748, average loss: 0.00039252982957081663\n",
            "Validation: accuracy: 0.7652, total loss: 40.655384331941605, average loss: 0.008131076866388321\n",
            "Epoch 350: accuracy: 0.9870666666666666, total loss: 18.253082166425884, average loss: 0.0004056240481427974\n",
            "Validation: accuracy: 0.7652, total loss: 40.47508695721626, average loss: 0.008095017391443252\n",
            "Epoch 351: accuracy: 0.9875777777777778, total loss: 17.39638900756836, average loss: 0.000386586422390408\n",
            "Validation: accuracy: 0.7648, total loss: 40.65497300028801, average loss: 0.008130994600057602\n",
            "Epoch 352: accuracy: 0.9873555555555555, total loss: 17.867938461713493, average loss: 0.0003970652991491887\n",
            "Validation: accuracy: 0.7662, total loss: 40.22575807571411, average loss: 0.008045151615142822\n",
            "Epoch 353: accuracy: 0.9875555555555555, total loss: 17.951986256055534, average loss: 0.0003989330279123452\n",
            "Validation: accuracy: 0.7658, total loss: 40.15375059843063, average loss: 0.008030750119686127\n",
            "Epoch 354: accuracy: 0.9881777777777778, total loss: 17.241611207835376, average loss: 0.00038314691572967504\n",
            "Validation: accuracy: 0.7628, total loss: 40.34954807162285, average loss: 0.00806990961432457\n",
            "Epoch 355: accuracy: 0.9871111111111112, total loss: 17.6477554384619, average loss: 0.0003921723430769311\n",
            "Validation: accuracy: 0.7642, total loss: 40.674726098775864, average loss: 0.008134945219755173\n",
            "Epoch 356: accuracy: 0.9877333333333334, total loss: 18.05574239511043, average loss: 0.0004012387198913429\n",
            "Validation: accuracy: 0.7656, total loss: 40.79566818475723, average loss: 0.008159133636951447\n",
            "Epoch 357: accuracy: 0.9877111111111111, total loss: 17.72246015071869, average loss: 0.00039383244779374863\n",
            "Validation: accuracy: 0.7642, total loss: 40.604556888341904, average loss: 0.00812091137766838\n",
            "Epoch 358: accuracy: 0.9878, total loss: 17.731097012758255, average loss: 0.00039402437806129454\n",
            "Validation: accuracy: 0.7612, total loss: 40.22648727893829, average loss: 0.00804529745578766\n",
            "Epoch 359: accuracy: 0.9873111111111111, total loss: 18.025176635943353, average loss: 0.0004005594807987412\n",
            "Validation: accuracy: 0.7586, total loss: 40.86733332276344, average loss: 0.008173466664552688\n",
            "Epoch 360: accuracy: 0.9884, total loss: 17.10683453269303, average loss: 0.0003801518785042895\n",
            "Validation: accuracy: 0.7606, total loss: 40.775925666093826, average loss: 0.008155185133218765\n",
            "Checkpoint saved as `/weights/360.pth`\n",
            "Epoch 361: accuracy: 0.9888888888888889, total loss: 16.92391525860876, average loss: 0.0003760870057468613\n",
            "Validation: accuracy: 0.7598, total loss: 40.91589477658272, average loss: 0.008183178955316544\n",
            "Epoch 362: accuracy: 0.9880444444444444, total loss: 17.339146940037608, average loss: 0.00038531437644528016\n",
            "Validation: accuracy: 0.7602, total loss: 40.453759998083115, average loss: 0.008090751999616622\n",
            "Epoch 363: accuracy: 0.9885333333333334, total loss: 17.242723969742656, average loss: 0.000383171643772059\n",
            "Validation: accuracy: 0.7618, total loss: 40.89388135075569, average loss: 0.008178776270151138\n",
            "Epoch 364: accuracy: 0.989, total loss: 16.903379559516907, average loss: 0.0003756306568781535\n",
            "Validation: accuracy: 0.7622, total loss: 40.91870981454849, average loss: 0.008183741962909698\n",
            "Epoch 365: accuracy: 0.9882666666666666, total loss: 17.233089602552354, average loss: 0.00038295754672338565\n",
            "Validation: accuracy: 0.7626, total loss: 40.6984837949276, average loss: 0.008139696758985519\n",
            "Epoch 366: accuracy: 0.9883555555555555, total loss: 17.32682293653488, average loss: 0.00038504050970077514\n",
            "Validation: accuracy: 0.7636, total loss: 40.747291803359985, average loss: 0.008149458360671998\n",
            "Epoch 367: accuracy: 0.9883111111111111, total loss: 17.276713164523244, average loss: 0.00038392695921162764\n",
            "Validation: accuracy: 0.7592, total loss: 41.061475813388824, average loss: 0.008212295162677765\n",
            "Epoch 368: accuracy: 0.9881777777777778, total loss: 17.42208303976804, average loss: 0.0003871574008837342\n",
            "Validation: accuracy: 0.7592, total loss: 41.321196019649506, average loss: 0.0082642392039299\n",
            "Epoch 369: accuracy: 0.9886666666666667, total loss: 16.55005245935172, average loss: 0.0003677789435411493\n",
            "Validation: accuracy: 0.7614, total loss: 41.259615033864975, average loss: 0.008251923006772995\n",
            "Epoch 370: accuracy: 0.9892444444444445, total loss: 16.28507803287357, average loss: 0.000361890622952746\n",
            "Validation: accuracy: 0.7604, total loss: 41.40226410329342, average loss: 0.008280452820658684\n",
            "Epoch 371: accuracy: 0.9892222222222222, total loss: 16.890115628018975, average loss: 0.0003753359028448661\n",
            "Validation: accuracy: 0.7612, total loss: 41.398016303777695, average loss: 0.008279603260755539\n",
            "Epoch 372: accuracy: 0.9886444444444444, total loss: 17.269091446883976, average loss: 0.0003837575877085328\n",
            "Validation: accuracy: 0.7608, total loss: 40.95722395181656, average loss: 0.008191444790363312\n",
            "Epoch 373: accuracy: 0.9892, total loss: 16.93543951306492, average loss: 0.00037634310029033155\n",
            "Validation: accuracy: 0.7604, total loss: 40.61847764253616, average loss: 0.008123695528507233\n",
            "Epoch 374: accuracy: 0.9893777777777778, total loss: 16.05894358921796, average loss: 0.00035686541309373244\n",
            "Validation: accuracy: 0.7586, total loss: 40.93289169669151, average loss: 0.008186578339338302\n",
            "Epoch 375: accuracy: 0.9880888888888889, total loss: 16.685058197937906, average loss: 0.0003707790710652868\n",
            "Validation: accuracy: 0.7654, total loss: 40.74176989495754, average loss: 0.008148353978991508\n",
            "Epoch 376: accuracy: 0.9879555555555556, total loss: 17.520664491690695, average loss: 0.0003893480998153488\n",
            "Validation: accuracy: 0.7658, total loss: 41.00688216090202, average loss: 0.008201376432180404\n",
            "Epoch 377: accuracy: 0.9886, total loss: 16.15351009555161, average loss: 0.000358966891012258\n",
            "Validation: accuracy: 0.7614, total loss: 41.07181839644909, average loss: 0.008214363679289818\n",
            "Epoch 378: accuracy: 0.9898, total loss: 15.875648595392704, average loss: 0.00035279219100872673\n",
            "Validation: accuracy: 0.7628, total loss: 41.13240960240364, average loss: 0.008226481920480729\n",
            "Epoch 379: accuracy: 0.9896222222222222, total loss: 16.241228049620986, average loss: 0.00036091617888046635\n",
            "Validation: accuracy: 0.7634, total loss: 40.76903784275055, average loss: 0.00815380756855011\n",
            "Epoch 380: accuracy: 0.9894, total loss: 16.06716882903129, average loss: 0.0003570481962006953\n",
            "Validation: accuracy: 0.764, total loss: 40.67795795202255, average loss: 0.008135591590404511\n",
            "Epoch 381: accuracy: 0.9886666666666667, total loss: 16.811357735656202, average loss: 0.00037358572745902673\n",
            "Validation: accuracy: 0.759, total loss: 41.342437490820885, average loss: 0.008268487498164176\n",
            "Epoch 382: accuracy: 0.9894, total loss: 16.406116999685764, average loss: 0.00036458037777079477\n",
            "Validation: accuracy: 0.763, total loss: 40.89982287585735, average loss: 0.00817996457517147\n",
            "Epoch 383: accuracy: 0.9901333333333333, total loss: 15.701132141985, average loss: 0.00034891404759966664\n",
            "Validation: accuracy: 0.763, total loss: 40.63429096341133, average loss: 0.008126858192682267\n",
            "Epoch 384: accuracy: 0.9888888888888889, total loss: 16.37149873562157, average loss: 0.0003638110830138127\n",
            "Validation: accuracy: 0.7628, total loss: 41.153748750686646, average loss: 0.00823074975013733\n",
            "Epoch 385: accuracy: 0.9908444444444444, total loss: 14.836325037293136, average loss: 0.00032969611193984746\n",
            "Validation: accuracy: 0.7636, total loss: 41.15160682797432, average loss: 0.008230321365594864\n",
            "Epoch 386: accuracy: 0.9895555555555555, total loss: 15.962627658620477, average loss: 0.00035472505908045506\n",
            "Validation: accuracy: 0.7638, total loss: 40.945028215646744, average loss: 0.00818900564312935\n",
            "Epoch 387: accuracy: 0.9892444444444445, total loss: 15.665274899452925, average loss: 0.0003481172199878428\n",
            "Validation: accuracy: 0.7626, total loss: 41.005849719047546, average loss: 0.00820116994380951\n",
            "Epoch 388: accuracy: 0.9890444444444444, total loss: 16.288439691066742, average loss: 0.00036196532646814983\n",
            "Validation: accuracy: 0.7604, total loss: 41.107568338513374, average loss: 0.008221513667702674\n",
            "Epoch 389: accuracy: 0.9896222222222222, total loss: 16.52005246002227, average loss: 0.00036711227688938377\n",
            "Validation: accuracy: 0.7624, total loss: 40.84320855140686, average loss: 0.008168641710281373\n",
            "Epoch 390: accuracy: 0.9892888888888889, total loss: 16.05683560948819, average loss: 0.00035681856909973756\n",
            "Validation: accuracy: 0.7614, total loss: 41.208021476864815, average loss: 0.008241604295372964\n",
            "Epoch 391: accuracy: 0.9892222222222222, total loss: 16.46407719887793, average loss: 0.00036586838219728737\n",
            "Validation: accuracy: 0.7632, total loss: 40.905913919210434, average loss: 0.008181182783842087\n",
            "Epoch 392: accuracy: 0.9888888888888889, total loss: 16.039899146184325, average loss: 0.00035644220324854056\n",
            "Validation: accuracy: 0.7652, total loss: 40.91001132130623, average loss: 0.008182002264261246\n",
            "Epoch 393: accuracy: 0.9894666666666667, total loss: 15.853197545744479, average loss: 0.00035229327879432177\n",
            "Validation: accuracy: 0.7648, total loss: 40.707062751054764, average loss: 0.008141412550210952\n",
            "Epoch 394: accuracy: 0.9891333333333333, total loss: 15.968682027421892, average loss: 0.0003548596006093754\n",
            "Validation: accuracy: 0.7662, total loss: 40.394105821847916, average loss: 0.008078821164369584\n",
            "Epoch 395: accuracy: 0.9904444444444445, total loss: 15.428804316557944, average loss: 0.0003428623181457321\n",
            "Validation: accuracy: 0.765, total loss: 40.84354645013809, average loss: 0.008168709290027618\n",
            "Epoch 396: accuracy: 0.9896888888888888, total loss: 15.70597545336932, average loss: 0.00034902167674154044\n",
            "Validation: accuracy: 0.7646, total loss: 41.10105186700821, average loss: 0.008220210373401642\n",
            "Epoch 397: accuracy: 0.9899111111111111, total loss: 15.582899624481797, average loss: 0.0003462866583218177\n",
            "Validation: accuracy: 0.7646, total loss: 40.99214754998684, average loss: 0.008198429509997367\n",
            "Epoch 398: accuracy: 0.9900888888888889, total loss: 15.08391725923866, average loss: 0.0003351981613164147\n",
            "Validation: accuracy: 0.7636, total loss: 40.72015184164047, average loss: 0.008144030368328094\n",
            "Epoch 399: accuracy: 0.9908444444444444, total loss: 14.641006915830076, average loss: 0.00032535570924066836\n",
            "Validation: accuracy: 0.7632, total loss: 40.9432258605957, average loss: 0.008188645172119141\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################################\n",
        "# 5. Try the network on test data, and create .csv file\n",
        "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "########################################################################\n",
        "#0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010\n",
        "test_transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "    transforms.Normalize((0.49139968, 0.48215827, 0.44653124), (0.24703233, 0.24348505, 0.26158768)),])\n",
        "\n",
        "\n",
        "testset = CIFAR100_SFU_CV(root=PATH_TO_CIFAR100_SFU_CV, fold=\"test\",\n",
        "                                       download=True, transform=test_transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "print(\"Test set size: \"+str(len(testset)))\n",
        "\n",
        "# Check out why .eval() is important!\n",
        "# https://discuss.pytorch.org/t/model-train-and-model-eval-vs-model-and-model-eval/5744/2\n",
        "\n",
        "checkpoint_file='./weight/280.pth'\n",
        "net.load_state_dict(torch.load(checkpoint_filename))\n",
        "net.eval()\n",
        "\n",
        "total = 0\n",
        "predictions = []\n",
        "for data in testloader:\n",
        "    images, labels = data\n",
        "\n",
        "    # For training on GPU, we need to transfer net and data onto the GPU\n",
        "    # http://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#training-on-gpu\n",
        "    if IS_GPU:\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "    \n",
        "    outputs = net(Variable(images))\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    predictions.extend(list(predicted.cpu().numpy()))\n",
        "    total += labels.size(0)\n",
        "\n",
        "with open('submission_netid.csv', 'w') as csvfile:\n",
        "    wr = csv.writer(csvfile, quoting=csv.QUOTE_ALL)\n",
        "    wr.writerow([\"Id\", \"Prediction1\"])\n",
        "    for l_i, label in enumerate(predictions):\n",
        "        wr.writerow([str(l_i), str(label)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tx0jcCZBTCxq",
        "outputId": "b6af12f9-21fd-459d-ebf7-36d3d1d1b766"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set size: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAZjIcLOdp-W"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1GE8t3mRdy9"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og2F2MLhs7L6"
      },
      "source": [
        "# Part 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prD0eXGpdoCR"
      },
      "source": [
        "\"\"\"Headers\"\"\"\n",
        "import os\n",
        "import os.path as osp\n",
        "import time\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchvision import datasets"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6CJz7OM0J9Z"
      },
      "source": [
        "# Pre-Trained Model\n",
        "\n",
        "TODO1. Load pretrained resnet model. Experiment with different models. \n",
        "\n",
        "TODO2: Replace last fc layer\n",
        "\n",
        "TODO3. Forward pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUdo6AkH0maX"
      },
      "source": [
        "class PreTrainedResNet(nn.Module):\n",
        "  def __init__(self, num_classes, feature_extracting):\n",
        "    super(PreTrainedResNet, self).__init__()\n",
        "    \n",
        "    #TODO1: Load pre-trained ResNet Model\n",
        "    self.resnet18 = models.resnet18(pretrained=True)\n",
        "\n",
        "    #Set gradients to false\n",
        "    if feature_extracting:\n",
        "      for param in self.resnet18.parameters():\n",
        "          param.requires_grad = False\n",
        "    \n",
        "    #Replace last fc layer\n",
        "    num_feats = self.resnet18.fc.in_features\n",
        "    self.resnet18.fc = nn.Linear(num_feats, num_classes)\n",
        "    #TODO2: Replace fc layer in resnet to a linear layer of size (num_feats, num_classes)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    #TODO3: Forward pass x through the model\n",
        "    x = self.resnet18(x)\n",
        "    return x\n",
        "\n",
        "  # def set_train_last_only(self, new_val: bool):\n",
        "  #     for param in self.resnet18.parameters():\n",
        "  #         if new_val:\n",
        "  #             param.requires_grad = False\n",
        "  #         else:\n",
        "  #             param.requires_grad = True\n"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_DRbNt8Jask"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujUNEVsEvWwv"
      },
      "source": [
        "def train(model, optimizer, criterion, epoch, num_epochs):\n",
        "  model.train()\n",
        "  epoch_loss = 0.0\n",
        "  epoch_acc = 0.0\n",
        "  \n",
        "  for batch_idx, (images, labels) in enumerate(dataloaders['train']):\n",
        "    #zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    #move to GPU\n",
        "    images, labels = images.cuda(), labels.cuda()\n",
        "    \n",
        "    #forward\n",
        "    outputs = model.forward(images)\n",
        "    \n",
        "    loss = criterion(outputs, labels)\n",
        "    \n",
        "    _, preds = torch.max(outputs.data, 1)\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    epoch_loss += loss.item()\n",
        "    epoch_acc += torch.sum(preds == labels).item()\n",
        "    \n",
        "  epoch_loss /= dataset_sizes['train']\n",
        "  epoch_acc /= dataset_sizes['train']\n",
        "  \n",
        "  print('TRAINING Epoch %d/%d Loss %.4f Accuracy %.4f' % (epoch, num_epochs, epoch_loss, epoch_acc))"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAbNgE4r7vm-"
      },
      "source": [
        "# Main\n",
        "\n",
        "1. Vary hyperparams\n",
        "2. Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZkI3scVWjOQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b36cd5aab8944947915b99bdfcee9905",
            "9821cd2b1d78410bac0ae1ce44b5c4a7",
            "fc2d24c89ef74614822b44788deeb5af",
            "072e10750a154bad93a12ae1033e4bff",
            "a52c328960a14fb38efcea323309fa9d",
            "a2bb853dabcb434da3db902525da4ac8",
            "83b563085c6f4b37a5b96173665ee61a",
            "49fe3fc41ac349369b1718391045b834",
            "770febf48afb48acbe4cc464ca720d73",
            "40b7c5cd86e6432cb8f73bb4645f1e1a",
            "499a586312074a14a69213d04b861295"
          ]
        },
        "outputId": "47e91964-9a5a-4250-f7bd-fdff7e0d2541"
      },
      "source": [
        "#TODO: Vary Hyperparams\n",
        "\n",
        "NUM_EPOCHS = 80\n",
        "LEARNING_RATE = 0.0004 \n",
        "BATCH_SIZE = 32\n",
        "RESNET_LAST_ONLY = False #Fine tunes only the last layer. Set to False to fine tune entire network\n",
        "\n",
        "root_path = '/data/' #If your data is in a different folder, set the path accodordingly\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        # transforms.CenterCrop(224),\n",
        "\n",
        "        #TODO: Transforms.RandomResizedCrop() instead of CenterCrop(), RandomRoate() and Horizontal Flip()\n",
        "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0), ratio=(4/5, 6/5)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        #TODO: Transforms.Normalize()\n",
        "        transforms.Normalize((0.49139968, 0.48215827, 0.44653124), (0.24703233, 0.24348505, 0.26158768)),\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        #TODO: Transforms.Normalize()\n",
        "        transforms.Normalize((0.49139968, 0.48215827, 0.44653124), (0.24703233, 0.24348505, 0.26158768)),\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# loading datasets with PyTorch ImageFolder\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(root_path, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'test']}\n",
        "\n",
        "# defining data loaders to load data using image_datasets and transforms, here we also specify batch size for the mini batch\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCH_SIZE,\n",
        "                                             shuffle=True, num_workers=4)\n",
        "              for x in ['train', 'test']}\n",
        "\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "#Initialize the model\n",
        "model = PreTrainedResNet(len(class_names), RESNET_LAST_ONLY)\n",
        "model = model.cuda()\n",
        "\n",
        "#Setting the optimizer and loss criterion\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#Begin Train\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  train(model, optimizer, criterion, epoch+1, NUM_EPOCHS)\n",
        "  \n",
        "print(\"Finished Training\")\n",
        "print(\"-\"*10)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b36cd5aab8944947915b99bdfcee9905"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINING Epoch 1/80 Loss 0.1687 Accuracy 0.0077\n",
            "TRAINING Epoch 2/80 Loss 0.1593 Accuracy 0.0303\n",
            "TRAINING Epoch 3/80 Loss 0.1512 Accuracy 0.0967\n",
            "TRAINING Epoch 4/80 Loss 0.1429 Accuracy 0.1743\n",
            "TRAINING Epoch 5/80 Loss 0.1352 Accuracy 0.2507\n",
            "TRAINING Epoch 6/80 Loss 0.1276 Accuracy 0.3333\n",
            "TRAINING Epoch 7/80 Loss 0.1202 Accuracy 0.4057\n",
            "TRAINING Epoch 8/80 Loss 0.1129 Accuracy 0.4697\n",
            "TRAINING Epoch 9/80 Loss 0.1063 Accuracy 0.5163\n",
            "TRAINING Epoch 10/80 Loss 0.1003 Accuracy 0.5600\n",
            "TRAINING Epoch 11/80 Loss 0.0943 Accuracy 0.5980\n",
            "TRAINING Epoch 12/80 Loss 0.0889 Accuracy 0.6370\n",
            "TRAINING Epoch 13/80 Loss 0.0840 Accuracy 0.6667\n",
            "TRAINING Epoch 14/80 Loss 0.0793 Accuracy 0.6940\n",
            "TRAINING Epoch 15/80 Loss 0.0749 Accuracy 0.7257\n",
            "TRAINING Epoch 16/80 Loss 0.0707 Accuracy 0.7433\n",
            "TRAINING Epoch 17/80 Loss 0.0673 Accuracy 0.7573\n",
            "TRAINING Epoch 18/80 Loss 0.0634 Accuracy 0.7770\n",
            "TRAINING Epoch 19/80 Loss 0.0600 Accuracy 0.8030\n",
            "TRAINING Epoch 20/80 Loss 0.0571 Accuracy 0.8167\n",
            "TRAINING Epoch 21/80 Loss 0.0539 Accuracy 0.8267\n",
            "TRAINING Epoch 22/80 Loss 0.0513 Accuracy 0.8310\n",
            "TRAINING Epoch 23/80 Loss 0.0487 Accuracy 0.8470\n",
            "TRAINING Epoch 24/80 Loss 0.0458 Accuracy 0.8707\n",
            "TRAINING Epoch 25/80 Loss 0.0436 Accuracy 0.8733\n",
            "TRAINING Epoch 26/80 Loss 0.0413 Accuracy 0.8853\n",
            "TRAINING Epoch 27/80 Loss 0.0390 Accuracy 0.8933\n",
            "TRAINING Epoch 28/80 Loss 0.0372 Accuracy 0.9093\n",
            "TRAINING Epoch 29/80 Loss 0.0352 Accuracy 0.9173\n",
            "TRAINING Epoch 30/80 Loss 0.0332 Accuracy 0.9193\n",
            "TRAINING Epoch 31/80 Loss 0.0317 Accuracy 0.9267\n",
            "TRAINING Epoch 32/80 Loss 0.0303 Accuracy 0.9363\n",
            "TRAINING Epoch 33/80 Loss 0.0290 Accuracy 0.9380\n",
            "TRAINING Epoch 34/80 Loss 0.0275 Accuracy 0.9413\n",
            "TRAINING Epoch 35/80 Loss 0.0258 Accuracy 0.9523\n",
            "TRAINING Epoch 36/80 Loss 0.0245 Accuracy 0.9533\n",
            "TRAINING Epoch 37/80 Loss 0.0237 Accuracy 0.9520\n",
            "TRAINING Epoch 38/80 Loss 0.0223 Accuracy 0.9643\n",
            "TRAINING Epoch 39/80 Loss 0.0214 Accuracy 0.9650\n",
            "TRAINING Epoch 40/80 Loss 0.0201 Accuracy 0.9700\n",
            "TRAINING Epoch 41/80 Loss 0.0194 Accuracy 0.9747\n",
            "TRAINING Epoch 42/80 Loss 0.0182 Accuracy 0.9767\n",
            "TRAINING Epoch 43/80 Loss 0.0175 Accuracy 0.9800\n",
            "TRAINING Epoch 44/80 Loss 0.0169 Accuracy 0.9760\n",
            "TRAINING Epoch 45/80 Loss 0.0159 Accuracy 0.9830\n",
            "TRAINING Epoch 46/80 Loss 0.0154 Accuracy 0.9847\n",
            "TRAINING Epoch 47/80 Loss 0.0146 Accuracy 0.9850\n",
            "TRAINING Epoch 48/80 Loss 0.0140 Accuracy 0.9840\n",
            "TRAINING Epoch 49/80 Loss 0.0132 Accuracy 0.9897\n",
            "TRAINING Epoch 50/80 Loss 0.0130 Accuracy 0.9887\n",
            "TRAINING Epoch 51/80 Loss 0.0123 Accuracy 0.9907\n",
            "TRAINING Epoch 52/80 Loss 0.0119 Accuracy 0.9900\n",
            "TRAINING Epoch 53/80 Loss 0.0112 Accuracy 0.9923\n",
            "TRAINING Epoch 54/80 Loss 0.0108 Accuracy 0.9927\n",
            "TRAINING Epoch 55/80 Loss 0.0103 Accuracy 0.9937\n",
            "TRAINING Epoch 56/80 Loss 0.0101 Accuracy 0.9940\n",
            "TRAINING Epoch 57/80 Loss 0.0095 Accuracy 0.9940\n",
            "TRAINING Epoch 58/80 Loss 0.0091 Accuracy 0.9923\n",
            "TRAINING Epoch 59/80 Loss 0.0089 Accuracy 0.9947\n",
            "TRAINING Epoch 60/80 Loss 0.0086 Accuracy 0.9957\n",
            "TRAINING Epoch 61/80 Loss 0.0083 Accuracy 0.9950\n",
            "TRAINING Epoch 62/80 Loss 0.0080 Accuracy 0.9977\n",
            "TRAINING Epoch 63/80 Loss 0.0077 Accuracy 0.9960\n",
            "TRAINING Epoch 64/80 Loss 0.0074 Accuracy 0.9957\n",
            "TRAINING Epoch 65/80 Loss 0.0071 Accuracy 0.9947\n",
            "TRAINING Epoch 66/80 Loss 0.0069 Accuracy 0.9967\n",
            "TRAINING Epoch 67/80 Loss 0.0065 Accuracy 0.9970\n",
            "TRAINING Epoch 68/80 Loss 0.0067 Accuracy 0.9960\n",
            "TRAINING Epoch 69/80 Loss 0.0063 Accuracy 0.9957\n",
            "TRAINING Epoch 70/80 Loss 0.0061 Accuracy 0.9963\n",
            "TRAINING Epoch 71/80 Loss 0.0060 Accuracy 0.9963\n",
            "TRAINING Epoch 72/80 Loss 0.0058 Accuracy 0.9970\n",
            "TRAINING Epoch 73/80 Loss 0.0054 Accuracy 0.9970\n",
            "TRAINING Epoch 74/80 Loss 0.0053 Accuracy 0.9977\n",
            "TRAINING Epoch 75/80 Loss 0.0052 Accuracy 0.9977\n",
            "TRAINING Epoch 76/80 Loss 0.0050 Accuracy 0.9970\n",
            "TRAINING Epoch 77/80 Loss 0.0048 Accuracy 0.9963\n",
            "TRAINING Epoch 78/80 Loss 0.0049 Accuracy 0.9973\n",
            "TRAINING Epoch 79/80 Loss 0.0047 Accuracy 0.9963\n",
            "TRAINING Epoch 80/80 Loss 0.0045 Accuracy 0.9990\n",
            "Finished Training\n",
            "----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEbsnh3a7ljw"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wyYKmQ91woU"
      },
      "source": [
        "def test(model, criterion, repeats=2):\n",
        "  model.eval()\n",
        "  \n",
        "  test_loss = 0.0\n",
        "  test_acc = 0.0\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    for itr in range(repeats):\n",
        "      for batch_idx, (images, labels) in enumerate(dataloaders['test']):\n",
        "        #move to GPU\n",
        "        images, labels = images.cuda(), labels.cuda()\n",
        "\n",
        "        #forward\n",
        "        outputs = model.forward(images)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "        test_acc += torch.sum(preds == labels).item()\n",
        "\n",
        "    test_loss /= (dataset_sizes['test']*repeats)\n",
        "    test_acc /= (dataset_sizes['test']*repeats)\n",
        "\n",
        "    print('Test Loss: %.4f Test Accuracy %.4f' % (test_loss, test_acc))\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znXWR6oWyl-B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6567054c-ed72-49e6-935b-828683540217"
      },
      "source": [
        "test(model, criterion)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.0535 Test Accuracy 0.5846\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNX2g3AYUbM2"
      },
      "source": [
        "# Visualizing the model predictions\n",
        "\n",
        "Only for viusalizing. Nothing to be done here. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zd_lkTdoUaOX"
      },
      "source": [
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(1)  # pause a bit so that plots are updated\n",
        "    \n",
        "def visualize_model(model, num_images=8):\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(dataloaders['test']):\n",
        "        #move to GPU\n",
        "        images, labels = images.cuda(), labels.cuda()\n",
        "        \n",
        "        outputs = model(images)\n",
        "        \n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "       \n",
        "\n",
        "        for j in range(images.size()[0]):\n",
        "            images_so_far += 1\n",
        "            ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "            ax.axis('off')\n",
        "            ax.set_title('class: {} predicted: {}'.format(class_names[labels.data[j]], class_names[preds[j]]))\n",
        "\n",
        "            imshow(images.cpu().data[j])\n",
        "\n",
        "            if images_so_far == num_images:\n",
        "                return"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxjSnLKOJsTW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "outputId": "f75868c3-f65d-4a32-f231-b28cf2fd4913"
      },
      "source": [
        "visualize_model(model)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUUAAABNCAYAAAAxUHaEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAZFElEQVR4nO2de7AkV33fP7+e6Xnf97600moX8TTCBmwLBSs8VAYMJSg5jrF5WwY5sZ1YlVQSl01IbCgZYyqmSpRdOBWDFYFjAoplBRmBY4wMSKYskMNj0RvtasW+d+9z3t198sfvnNu9V3fv7V1JK6/y+1RNzUx3zzk9PWe+/fud3++cI845DMMwDCV6uk/AMAzjHxMmioZhGAVMFA3DMAqYKBqGYRQwUTQMwyhgomgYhlHgrEVRRK4Rka89mSdjnDkiskdEnIhUz2GdN4rI9eeqvo0QkTtE5Fr/+u0i8lfnoM5zfs2fCZwvbfW8tRRFZFZEbhGRrojsF5G3rdn/ayLyiIgsicg3ROSfFvb9toiMRWSl8LjkNPWsK/4isk9EXvPkf7OnhqJ4PFNxzv2pc+51mx3nf/9PnYtz8vVZWz0Dnu62et6KIvCHwAjYDrwd+JiIXAogIpcDHwJ+FpgCPg7cIiKVwuf/p3OuU3h8/1yc9PluXay5hk922ef1tdkAa6tPA2fbVjcVRRHZJSJ/LiLHROSEiPzBaY67QUQO+LvdN0XkFYV9L/N3wCUROSIiH/HbGyLyKV/ugojcLSLbS5xTG/jnwH9yzq04574G/G/gnf6QPcBe59w3nQ7ZuQnYAmzbrOyzQUQiEXmftwKOishNIjLl9wWX4T0i8ijwN+GOLiL/RUTmvZXwhkJ5UyLycRE5JCI/EJHrww8sIhX/ueMi8n3gqifh/D8rIodFZFFEvhL+sH7fjSLyMRH5vIh0gSvXfHZCRL4sIh8VEVmn7DtE5HdF5O/973+riMye7tr47e8WkXv9tfmiiOwulPdaEbnPn+sfAFLYd4qlJCKXisj/EZGTvt29V0ReD7wX+HlvdX3rqbzm1lbPn7Ya2FAU/Ze7DdiP/ngXAp8+zeF3Ay8BZoH/AXxWRBp+3w3ADc65SeDZwGf89l9A7467gDngl4G+r/s3ROS209T1PCBxzj1Q2PYtIFyg24GKiFzuv8O7gf8LHC4c/yb/Z9krIr+y0XUowTX+cSVwCdAB1t48XgX8EPBT/v3lwP3oH+DDwMcLP9SNQAI8B3gp8DoguBO/BLzRb/9x1MJ4otwOPBf9I94D/Oma/W8DfgeYAIqiMwd8CbjTOXedO/2Y0Xehv8EF6Pf66Jr9q9dGRK5GRetngK3AV4E/8/VtAf4ceB963R4GrlivQhGZAP4a+AKwE72WX3LOfQH4ILn19WL/kRs5y2tubfUZ1VbBOXfaB/By4BhQXWffNcDXNvjsPPBi//orwPuBLWuOeTdwF/AjG53HOmW/Aji8ZtsvAXf414L+scboD3YcuKxw7AvRP0oF+AngEPDW09R1jS9jYc0jA17jj/kS8KuFzzzf111FbyYOuGRNmQ8V3rf8MTtQF2sINAv73wp82b/+G+CXC/te5z/7uN9ozfe4A7i2xLWd9uVN+fc3AjetOeZG4BPAd4H/UKLeD6259iN/7de7NrcD7ym8j4AesBsV168X9gnwWPhexTbpr9k/nOacfhv4VOH9U3LNra2eX201PDZzn3cB+51zySbHISL/3rs8iyKygFqAW/zu96B3zPtEXeQ3+u2fBL4IfFpEDorIh0Uk3qwuYAWYXLNtElgu1PeL6N24BrwDuE1EdgI4577nnDvonEudc3ehluxGd7GvO+emiw/g0cL+nag1HdiPNrJiV8CBNWWuWgLOuZ5/2UH//DFwSLRLYQH4r+Tu1M41ZRXrPWO8i/MhEXlYRJaAfX7XlsJha88d1BVqAn9Uopq15xtvUP5u4IbCdz+JCseFrPnuTlv9eucG2nYfLnFuoc6n6ppbWz21rrPmHLXVTUXxAHCxbNLhKtp/+OvAzwEz/odYxPf3OOcedM69Fb1YvwfcLCJt59zYOfd+59wL0bvgG1FrYDMeAKoi8tzCthcDe/3rlwC3OececM5lTl2mQ76O9XAU+qbOgoNoAwlcjN6xj6ypowwH0LvvlkLDnnTOBXfrEPqHL9b1RHgbcDXwGvRGtsdvL16P9c79v6Gu6ed9v9lGrD3fMWoRrVf+AeBfrvljN70gnPLdvQtXLJs15awbpeXx3+epvObWVk+t64lwLtrqpqL49+gX+5CItEUDI+v14UygF/YY2gD+M4W7o4i8Q0S2Oucy1JwHyETkShH5Yd+XsoT+WbLNTto510X7lj7gz+sK9GJ90h9yN3CViFwiymtRS/W7/nyuFpEZv+9lwHXArZvVuwF/BvxbEXmWiHTI+6w2tbDX+W6HgL8Cfl9EJkU7xp8tIq/yh3wGuE5ELhKRGeA3zqD4qv8NwyNGf7shcAJ1jT54BuX9a7Sv6XMi0tzguHeIyAtFpAV8ALjZOZee5tg/An5T8ujslIi82e/7S+BSEfkZf6O+DnXj1uM24AIR+TciUved7Jf7fUeAPSISwVN7za2tnndtdWNR9A33TWgn6qNo/83Pr3PoF1ElfgA1kQecasa+HtgrIiuo+f8W51wfbdA3o4J4L/C3+MYiGim8fYPT+1XUJD6K/tC/4pwLd9+b0IDQHb7sj6LWx31+/1uAh1AX5ibg95xz/z0ULBqVXI2el+AT/ry/Ajziv/+vncHn1/Iu1JX6Hto3ezMapAC9630R7ay/B/3DleVjaCArPP4E/f77gR/4+r5etjDvvv4LtF3cKnlgbS2fRPt2DgMN9I99ujJvQb2JT3sX6bvAG/y+48Cb0RSWE2iH+52nKWcZeC3afg8DD5JHIz/rn0+IyD3+9Vlfc2urz6i2ivjOSMN4ShCRO9Cgxh8/3ediGGU4n5O3DcMwnnTO64x141R898R6vME599VzejKGsQH/mNuquc+GYRgFzH02DMMoYKJoGIZRwPoUjdKIiPa11NAR7i30thqhg9CaflsgQdNqa/51FU3IaUNUg6gPyRE0uabn921FM1xraBpuKD/z70M5mX9d948VNHFjgTzbtQnMoGMvEnB/6Z5I0rPx/wkmikZ5qqjItVHxqxX21VFBa5IL1sjvi/zxTV9GC9oNqE3AUgzjFpqF58gFz6EZdEUhrPqywkjdui+zUjg3IU//z/yj78syjBKYKBrlaaMi0yAXr9Q/V8gtuyBe+H0xKqANf0wGToA6VDqQJOBS1NobkluGS4W6wzQEDhXFFmoBRv4cQr3496Bi6Otj/GRdBOOZjomiUZ4qarUlQBe1BIPbWyGf+wZUCMeoWNXJW5oXsV4Xeg6yLjpK/pgvq05u7Y3IR/qGsqrkAlf3x4X9GbkY9315I62P0w0qNIw1mCga5WmjQjNAhTG4pcFyG/pHAxWhLnlfY7DWvLWXDcld7EX/ueDihgy2jFyIg0scRHPgz6VFbrVW/Psg3ONCWYZREhNFozxCbnVF5PORVMmDLHW/fYQKVdcf0/bbBBXAMX46YXJrLlh540JdQRib5G61cKrlGcInDrVQfWCFBvkEXYZREhNFozxDVKhq5IGPGI3wzkBlGpodGA1gFNzmISp+IRgS+h2DsA0L20JwJQRW8Nvq6ERRwU2uoyJb9/VnvpyE3F2v+/MUyk+EZRiYKBpnQki3aaBiE6OCMwFcAPEUTE9Brw9LESQOdV/DpFRNPV7a4BZ9OSGdZ4Sm0xwnd5Ub/jGNimJwk+vk/Zgh8NIsnM8YTRlKUEuxi/UpGqUxUTTK0yZPwwmWmE+LiTvQ6UBrEuIJGDegW4GsjwpeTY9rtqBdg8EsJBlU6yB16I8hOY6K3Txq+dV8nbOoMLZAqiACcQXSBJLgpjfJhS+IZLAQj3LqlLaGsQEmikZ5guUW0ly8Syw1aDtoJxAJ1GJotTXVpj8ABiAtaLRhbhpmOz7AnEKtAamD+R4sOOhmkIS+wAqayN3Wz8cdaDV1c80L4skqpCF3MQRYApOoIA/JcyYNYxNMFI3yTPvnlDxoIuAiFbjxGKSvlp9EEFdh1FKhbDdhJobZirCj2SaKU7JsTFaNSNKMdprQasKxGZgXSOsQVUEakMXQiKHmoDpU0R0nkAk0BdIW9Pusus/iAzguuNghWGMYJbCmYpSmVoV0BKmAdMCFVJkhDHwEuB5DZaCCVRtAMoRGCrMCs1GVTlajnXWInCONVhhkI5zvQ2w1YWqkYtbNQJpaDhk0E1//EAaJWphZAhWBaqzCKZG61hLpvjiCocBoBUveNkpjomiURoYQZZD6ESmVWIUxG2rEOelBPwHndHssKqKRg6wBJDDsZcwzImo4XDxkQMJwBMkY+j3on4S0D7ICybL2OzZikDEkHbUQI4FxqvsygTiBqSpUnJbTI6yjCdRhNIW60IZRAhNFozSjJTT40QMq0NwG4yoMfdpN1oVsjFplCyqeRJDW4MQAsuUUyTJcJSFrZSStjGYdahWIhtBdgd5RGByZ4uJoOyv9FY5Hxxm2RqQjaC5DWoGkohpXrfsgdFWtxUoEbgzVSIVTKt6SbKigGkYZTBSN0rgD5MnZbZCUfMKGDM0xDCNcjvnXdcimYHEBVioOnMO5DBrgZmBiB3TaUB1X6J4Qkn1trn7tL3LlFf+E/Q8+wC23/QX3PXIPoxXIZtS1zvzIlbiqgpoMwA21DzJJfepjpG5+lEG158XaMEpgM28bpZFI3Gri9SzUd3vx8SNQ0h4qkEtozuEIHV0S3Neufw6jUmaAZ0E1qdLmYrbVpnjuzj1ce/WrOD7s4qrTHD4+z2/95vs0iHKB/0xL0346E9CJoduFLIPBCLIIxn6MdIizyBh6i5DcaVOHGZtjlqJRHkeeC7gAw5ZGh0H7DyM/BHA8JJ/aaxIVxiYqkmHmGoeK5F5IRsKofohk+5Af/fFpnrd9yCsvfSW3fvkevvaVO6lWY5LRWFcg94nZlRbUIpBM+y9XljWoszpnI9qPGftRNKddZdow1mCiaJw9XpBIfADEB16I0FEuMxDNQXsKnBO6kw53EBXDMJwvARhz6fN28f73XcfunW1u/ou7ee9P/it+4W0v5SUveTVv/M5VHDrwmIrqvJZfmYOWACOhmjp15fvoqJsxUNFItHOQDiDrnfOrY5ynmCgaZ4egguanCnM+0swIFbwYaECtDZ3JKs1GjeFUytLWMd0lR7bPwUFABJGIB/ef5Pc/8TkmmhE/9aafphrrmMI0S1haWDy13qFGmYeZkPSFwcDlk0eE3qCKJndnwHgFm2TWKI2JonF2jNEpv9qoZRiG/wW32Y87HtahV0+pVTO2TbeZmahw4L4+i8e6SFxjYtt2WtUxjQju/95efvpn38y7fu5diAjjJOH666+nu+ynugk9ggMYLsHxmiNZcuoe91AFbLM652ISLFETROMMMFE0zp4q+ew3Yep/IV8yYAVcDbqRo86QNG7i0mmW716g0t7Kj11zLZ3xAt+65WaWV1Z4xRv+Gb/+7/4jrUYb5xz79j/KXV+9U+sIKTVB6OZhFPoQwyQQsa+7h1qOYY7GMNmsYZTARNE4O0KwJPaPCrkLGyzHFDgJyQqcXHZ09w1whw4jWZ3nvPzVXP6iF/KZ330/C0sDXvHOt/ORD/wOu+a2ALDcXeG3PvgBjnePwja0jzJEthfR6Pa8r7tFHmoe+2MiVBDDglaWZGGUxETRKE+YnzAsE7CMik9YyCoIzwQqjmHNFQfZcoXusTGdzlZe9I6rufT5u/jC//owR+RBpn70Mva88jUc7q4wEWc8sO9+PnLDR7j9zs8hL4NoKiJNMrX49qOi1yvUF2bwjta8n8cE0ThjLE/RKI00xNEm76fzi0/RQHMRg6U4jVpwYbmCFFiOaHe2M/fq57MQHaTbe4SsPqYSC5dEW3j0q0Oa3SYzL53jB8n3GY4HxDMwuyOmEldY6A7oL4C7D3jEl52ymjMZ0nAIcziGnMgCzlmeorE50eaHGIYnLBkwS742c0LedxfrYTIGcT6HsQEsVmjMTTG+cIlHH72DpeUHcPUx1VqVGk1+4uGI+oEhiz84zjtffi3P3vNDRNug3YFtszWmJ4RmB2QSTeC+ANiCJnJvB3b6Z0HFMMzHaBhngYmicWaEBaECIT+w558jn6vo/KQRgzqVHTUG7QVG4y6I5g9Wa1WIa9QaLW76u6MsrgzJ0pTB0QNc+WNX4EZ+1YPEISJkTsc1U0EFcRbYBdULoL4FZBp12ztYp5DxhLDmY5SngvYjhjVUwqL3YTGqIepGV9DpwCJIp4a6P8x3OAa3AkkdaMaMexWyUUwYQL2y3KexMonbB90dMB/1SSZ0KJ8LC15VgWmoTURsqUVE7YRurJPUutDneYR8WQPDOANMFI3ytFFRHJBHn4PwhKVER6wGXaJpiNswcuC66JIAfsxzFqXQTBkOMnBhAeeMmiQMDvXgAehlMB856MG4jwZYTvg66lCZyKjWM9K2d9UPk/d1TvjjrcvcOENMFI3yhIWikg2OCWstV6DeFBoO0qEjCYKWoCk1VQdbUpKk5gvV6bIH/T6DFQddFdLekrrjLiVfBMuvKjjqwrGazr/oxuDCIlXL5EEYE0XjDDFRNMoT1mfeDG9BZss1RnGVbKkHJ13eF7mIBkNGgmQD3KrZCZPbtrA0f2I1+XrcZ3WEyuoa0nU9l7QL/bA0wpA8BWgFE0PjrLFAi1GekKh9Ovw4aAQkjhgnKcPeSCdjKAZngsANxrg0JD2qil2w4yIYu3yN5yGaqB3mZyy67UuoS37c7z+CWonFtaUN4wwxS9EoT4Sm2Iw51RKLyResRyBzuKbD9ROyJR4/9jik6khwmycICnZyqZ8PGRygViX+dZN8GGENtQgXUHHM/Oshfrk/8gCPWY3GGWCiaJQnjG0O7mwgzMAtEE1VcJLhqpluCxHj4HYHURNglPmFVMaEiM0o8QWHzSH5u8qpyeJBELvko1tC0Ccsv+qj3bZolXEmmPtslCeMElnbr5ihwjQP2bEEN8g0IFIjHxcdcKjQLQNHMhU21yI0xV0XX5QfO0YtvbCcaphAtuPLDAIbRtIU6whi7JcuOOUcDGMDTBSN8oRhe+vl/0V+/wIaZT5JCCg/3lJLUZf3CJpG43RKb5GI3bv25Md1fTnzhfLCWOtZ/7xeH2eMCmfNH1/FfCKjNNZUjCdOsNZArbpg4U2hwjVC+waLYtr1x3UcuEGhoHVULixdUCdfGEtQ0WuQ91mGaczqhfMZF443jBKYKBrlWS/vL0bHQfc4NTocRp4Eay4jD5oEwnyMq82wAjT0KUxFViSk3oQ+xg4aowlrx4Tx2Au+rnAuESqUhlECE0WjPDGPn6w1BEBqIFLBjUSnvK6ifXmTEFcgiQVXdeoKZ76saaAjcLzixVZwVEmi7FTrM5Ci4tuFqF0lG/t65sgjzsW1WIKAR+jkEYZRAhNFozQvfv1bWGhMcmxhmdqJR3nWrosY0OLQyYeY3DZLXNtJtCL0lx5k+/QUh5pNFsYPsfuiPRw9HjO8aB9zbpoonmRrc8y377qDwcGj4E5oBZFjwIijsy9g5opXU19eYrg8QPrg4ho9emx9wSyDVpX+fEpr54gleYxMIuouYzxwjIYJUTYmalbIoggqY6QTU906+7ReO+P8wUTRKM29f3crcTRFksFwZYmDxw5Qm5xgePIgh/dmtGa30KzWOLZvP+Mtc7iLJ0ndYywM91NtdFg6fpzRxE527riUH3nRS9n70F8T9SKyYBJmYx45+AhLo3kufM4Onj93Gd/+xl6manWmd+/h29/5DnPVBtH0Vu7f+7e0LtrK8y67ipWkxaH77mXuORczarZZfvghZidmqO3eyYFv30Vzbo6Gs5iiUQ6bZNYojdTErZvzF4IYa5tSE9gFlWmgAmmYd3FJqG7bRjxzlGgxovuNPInxgtdfxeKWbzJ47BDZvehIlWKApoq6wrOoWzxPPm3Z2tSccA4dLdsdzSzcYmyK3T6N8tROsz0ENELuIP7Z5y+mQ0jT4n5HOjhCvepo1dwpBR165F5Ix3RqUJkhn1E74FcJZB5N/RmQR6TXSxXqo+k8J+zmb5TD3GejPJutiJeRJ2yDWmgNdKRLDG6Uz7PoMuj3obI2mDI5wUynR2c5YqnjmJ92jASyjHxCirA4Vezfb3ZeZSaxMAyPiaJRnjCh7OmMLoe2qG1ADJVqrkdZChIixH42m/EIv1FNPKlUiGe2cVHrJJWOUN8SMTvdZmWQMr/cZ/lkipvn1JEqKflCVWsJwwmFjac7M4wCJopGecK6yhstLh8DTZAmNGrQO+HnOWz5WbFhde3mbBlWCmrVarV40WWvYnjsfhikuEpMvRYTd6p0tlY5sbPHicdGJMs6x6JMgwx0gtn0II93n1tofqJD3W3DKIGJolGeBiqKYUH6tdTR3MMWuAoMRn4JgUU06Tq4vz1W50QcNnKzsyLCC2a28/nvdYkXIUvGxCzSbjZIopRqJWNiBkYtGKQQ12HcgHSex49YqaFBlrCqoGGUxETRKE8YQxxmqSlSAS6EaBur03a5MPt1nXxChyG5qIbRJh4HpGnKie+PqPQgXYEoHVNrjElqupqfy9TiTDMQbwm6CqsiuxrMyVA3O6x0YBglsZQcwzCMApaSYxiGUcBE0TAMo4CJomEYRgETRcMwjAImioZhGAVMFA3DMAr8PxdeQWhIJ93VAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}